# -*- fill-column: 80; eval: (auto-fill-mode: 1); eval: (zotxt-easykey-mode 1); -*-
#+TODO: IDEA TODO DOIN WAIT | DONE CANC
#+PROPERTY: COOKIE_DATA recursive
#+STARTUP: overview
#+STARTUP: indent
#+STARTUP: align
#+STARTUP: inlineimages
#+STARTUP: latexpreview
#+OPTIONS: toc:nil creator:nil todo:nil stat:nil tags:nil inline:nil
#+OPTIONS: H:5 ':t ^:{} tex:t

#+TITLE: Detecting Double-Talk (Overlapping Speech) in Conversations using Deep Learning
#+AUTHOR: Abdullah

* Abstract

* Dedication
To Ammi, Abbu, Gudiya, Bushra, and Khushi

/To Happiness indeed/
* Acknowledgements

* Table of Contents

* [0/3] Introduction
- [ ] Double Talk (aka Overlapping Speech) is ...
** [0/3] Motivations for Detecting Double Talk
*** TODO [0/1] Linguistics                                           :cite:
- Manual annotation takes expertise and is time consuming.
- Can give ideas about conversations, the mood, the scencario, cultural aspects, etc.
- We are not concerned with the language model.
- While thesis is limited to Fisher dataset, evaluation also performed on KA3.
  - [ ] Add citation to Fisher dataset paper.
*** TODO [0/1] Speech and Machine Learning                           :cite:
- /Achille's heel/ of Speaker Diarization and Recognition systems.
  + leads to impure speaker models.
- A recognized source of errors in Speech Recognition systems.
  + [ ] Talk about that Microsoft paper.
- We are not concerned with overlapping speech recognition.
- We are also not concerned with diarization. Our modelling assumptions are
  speaker indpendent.
- We are also not concerned with how many speakers are talking simultaneously.
- We are also not concerned with source separation.
*** TODO [0/1] Artificial Intelligence and Future User Interfaces
- Make the assistants more natural to converse with, especially with back-channels, etc.
  + [ ] Search for continuous conversation type AI assistant research.
** [0/2] Motivations for using Deep Learning                          :cite:
- Incredible recent results.
- [ ] Learn features for us. Learn appropriate representations.
  + Existing research has pointed to potential improvements with better features.
- Relatively little investigation in this area.
- Made more accessible with open-sourced frameworks.
- Cons:
  + Need lots of data.
  + Long training times.
    * Esp when the hardware is not upto par.
    * [ ] Long lead times meant use reliable and easy to use Keras framework.
  + fine-tuning is still an art.
** [0/1] Challenges                                                   :cite:
- Not enough high qualitly data.
  - difficult to judge ground-truth anyway.
- Heavily imbalanced classes, with two being extremely closely related.
  - [ ] essentially a linear combination.
** [0/0] Related Works
* Approach
** Scope
- Speaker and language indpendent model.
- Assuming it to be a purely acoustic phenomena.
- Learn appropriate features.

* Evaluations

* Conclusions
- Context helps. Single frame is not enough for DT.
- Deep learning relies heavily on the optimizer.
- Summary statistics can definitely be misleading.

* Future Work
- Evaluate on different dataset
- More context?
  + Hardware limitations.
- More/better features.
  + If Fbanks do in fact work better, then, more hand-tuned ones as well.
- More complicated neural networks.
  + Bigger size ones.
  + Heirarchial model.
  + LSTM.
- Use language model.

* Appendix

* Bibliography

* Workflows                                                        :noexport:
[[https://bitbucket.org/motjuste/masters][This repository on BitBucket]]
** org-mode setup

Look at all the fiddling I have done, and there is bound to be more.

We have some example thesis.org files in `Documents` if you ever need
inspiration. Also checkout the references.
*** References
- [[http://bastibe.de/2014-11-19-writing-a-thesis-in-org-mode.html][Writing a thesis in org-mode]]
- [[http://www.macs.hw.ac.uk/~rs46/phd-thesis.html][Rob Stewart's PhD thesis]]
- [[http://orgmode.org/manual/In_002dbuffer-settings.html][Summary of in-buffer settings]]
- [[http://orgmode.org/manual/Export-settings.html#Export-settings][Export settings]]
- [[http://orgmode.org/manual/Embedded-LaTeX.html][Embedded LaTeX in orgmode]]
- [[https://www.gnu.org/software/emacs/manual/html_node/emacs/Specifying-File-Variables.html][Specifying File Variables]]
** DOIN [0/35] Finale Planne whatever
Most of this is going to have to be talked about in the [[Approach]] Section of the
thesis, and maybe also in the [[Introduction/Preliminaries]] where the concepts are general.
*** WAIT [0/4] Data Analysis
**** WAIT [0/2] Fisher
***** WAIT [0/9] About
- [ ] Where does the data come from, with reference to paper
- [ ] What does the data have
  + [ ] From the main readme of the dataset, all the params
- [ ] Why use this dataset
  + Real Double Talks, similar to KA3
    + [ ] Some examples
  + Not a laboratory dataset (?)
  + SNR (?)
  + Giant, may help the models generalize better
  + [ ] How have others used it?
- [ ] What part was used
- [ ] How is double talk inferred
- [ ] What are the limitations / problems
  + Only Telephone conversations, and only in English
  + Designed for speech recognition for conversations
  + VAD done automatically, not manually, only transcription done manually
  + No way to explicitly determine unique number of speakers over the dataset
  + Segmentation not as fine as TIMIT
  + Some parts are not annotated, and have to be taken out carefully
***** WAIT [0/13] Analysis of segment lengths : General, 0T, 1T, 2T
- [ ] *Do all analysis in a notebook, either here or `rennet-x`*
- [ ] *Do All analysis at /frame level/*
- [ ] *Use consistent colors*
- [ ] What is the annotation length + histogram
- [ ] What are the inferred segment lengths for 0T, 1T and 2T + histograms
- [ ] When do 2T segments occur?
  + [ ] S1 -> DT -> S1 (back-channel)
  + [ ] S1 -> DT -> S2 (turn)
  + [ ] S1 -> DT -> No (back-channel)
  + [ ] No -> DT -> Sx (overlapping-start and takeover)
  + [ ] No -> DT -> No (overlapping-start and backing down)
- [ ] What is the gender distribution for different segment lengths, 1T and
  2T, + pie-chart of n-frames + /maybe/ histograms
- [ ] /maybe/ What are the distributions for other params, like topic-id,
  dialect, etc.
**** IDEA [0/1] KA3
- [ ] Relevant things from Fisher later. Add here.
*** WAIT [0/2] Data Preparation
**** WAIT [0/5] Split into train, val, test/eval
- [ ] Which groups were added to which split, and possibly why.
- [ ] Check the distributions of different statistics
  + [ ] segment lengths : general, 0T, 1T, 2T + histograms
  + [ ] gender distributions for 1T, 2T + pie-chart + /maybe/ histograms
  + [ ] /maybe/ the distribution of other params
**** WAIT [0/7] Convert all to merged, mono, 8kHz, wav files
- [ ] Mention that we only export parts of the audio that are within =min-start=
  and =max-end=, although we actually do it before feature extraction on the
  read =numpy-data= later on.
- [ ] Check how it is being done in =pydub= and document
- [ ] expected to be =int16= files, without compression, and equal weights for
  all channels.
  + [ ] The values are normalized at the time of feature-extraction to be in
    range (-1, 1) and mean 0 when getting input to feature extraction. Done by =librosa.load=.
  + [ ] Check for each split to confirm.
- [ ] this is where the model hyperparameters have already started to
  accumulate, although it is arguable if using only Telephone conversations
  should be made part of that, especially since we are working with Deep Learning.
  - [ ] how to account for robustness?
*** TODO [0/8] Feature Extraction
**** TODO [0/3] Load audio using =librosa.load=
- [ ] make sure that they are in the range (-1, 1) and mean close to zero.
  + [ ] /maybe/ document what is being done to maintain that
- [ ] Take only the slice between =min-start= and =max-end= calculated with =samplerate_as(audio_samplerate)=.
**** TODO [0/7] Calculate the spectrogram
- [ ] use params:
  + win_sec = 0.010
  + hop_sec = 0.032
  + samplerate = 8000
  + window = 'hann'
  + power = 2
- [ ] Choose between =librosa.spectral._spectrogram= and =scipy.signal.spectrogram=
  + which one gives the values in a good range
  + when are the equivalent
  + which one is used in other =librosa= features, especially the melspectrogram
    + [ ] check which one gives good results for melspectrogram calculation, as
      in, which one keeps them in a good range.
  + [ ] Decide if to keep 0-fft-frequency.
  + [ ] *Make sure that the final shape is in terms of (time, frequencies).*
  + [ ] *Make sure that the shape in time dimension matches =samples_for_labelsat=.*
- [ ] keep the fft-frequencies to be added to the dataset h5, probably
  calculated using =librosa=. *Confirm with =scipy= and =numpy=.*
**** TODO [0/2] Make 16k equivalent long chunks per-file and save as single dataset in master h5
***** TODO [0/10] Dry run with a single file from validation set
- [ ] Make overlapping chunks with =strided_view=
  + win_shape = 2**14 = 16384
  + step_shape = 10 seconds = 10 * 100 = 1000
- [ ] Concatenate them either using =numpy= or =dask=
- [ ] when reading into dask, make sure that chunk-size is win_shape, aka 16k equivalent.
- [ ] Create one hdf5 dataset per file.
- [ ] Make sure that all chunks for a file are stored in the same dataset in h5.
  + [ ] Make sure that the chunking value is the same as the 16k equivalent we created.
  + [ ] Check that reading all chunks do give the expected results.
- [ ] Use compression
- [ ] Use Checksum
- [ ] Add fft-frequencies as attribute or whatever =h5py= provides, to each dataset.
***** TODO [0/5] Final notebook for all splits
- [ ] Keep to and from location for data configurable.
- [ ] Run on *myrmidon*
  - [ ] Remove old data for new space.
- [ ] Run on *unumpu*
  - [ ] Save the data directly on *nm-raid*
**** TODO [0/4] Normalization
- [ ] Check what log-ing the spectrogram does to the range
  + [ ] Check the impact of =librosa.logamplitude= with =ref=1.0=
- [ ] Choose if CMN or CMVN.
- [ ] Normalize on chunk (== utterance) level at the time of feeding into the network.
**** TODO [0/4] Mel-Spectrogram calculation
- [ ] Make sure to do this before log-ing, and do CMN or CMVN after
- [ ] Choose n_mels
  + 64 will be faster
  + [ ] check that audio-classification-keras guy's explanation for 96
  + don't go 128
- [ ] Do at chunk level at the time of feeding into the network.
**** IDEA [0/2] Add other possible features for future investigations, mainly as text
- [ ] look at links on MFCC, iVectors, prosody, pitch, CNSC, PLP, etc. for inspiration.
  + [ ] See if the argument that 'spectrogram' is the mother of all features
    still holds.
*** DOIN [0/17] The Classifier and Configurations
**** WAIT [0/3] Fixed number of =steps per chunk=, {=8= or whatever runs (likely one with 2k examples)}
- This is to make sure we can predict exactly how many steps are required for a pass
- The same number of chunks are to be used with the same number of steps per chunk
  + For skipping/sub-sampling, do it before feeding into the stepper.
- [ ] How many total steps are there in the training data per sweep?
  + [ ] on myrmidon
  + [ ] on unumpu
**** WAIT [0/5] Number of epochs / passes over the dataset - =8 epochs per pass= * {=1=, =6=, =11=, =21= =passes=}
- [ ] Make atleast one pass over the entire dataset for every model.
- [ ] upto 20 total passes for the best/most promising/relevant models.
  + 2 types of promising results both working with atleast the same features,
    and other such input of same parameters, like, context, etc.:
    1. Excellent 0T v/ {1T + 2T}
    2. Best 1T v/ 2T, with 0T skipped
  + [ ] build upon saved checkpoints from earlier runs.
  + [ ] pass starting epoch as a parameter to Keras.fit_generator?
- *Fixed number of keras epochs per pass anyways* == 8
- Since there is a fixed number of steps per pass, irrespective of skipping or
  subsampling, the number of steps per keras epoch is also fixed.
  + equivalent to (total_steps // 8) + 1 for a keras epoch.
  + equivalent to ((nchunks * 8) // 8) + 1
  + [ ] Make sure these invariants hold
- It is okay if we pass over a little more due to rounding, but we don't want to
  pass less than the entire dataset.
**** WAIT [0/0] Part of =training data= to use {=all=}
We use all the data we have for training. We'll see that we train each model for
atleast one pass.
**** WAIT [0/2] The Neural =Network= - {=c3=}
- There is essentially just one model based on the code below.
- There is one output per-sequence, as in, we do sequence classification, but
  not at utterance level.
- The configurations will decide:
  + The input shape, and hence the context per frame.
  + The number of classes.
- We use BatchNormalization *BEFORE* Activation, to follow the original paper.
- We use Categorical crossentropy, and categorical accuracy.
- [ ] We use adamax as optimizer, but this can change
- [ ] Check more conv nets for speech and decide one final that we can run.
- [ ] Move this to =keras_utils= or =models= or =model_utils=, and actually see
  the model output. Too much time getting wasted in making it work in spacemacs.

#+BEGIN_SRC python :results output
  from keras.models import Sequential
  import keras.layers as kl

  def c3(input_shape, nclasses=3):
      model = Sequential(name='conv3')

      # first conv2d layer
      model.add(kl.Conv2D(
          64,
          3,
          strides=1,
          data_format='channels_last',
          input_shape=input_shape[1:],
          name='c1_3_64_1',
      ))
      model.add(kl.BatchNormalization(name='c1_bn'))
      model.add(kl.Activation('relu', name='c1_relu'))
      model.add(kl.Dropout(0.1, name='c1_d_10'))
      model.add(kl.MaxPool2D(2, name='c1_mxp2_2'))

      # second conv2d layer
      model.add(kl.Conv2D(
          128,
          3,
          strides=1,
          data_format='channels_last',
          input_shape=input_shape[1:],
          name='c2_3_128_1',
      ))
      model.add(kl.BatchNormalization(name='c2_bn'))
      model.add(kl.Activation('relu', name='c2_relu'))
      model.add(kl.Dropout(0.1, name='c2_d_10'))
      model.add(kl.MaxPool2D(2, name='c2_mxp2_2'))

      # third conv2d layer
      model.add(kl.Conv2D(
          256,
          3,
          strides=1,
          data_format='channels_last',
          input_shape=input_shape[1:],
          name='c3_3_256_1',
      ))
      model.add(kl.BatchNormalization(name='c3_bn'))
      model.add(kl.Activation('relu', name='c3_relu'))
      model.add(kl.Dropout(0.1, name='c3_d_10'))

      # max globally
      model.add(kl.GlobalMaxPool2D(name='gmxp'))

      # first FC
      model.add(kl.Dense(512, activation='relu', name='f1_512_relu'))
      model.add(kl.Dropout(0.1, name='f1_d_10'))

      # second FC
      model.add(kl.Dense(128, activation='relu', name='f2_128_relu'))
      model.add(kl.Dropout(0.1, name='f2_d_10'))

      # second FC
      model.add(kl.Dense(32, activation='relu', name='f3_32_relu'))
      model.add(kl.Dropout(0.1, name='f3_d_10'))

      # output layer
      model.add(kl.Dense(nclasses, activation='softmax', name='sfmx'))

      # Compile and send the model
      model.compile(
          loss='categorical_crossentropy',
          optimizer='adamax',
          metrics=['categorical_accuracy'],
      )

      return model

  input_shape = (None, 21, 64, 1)
  c3(input_shape).summary()
#+END_SRC
**** WAIT [0/2] Features to use - {=melspectrogram=, =spectrogram/cepstrogram=}
- *Priority* is for melspectrogram.
  + choose one between {64, 96}, and stick to it.
  + They will be calculated on the fly from the saved spectrograms.
- spectrogram can be used for making an argument on the lines that it requires
  more work, and/or the argument that network is supposed to learn the features,
  but takes too much time ... or whatever.
  + Run spectrogram only for at-most 3 best models, based on results.
  + May incur more epochs.
- [ ] Make sure to log and normalize the inputs before feeding them into the network.
  - [ ] do it *before* making it into sequences.
- [ ] We wait on the final decision any way from [[Workflows/Finale Planne whatever/Feature Extraction]]
**** WAIT [0/2] Making sequences to input with =context= - {=±10=}
- There are multiple options, and adding more context has helped results.
- I have decided to choose and evaluate only on ±10 frames (±100 ms).
- The decision comes from @ryant:2013speech
- We can go for ±20 or ±30 as in @xiong:2016achieving, but why not:
  + hardware limitations
  + Run time limitations
  + [ ] Add this to future works
- [ ] Add `[..., None]` at the end to make it `channels_last` for conv2d
**** WAIT [0/1] =Skipping= class(-es) {=0T=, =None=}
- *We only experiment with skipping 0T when we choose to, and it is preferable*
  + skipping 1T does not make sense, use subsampling instead
- We *still* maintain the same number of steps per chunk, even though the
  batches now may be of different sizes
- [ ] should check it out offline first to see that there are no unforseen
  circumstances where the batches may end up being empty.
- We want to avoid making copies of giant arrays, so the convoluted algo below.
***** TODO [0/0] How to skip, the algo
- Do normal strided data_prep
- Do normal strided label_prep.
  + This is the final decision of labels to skip or not is made.
- The label prepper returns two things
  1. the prepped_labels
  2. keep, which is:
     + True, if to keep all
     + np.array of booleans of size of prepped labels, indicating which example
       to keep
- The packaging method that sends data to stepper forwards all three
- The stepper looks at keep
  + if not is nd.array and True (check beforehand that this is never False)
    * return step-wise in nsteps_per_epoch
  + if nd.array of booleans
    * do cumsum of keep
    * nexamples_per_step = cumsum[-1] // step_per_epoch
    * ends = searchsorted(cumsum, arange(steps_per_epoch)  * nexamples_per_step,
      side='right')
    * starts = [0].extend(ends[:-1])
    * return data[keep[start:stop]] and label[keep[start:stop]] for start, stop
      in zip(starts, ends)
- Make sure that we never go out of bounds in our calculations, and never reture
  empty batches/steps, and always return the same number of steps per chunk.
**** TODO [0/0] =Sub-sampling= class(-es) {=1T_0.25=, =None=}

**** WAIT [0/0] Class-/Sample- =Weights= {=clsw_1= =clsw_2=}
- *We only use class weights, set them to 1 for both 0T, and 1T*
- we prefer to set class weight for 2T as 1, and at most 2
  + prefer 1 especially when skipping and/or sub-sampling
  + using 2 to perhaps support the argument that adding a cost matrix doesn't
    help much. I hope the results support it.
- *We use all ones as clsw when either skipping or subsampling*
- Why not more:
  + because, 2T is very similar to 1T.
  + It hardly ever gets confused with 0T.
  + Too much clsw for it has been shown to make the network results less confident.
    - Elaborate, with examples.
  + Slows down training in some ways.
  + We are also not 100% sure about each and every label. There is a collar.
- Why not based on data, entire or per batch:
  + the class weights become even more skewed.
  + Experiments were performed, things went wrong
**** TODO [0/0] Choosing label for a sequence - {}

**** WAIT [0/1] Save model =checkpoint= on every keras epoch {=every=}
- [ ] decide on file name formatting.
- Save checkpoints every epoch
**** WAIT [0/2] Save =Tensorboard= events {=yes=}
- [ ] Is the images and stuff not showing up an issue from my side?
- [ ] Is there a way to append to existing events file, instead of adding a new one?
  - if nothing else, if keras reflects what epoch we are training on, maybe that
    will help.
**** WAIT [0/3] Validation data to be given to =keras.fit=
- [ ] possibly give the same files as those given for the confusions calculations
  + so the confusions are printed for the same predictions/loss
- [ ] give as generator, and get the step size by running a fake loop.
  + We can also predict the step size as it is, but let's stick to what we can trust.
  + Or, implement it into the data provider class. We can, cuz we plan to keep
    the n-steps constant per-chunk.
- [ ] Do not do any sub-sampling in the validation data provider.
  + skipping will be done however.
**** WAIT [0/10] Confusions calculations in callback
- [ ] Add to a new =keras_utils= file.
- [ ] Make a confusion calculation on 0th batch and 0th epoch
  + this is to make sure that any errors due to the size of the batch are caught early.
- [ ] Only do per epoch, and hence, multiple epochs per sweep has to be implemented.
- [ ] Make sure that there is no code being called that needs packages not on
  the GPU.
- [ ] Structure of the h5?
- [ ] use one call, maybe 2, and save the predictions, generated using normal predict.
  + [ ] '00003' and '00086', or some other based on analysis, /maybe/
  + [ ] Save the ground-label only once, maybe also with chunking information.
- [ ] print out the precision and recall for all classes being trained on
  + [ ] Save the full confusion matrix as well.
**** IDEA [0/3] Predict over validation and/or test dataset at the end of training, in the script
- [ ] How much time is to be expected? Wll have to consider that while
  submitting jobs.
- [ ] predict only on the last epoch? What if it starts overfitting?
- Prioritize saving the model first before this. We can do it offline, and
  probably will have to anyways.
  + Want to do on GPU cuz there is so much data, and inference takes that long.
- [ ] Have to come up with a better /loop/ to save the predictions, cuz
  =predict_generator= works on returning one giant numpy-array.
**** IDEA [0/1] Adapting a model for KA3
- [ ] This depends on whether or not the models will be evaluated on the KA3 dataset.
- Looks time consuming, and not much promising.
*** [/] Post Processing / Smoothing / Inference
*** [0/0] Evaluation



* Logs                                                             :noexport:
** 27-Jul-2017
*** DONE [0/0] 5:53 PM : Setting up.
CLOSED: [2017-07-27 Thu 18:43]
I think I am going to be wasting a lot of my time fiddling with org-mode and
spacemacs. Add to that my perversion for using [[https://normanlayout.info/][Norman layout]] for typing, and I
am not sure how my numbers for productivity will look like.

And it is stupid, especially in the current context. There is a lot of stuff to
write and there is lot of stuff that will need to get done before a lot of stuff
gets written. And don't even get me started on the amount of back and forth that
will inevitably take place until the final document is ready to submit.
**** Why choose org-mode?
***** Pros
+ Pure text is easy and convenient to write, and adding $$\LaTeX$$ formatting is
  pretty easy towards the end.
+ Text files are easy to put in git.
+ There are many handy tools available for exporting, formatting, task
  management, etc.
+ I can run code from within the the org file, potentially making this repo a
  single file one.
+ I have some helpful reference usages available for using org-mode to write theses.
+ The experience can result in a life-long competency.
***** Cons
- Too many opportunities to fiddle with, especially considering I don't have
  much exprience of working this seriously, at least not with success, in
  org-mode beforehand.
  + I don't have enough experience with $$\LaTeX$$ either, but it is likely that I
    would have used Atom and some hacky, possibly inefficient process to make it
    work, just like I did for my seminar report.
- Too many opportunities to get distracted by, including making this my one-file
  repo idea, where this file holds other, non-thesis related, stuff as well,
  like these loggings.
- Can only use emacs to make best use of this file.
  * Frequent exports may be necessary.
**** Why do [[https://normanlayout.info/][Norman keyboard layout]]?
***** Pros
+ I type faster in it.
+ It is overall more comfortable for me.
+ I have some practice of using this layout while using org-mode, so not very many keys to relearn.
***** Cons
- Not very comfortable while using VIM keybindings, but not absolutely abysmal either.
*** DONE [5/5] 6:40 PM : First incision.
CLOSED: [2017-08-01 Tue 13:55]
If you don't believe me, I have writing the above log entry till *now!*

I have a bunch of things to do in order to even call all these hours to not have
been a waste. Those things shall be, at least for today:

- [X] Create an outline of the possible chapter headings.
- [X] Add some outlines in [[Introduction/Motivations]].
  + Added to a bunch of other headings too, main points that is.
  + There is still a lot of literature review kinda things needed.
  + I can keep on going, but ... hey ... good start eh!
- [X] Add links to pages that helped setup org-mode this far as references in
  [[Workflows/org-mode setup/Refrences]].
- [X] Test a preliminary export. Make sure git doesn't find it interesting.
- [X] Sync Google Drive.
*** DONE [0/0] 8:31 PM : After first incision
CLOSED: [2017-07-27 Thu 20:35]
I hope I can do this. I am finding this interesting, so that is a positive sign.
And I am talking about writing, not just fiddling with org-mode. In fact, it is
very likely that I never close this window of emacs, unless something forces me to.

I have done my things till syncing with Google Drive. It is a nice Checkpoint.

Next changes at hand are not exactly here, but in Todoist, essentially a
complete overhaul. That is definitely daunting and time-consuming, and I am
already hungry.

I hope that the next update is today, and I hope it comes with good news.

Back to the writing experience, I need to read a lot of papers again, if I have
ever come across them at all. That ... is ... scary.

Hope Allah Helps.
**  1-Aug-2017
*** DOIN [0/3] 11:46 AM : Final Planning
No, I have not reorganized Todoist yet. Fuck!

But today, We do it!

After brainstormings and experimentation in the past days, I have come to a
conclusion which means that I basically have to start over ... from scratch.
That is definitely a daunting task. And I have to finishe writing this thing in
the meantime as well. I am very much screwed, and that will be mild to say.

And, since I have less than a month to do all of that (for buffer, we see why
later), not only does it demand excellent efficiency, but also aggressive
pruning and perhaps compromises. There is a small buffer to accommodate any
unforseen emergencies, but don't rely on it. There will be emergencies, the
first of which has been that I may have been calculating my spectrograms all
wrong till now!

Therefore, the plan, the final plan. Also, moving updating todoist to today as well.
- [ ] Make a final-ready plan in Workflows for all the things that need to be run.
- [ ] Reorganize Todoist ... please ... dude ... it is unusable ... cluttered
  with outdated and/or impossible ideas and tasks.
  + [ ] Find paper about `fe_03_p1` and add to zotero. :todoist:
*** DONE [3/3]  1:56 PM : Progress ... is slow
CLOSED: [2017-08-02 Wed 14:28]
After quite a bit of unnecessary waffling, I have finally started writing the
[[Finale Planne whatever]]. I started from the very beginning, hoping to make sure
that I don't miss any thing, and to organize my thoughts anyway.

I have only reached till feature extraction, although there is still a section
left for melspectrogram and normalization. There are more todos here that I am
not sure what I should do with them, and will multiply the [[The Classifier and
Configurations]] set of todos even more.

But, I am making progress. One idea that I got in the middle was to save the
chunks as overlapping by 10 seconds. That will help solve the issue of making
appropriately overlapping context frames. The choice of 10 seconds is to set the
upper-bound of the context frames I will be using later. I know that I will
actually only need like 100ms, but ... the repetition will hopefully not be an issue.

Furthermore, I may then settle to do CMN or even CMVN on the chunk level,
treating it as an utterance that is more than 2 minutes long.

Finally, I have had a few other ideas for aggressive sub-sampling.
- [X] Make sure to make this parametrizable, and skippable for validation data provider
- [X] In order to remove silences, and train only on 1T and 2T
  + Read the chunk for audio and labels
  + Remove the audio and labels where label == 0
  + Calculate the mean and variances on these
  + group the audio and labels based on label == 0 or not
  + If label == 0, keep a (context_len) amount of data, initially set to None, to_prepend
  + if label != 0, and if to_prepend is not None, prepend it to the current data
  + Make strided views for each group
    * No need to keep data from from here, cuz we are only going to group if/not silence.
  + Concatenate the strided views ... will need to make copies and increase memory need.
  + Give this to the stepper
- [X] In order to aggressively subsample 1T and 0T
  + If we have groups (as in, skipping silence)
    * create strided views with step_size = int(win_size / (larger_factor))
    * concatenate, and give to stepper
  + If not working with groups
    * still make groups based on label == 0 or not
    * Repeat above for grouped case only if all labels in the group are 1T (and/or 0T)

It is already 2:37 PM! Actually, I finished the normalization and melspectrogram
sections as well, for now. There are a LOT of todos, just for today, and many of
them are decisions and explorations. I am gonna go shower and pray. Lunch only
after all the planning has been done. Damn, the hardest parts are still to come.
*** DONE [1/1]  5:05 PM : I am late now am I not?
CLOSED: [2017-08-01 Tue 19:01]
Can't deny that I predicted this 'not being back before 5pm' thing. It's a
tragedy, and I didn't spend the time on eating. And all that while knowing that
the next sections to work on are by far the most crucial, and probably will give
me the most peace of mind. And also the fact that *I am only planning*, and all
those todos need to actually get done, and then written about in the thesis.

Hours are passing by in minutes, and I am waffling.

Here's the thing... I know that when I say that the tasks ahead are the most
important and difficult and what not, my heart starts beating like crazy and I
get stressed, and that makes decision making even more difficult and scary.
Therefore, I am not going to be too angry about this whole thing, because, one
way or another, I have to make sure all the things get done, and breaking down
will be a disaster. Calm down.

Here's an idea ... start tracking time. To the minute I say. The hope is that it
will pull me back to work when I am wasting it away, and push me away from it
when I am waffling too much and spending too much time on some thing. Like right
fucking now!. I can't plan to the minute, but that shouldn't stop me. And hey,
may be the tracking and looking at the actual time being spent will help me make
better plans!

- [X] Start tracking time, granularly, for *EVERYTHING*
*** DONE [1/1]  7:01 PM : Starting with what is kinda fixed.
CLOSED: [2017-08-01 Tue 20:07]
Have setup the trackers, tracking *EVERYTHING*, and then took a break for prayers.

The main thing is, there are so many options for the networks and the configurations.

I started with what I know, it is going to be a CNN classifying sequences, where
sequences are essentially single frames provided with left-right context.

At first I was thinking of adding one with starting filter size 3, but keep the
one with 5 as my top priority. Then, something stupid happened while adding the
source for the model and I had to restart. Then I decided to fuck it, and go
with a single network config, just the 5, and wrote the whole code.

But ... once done, I decided to make that 3. You see, now I will be working on
10ms hopped data. Furthermore, there are very few (64) filters in the first
layer (limited by hardware, per experience), so ... made that 5 into 3. /sigh/.
And, it makes sense that it fewer local patterns, and then more and more global
patterns going up. I hope it is fine. It looks a lot like the conv from that
@xiong:2016achieving paper (actually, more like VGG net) but with much fewer layers.

So, I have only one architecture to train on. I hope it works.

Furthermore, even though I may look into using configs with different context
sizes, I believe I am going to stick with (-10, +10) frames ((-100, +100) ms),
ending up in size 21 frames per sequence (210 ms). I don't want to test out the
other one for one obvious reason that I don't have the time, and there are far
more number of other hyperparameters to test out. Also, it makes sense to add
more context, and was shown to be good in that paper doing SAD on YouTube @ryant:2013speech.

And you know what ... there is still a lot of work to get done, and it is
already past 7:00 PM. 💩.

- [X] Prioritize models with ±10 frames context, and maybe skip the others.
*** DONE [0/0]  8:07 PM : Aggressive pruning ... slowly
CLOSED: [2017-08-01 Tue 20:13]
Added that section on options for context. I am adding the set of options
available for each 'hyperparameter' at the end of the heading. So far, the
features to use has more than 1 option, but I have to keep in mind that I have
not even begun to add a lot of other things.

They will depend on the arguments I want to make. The fewer, and clearer
arguments I want to make, the better I can design my experiments, and definitely
the more time I will have ... if I don't decide to go too overboard with nepochs.

I will, however, also have to keep in mind that I show-off some novelty in my
approach. Just CNN on context is an okay bet, but you know how much I want to
use the max-in-center-label approach. The only problem there is post-processing,
and that is another can of worms I have not opened yet, and it is past 8:00 PM.

Wasn't the plan that I get done by 12:00 PM. /sigh/
*** DONE [1/1]  9:19 PM : Couldn't take a break, updated network
CLOSED: [2017-08-02 Wed 14:27]
I wanted to take a break, and have dinner and what not, but in the cooling down
period and looking at how much still had to get done, I was ... waffling about
making myself something.

I realized that my earlier and complicated idea to do silence-skipping was
overly complicated, and could be done by simply removing the silence labels
/after/ they have been made into sequences. Yes, there will be forced copying in
=numpy=, but my earlier solution also involved that. Furthermore, this solution
is much less complicated, and doing sub-sampling of 1T is pretty similar, except
that I will already have the labels for the sequences, and can choose create my
boolean for keeping things by working only on the final sequence labels.

- [X] Add the approach of filtering away /after/ sequences have been made, in
  addition to the one brainstormed previously, and choose one.

After that, I actually got back to running the network and looking at the number
of params. I stopped fiddling with org mode to run my source block, and simply
copied and pasted the code in ipython. And lo-and-behold, I had missed max-pool
layers in between conv layers. Because, my number of trainable params were
beyond 1M. Now, that could definitely improve our model, but the problem in that
case would be training times. Plus, how many params do I need to learn for 3
fucking classes?

Anyway, I can still fiddle with it a little more, and change things. Like I did
by making the final FC layer tapered, instead of square. Too many params.
*** DONE [0/0]  11:32 PM : There are only so many hours ... I am awake
CLOSED: [2017-08-01 Tue 23:41]
The dinner break was a forced and unrewarding one. And then came in my nightly
routine. I don't feel very accomplished today. The biggest things are still left
to do in the planning task itself, let alone to start implementing on those
plans, which could definitely prove a lot more ominous, if not filled with
distractions and what.

If history is worth trusting, I may not even finish this planning thing. And,
also, I may not even end following a lot of it. There are definitely things that
can go wrong which may force me to abandon all hopes and get the minimum done.

The only reason I decided to get through with this was that I believed that I
had more knowledge now than ever. And that the deadlines will force me anyway.
And, otherwise, I can stop my brain from having incoherent thoughts.

The sad thing is ... a lot of the tasks are like the 'figure this out' kind, and
there are definitely more to come I am sure, my first deadline of having getting
this planning done today has been horrendously overspilled, and, just look at
how many hours it took me to write that much, I was that incoherent with my ideas.

Tomorrow is another day. Just like today.

I hope having finished a few of the easy sections will not come back to bite me
and make it more difficult for me to get started tomorrow.
**  2-Aug-2017
*** DONE [0/0] 12:04 PM : Half the day left, after the other wasted frivolusly
CLOSED: [2017-08-02 Wed 12:08]
Well, it is officially more than 12 hours since I did something. Literally
anything useful, except probably sleep, but feeling the way I am right now, I am
not sure I even did that correctly.

We finish the plan in the next 6 hours, InshaAllah. It is very likely that I may
not have very good plans about the post-processing and evaluation sections, but
we can leave them with a few todos that can be figured out while our models are
training and we are writing.

Oh fuck ... yes ... I have been relying on writing the thesis while the
trainings ran in the background. I am screwed ... but that is not new.

Let's get started. Remember to sit properly, last night's back pain was not good.
*** DONE [0/0]  2:28 PM : Been making progress, better skipping algo now, I hope
CLOSED: [2017-08-02 Wed 14:34]
I have been making progress on certain decisions and plans. For example, I now
have it decided that only clsw are going to be used, and that too only for 2T,
and only upto 2.

I also made a lot of other adjustments and additions, that frankly I don't
remember much about right now. But, at least, I am in the zone.

Given the amount of time that has been spent, I am still going slow, but the
decisions are making me feel good, even though it is only an iceberg.

I will continue working on this, especially because I am in the zone, and don't
want to get it interrupted by anything. Not really hungry.

Furthermore, the next major sections on post-proc and evaluation can either be
straightforward, or require a lot of research. (FML). But, I can add some
desirables and todos for now and take them up later.

There is a lot of dev work waiting for me, and quite an uncomfortable few of
them will need careful research as well. Hence, I am pushing for finishing the
planning before I put something energizing (more like sleepy-making) in my stomach.

I really hope that there are no big surprises waiting for me. I really hope I
haven't been something really obvious.
