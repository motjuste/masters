# -*- fill-column: 80; eval: (auto-fill-mode: 1); eval: (zotxt-easykey-mode 1); -*-
#+TODO: IDEA TODO DOIN WAIT | DONE CANC
#+PROPERTY: COOKIE_DATA recursive
#+STARTUP: overview
#+STARTUP: indent
#+STARTUP: align
#+STARTUP: inlineimages
#+STARTUP: latexpreview
#+OPTIONS: toc:nil creator:nil todo:nil stat:nil tags:nil inline:nil
#+OPTIONS: H:6 ':t ^:{} tex:t

# #+LaTeX_CLASS: phd
#+TITLE: Detecting Double-Talk (Overlapping Speech) in Conversations using Deep Learning
#+AUTHOR: Abdullah

#+LATEX: {\small \begin{abstract}
Detecting Double-Talk
#+LATEX: \end{abstract} }

#+BEGIN_LATEX
\begin{dedication}
\vspace*{\fill}
\begin{center}
To Ammi, Abbu, Gudiya, Bushra, and Khushi

\textit{To Happiness indeed}
\end{center}
\vspace*{\fill}
\end{dedication}
#+END_LATEX

#+LATEX: \begin{acknowledgements}
Joachim, Alex ... set the bar that every other human has to reach ... and they
set a very high one.
#+LATEX: \end{acknowledgements}

#+TOC: headlines 3

* DOIN [0/3] Introduction
** WAIT [0/4] Motivations
Conversations are the dominant form of social interaction between two or more
people. While text based messaging has seen meteoric rise in recent years,
speech is still the primary medium of holding conversations. These interactions,
especially face-to-face ones, are often enriched by both verbal as well as
non-verbal acts by the participants in the facilitation of the exchange of
ideas, information, thoughts, feelings, etc.
- [ ] Mention how these acts make life hard in ASR and other speech systems

The study of verbal and non-verbal cues are key to conversation analysis
linguistics. In addition to the choice of words, the use of
intonation, pauses, feedback signals, turn-taking, and other conscious or
unconscious phatic expressions, which may on their own not hold any information
of value, can still provide fascinating insights into various aspects of a
particular conversation. One could analyse their use and frequency to determine
the scenario, the topic, the structure of the interaction, pre-existing or even
ad-hoc relationships between the participants, their cultural backgrounds and
predispositions, their level of engagement and comfort, etc. To the inquisitive
mind, such analyses and related discoveries are the means to kindle the fire
that it is. However, the practical aspects of their study can inspire richer and
more satisfying social interactions in the future, and, where relevant, guide
the participants towards achieving specific goals of the exercise. The
popularity of emojis perhaps arises from the need to simulate such acts in a
concise way under the constraints of medium of text.
- [ ] fix the last sentence
- [ ] Mention Automatic Dialogue systems

For their studies, the researchers collect data in the form of video or audio
conversations. These need to transcribed in detail for the different aspects of
the conversation. The process, even with the available of digital technologies,
is a painstakingly manual and time-consuming task, and often require expert
knowledge. Using software tools like ELAN or PRAAT, they often have to go
through a conversation multiple times to finely annontate different events in
the recording. This involves disambiguating not only what is spoken when and by
whom, but also other verbal and non-verbal phenomena like intonation, prosody,
gestures, etc. When necessary (for example in bad recording conditions), and
when possible, this transcription is carried out by multiple parties before
final adjudication. Certain phenomenon of interest occur for such small durations
that locating them at a high temporal resolution is necessary, and consequently
require expert ears and eyes for the task.

The process gets increasingly expensive on multiple metrics very quickly,
especially in cases of rare phenomena in rare languages. Furthermore, high
standards are imperative to be maintained since these transcriptions will
essentially be the /ground truth/ for data driven models in academic and applied
research.
- [ ] A little more elaboration

The work presented in this thesis is part of a larger project with the ultimate
goals of using machine learning to speed up if not fully automate the processes
involved in such linguistics research. Specifically, this thesis focuses on
detecting and temporally localizing /Double-Talk/ or /Overlapping Speech/ which
occur when more than one speakers speak simultaneously, and can be used as an
incredible meta-conversational metric for analysis. Depending on context and
several other factors, such overlaps can be quite frequent yet
characteristically ephemeral individually, especially in spontaneous
conversations. For one's objectives, this phenomenon can be incredibly difficult
to precisely annotate.

For a yet another group of objectives, double-talk situations can prove to be a
major source of errors. For systems that aim to automatically transcribe and/or
annotate speech signals, high performance on spontaneous conversations
represents their flagship task. Hesitations, overlaps, and other disfluencies
are typical in such unplanned settings, and pose significant challenges to these
speech technologies. In speaker identification and speaker diarization systems,
overlapped segments of speech are referred to as their /Achille's heel/ since
they lead to the underlying models being impure when they are not compensated
for in the training data ~[]~. Furthermore, such segments account for a majority
of errors even in state of the art performance when applied in conversational
settings ~[]~. Similar degradation is also observed in Automatic Speech
Recognition (ASR) where the systems fail at disambiguating the contents of the
spoken terms in regions with overlapping speakers ~[]~.

Precise and Reliable detection of overlapping speech in conversation can thus
play important role in improving existing systems, and not only improve the
workflows for conversation analysis.

The phenomenon of Double-Talk is discussed in more detail in the next section,
which is followed by putting it in perspective of research in speech
technologies and existing work on the task of detecting them. Aided by this
background, the objectives of this thesis are made concrete at the end of this
chapter.

** DOIN [0/5] Double Talk in Conversations
Double talk can be straightforwardly defined by the physical phenomenon where
more than one speakers speak at the same time. Nevertheless, it is pertinent to
informing different decisions in the approach taken to detect them in
conversations that different characteristics of their occurances under such
settings are analyzed beforehand. An approach suitable for detecting overlapping
speech segments that are 2 seconds long may not be suitable in situations where
a majority of overlaps are significantly smaller in duration. Furthermore, these
charactristics, like frequency of occurrence, typical duration, content, etc.
may vary widely based on a variety of factors. For example, competitive
conversations, like those of argumentative nature, may have frequent or rarer,
typically longer or shorter, involve a wider vocabulary or smaller, than
conversations in more cooperative or relaxed environments. Similar differences
can arise when the setting is formal v/s informal, involves 2 v/s more
participants, are carried out face-to-face v/s using a telephone, etc. Finally,
theories in the area of conversation analysis that try to explain these
phenomena can not only motivate one's decisions between different methodologies,
but also help extracting more information from the evaluations.

Before moving forward however, it is important to note that the figures reported
in this section, unless sourced from a different study, were calculated on the
(entire) Fisher corpus ~[]~. Relevant to the current section is that the corpus
is of telephone based informal conversations between english speaking
participants. Due to some factors, certain extrema of different statistics
mentioned here should be taken with some caution. These factors, if they are not
explicitly pointed out here, are discussed in more detail when the dataset is
formally analyzed in the next chapter.

Double talks become interesting in the analysis of turn-taking management. In
the absence of any strict guidelines, participants in a conversation manage when
they speak to avoid speaking at the same time and probably defeating the purpose
of the activity. There are two popular theories that try to model how the
participants find the appropriate moments to start speaking @heldner:2010pauses.
When one speaker is active at a time, according to the first theory ~[]~, the
next speaker projects the possible end-time of the current turn based on
syntactic, prosodic, pragmatic and/or gestural information. The other theory
~[]~ suggests the next speaker starts talking as a direct reaction to a signal
that the current speaker has finished, or is about to finish. Overlaps can occur
when there is a mismatch between predictions by the next speaker and the actual
time the current speaker stops talking. From both perspectives, it is expected
that having contextual information can benefit the approaches attempting to
detect double talk occurrences. Particularly helpful can be information which can capture
or predict based on content, vocalization, or even information from additional
modalities like accompanying video or extra microphones.

#+LATEX: \begin{figure}
# \centering
#+LATEX: \includegraphics[width=0.90\textwidth]{img/example-ovl-conv}
#+LATEX: \caption{Examples of overlap scenarios in conversations}
#+LATEX: \label{fig:example-ovl-conv}
#+LATEX: \end{figure}
#+LATEX: \end{small}


Furthermore, according to both theories, most of the time, a single speaker is
active for varying lengths of time, and turns are taken with (relatively) minimal gap or overlap with
respect to duration. Furthermore, speakership changes occur regularly in
conversations, so instances of double talk can be quite frequent. These can vary
in different scenarios, and are an area of interesting research.

#+LATEX: \begin{small}
#+LATEX: \begin{table}
|-------------+-----------------+---------------------|
| /#Speakers/ | /#Segments (%)/ | /#10 ms Frames (%)/ |
|-------------+-----------------+---------------------|
|           0 |           20.39 |                6.86 |
|           1 |           50.93 |               79.61 |
|           2 |           28.68 |               13.53 |
|-------------+-----------------+---------------------|
#+LATEX: \caption{Overall ratio of active number of speakers in at a time in terms of segments and frames in the Fisher Corpus \emph{LDC2004T19}}
#+LATEX: \label{tab:actspk-all}

#+LATEX: \end{table}
#+LATEX: \end{small}

#+LATEX: \begin{small}
#+LATEX: \begin{figure}
# \centering
#+LATEX: \includegraphics[width=\textwidth]{img/actspk-hist-all}
#+LATEX: \caption{Histogram of durations of segments (truncated to < 4 second long) with different number of active speakers in \emph{LDC2004T19}.}
#+LATEX: \label{fig:actspk-hist-all}
#+LATEX: \end{figure}
#+LATEX: \end{small}

In Table \ref{tab:actspk-all} the ratios of /inferred/ number of segments of
speech and those of number of frames calculated with centers at every 10 ms (the
temporal resolution of the transciptions) over 5850 telephone based
conversations in the /Fisher English/ /Training Speech - Part 1 - Transcripts
(**LDC2004T19**)/ are reported (details in Section [[Dataset]]). It can be
seen that while segments with more than 1 speakers active at the same time
account for a significant portion, they contribute proportionally less to the
overall number of individual frames in time. This indicates, and is further
illustrated in Figure \ref{fig:actspk-hist-all} that the distribution of
segment-lengths with overlapping speech is skewed towards smaller durations.
Overlaps longer than 4 seconds are less common and are often a result of
erroneous transcription and temporal resolution of the annotations. For various
conversational datasets, the reported median value is typically around 0.5
seconds, and the distribution has a very thin tail in longer durations ~[]~.
Knowing this, a system for detecting double talks should work at a high temporal
resolution. For systems that aim to classify each frame for this, additional
challenge will arise due to heavy imbalance between the classes designed based
on number of speakers active at a time.

Literature in the area of linguistics classify overlaps into further categories,
but these are not discussed in this thesis. The reader should refer to
@minna_stolt:2008many for an extensive overview.

# Pertinent to the work here,
# however, would be to analyse the genderwise raios and distributions of the
# occurences of double talk. Since there are necessarily more than one speakers
# active in segments with overlap, and that there can be significant differences
# or lack thereof between the voices in different combinations of genders,
# situations where the two speakers are of the same gender can prove
# more challenging than when they are of different genders. Approaches that target only
# certain combinations cannot be safely assumed to generalize well. This becomes
# especially important when the dataset being used is artificially generated,
# often done to mitigate the class imbalance issue mentioned earlier.
# However, for approaches using natural conversations, as is in the case of this
# thesis, this factor may be beyond their control.

# #+LATEX: \begin{small}
# #+LATEX: \begin{table}
# |----------------+-----------------+---------------+-----------------+---------------|
# | /Gender/       | /Segments with/ | /Frames with/ | /Segments with/ | /Frames with/ |
# | /Combination/  |    /Speech (%)/ |  /Speech (%)/ |   /Overlap (%)/ | /Overlap (%)/ |
# |----------------+-----------------+---------------+-----------------+---------------|
# | Female, Female |           42.18 |         40.12 |           36.30 |         15.23 |
# | Female, Male   |           32.49 |         32.32 |           35.07 |         13.87 |
# | Male, Male     |           25.33 |         27.56 |           36.80 |         14.26 |
# |----------------+-----------------+---------------+-----------------+---------------|
# #+LATEX: \caption{Contribution of different gender combinations to speech (with >= 1 speakers) segments and frames, and the proportion of these segments and frames that have overlap, in the Fisher Corpus \emph{LDC2004T19}}
# #+LATEX: \label{tab:seg-gender-all}
# #+LATEX: \end{table}
# #+LATEX: \begin{table}
# |----------------+------------------------+----------------------|
# | /Gender/       |      /Contribution to/ |    /Contribution to/ |
# | /Combination/  | /Overlap Segments (%)/ | /Overlap frames (%)/ |
# |----------------+------------------------+----------------------|
# | Female, Female |                  42.50 |                42.05 |
# | Female, Male   |                  32.62 |                30.87 |
# | Male, Male     |                  25.88 |                27.06 |
# |----------------+------------------------+----------------------|
# #+LATEX: \caption{Contribution of different gender combinations to all overlaps, in the Fisher Corpus \emph{LDC2004T19}}
# #+LATEX: \label{tab:ovl-gender-all}
# #+LATEX: \end{table}
# #+LATEX: \end{small}

# **LDC2004T19** consists of 5850 calls, all of roughly the same duration, and all
# have conversations between two speakers. Table \ref{tab:seg-gender-all} shows
# that there are more number of calls (and hence segments and frames where at
# least one speaker is active) where both participants are female, and,
# since the ratio of segments and frames that have overlapping speech within each
# combination are roughly the same, the final contribution to all overlap segments
# and frames are similarly skewed \ref{tab:ovl-gender-all}.
Almost all approaches for detecting overlapping speech, including the one
presented in this thesis, do not further categorize examples of overlaps,
especially when working with naturally occuring double talk, mainly to avoid
further decimation of which is already a minority class.

# What the analysis above motivates is to use information that can capture
# differences in the voices of the speakers involved.

A straightforward approach can be to have a system that can identify individual
speakers, as is done in speaker identification systems. This system can then be
used to detect overlaps if it identifies multiple speakers with similar
confidence in a given segment of speech. Unfortunately, in addition to needing
prior information about all possible speakers (which is often not available in
the application phase), so far these systems do not work at high temporal
resolutions. A simple reason for this is that speaker specific information can
only be reliably calculated over long durations.

#+LATEX: \begin{figure}
# \centering
#+LATEX: \includegraphics[width=\textwidth]{img/actspk-whist-all}
#+LATEX: \caption{Weighted Histogram of durations of segments (truncated to < 4 second long) with different number of active speakers in \emph{LDC2004T19}.}
#+LATEX: \label{fig:actspk-whist-all}
#+LATEX: \end{figure}
#+LATEX: \end{small}

High temporal resolution in double talk detection is, nevertheless, necessary.
When the histograms in Figure \ref{fig:actspk-hist-all} are weighted by their
respective durations (Figure \ref{fig:actspk-whist-all}) to indicate the overall
proportional contribution of different segment lengths, double talk segments between 0.5 and
1.5 seconds long are seen to be largest the contributors to overlapping speech.
Applications where the detection and appropriate treatment of overlapping speech
can significantly improve the overall performance, should therefore been keenly
interested in working with such small segments. This has proven to be extremely
challenging so far, and the work for this thesis's objectives is not immune to
it either. Existing works on this and related problems are discussed in the next section.
** Double Talk and Speech Technologies
Natural conversations are one of the toughest scenario where most of the
automated spoken language technologies have to prove their metal ~[] corpuses~
~check @cetin:2006speaker~. In addition to having multiple speakers, the
spontaneous nature of the utterances and their content, and the presence of
hesitations, self-corrections, and other disfluencies (including double talk
situations), make conversations in meetings or other informal settings much more
challenging than planned and/or read speech (e.g. broadcast recordings). Due to
the detrimental impact of overlapping speech on their performance, such systems
often remove them in a pre-processing step, or mitigate for such situations with
the help of extra information. In an automatic speaker identification /
verification system, for example, individual speaker models must be learned on
non-overlapping examples to ensure purity, and the system should be robust
against the presence interfering speakers in real world application, or at least
refuse to assign a speaker by identifying presence of competing speakers. There
has, nevertheless, been relatively few examples in existing publications
attempting detection and temporal localization of double talk situations in natural
conversations.
*** Speaker Diarization
Over the past decade, most of the significant attention to detecting overlapping
speech in conversations has been motivated in /**speaker diarization**/ systems
whose main task is to determine 'who spoke when' in a recording with more than
one speaker. This involves the unsupervised identification of each speaker
within a given audio stream and the intervals during which each speaker is
active. The methods are unsupervised due to lack of prior information about
speaker identities in most application scenarios. These systems find utility in
many audio/video document processing tasks, and are integral to automatic rich
transcription of these documents for a variety of applications, like indexing
and retrieval, etc. For several conversation analysis workflows, an ideal version of such system
is almost perfect, especially when combined with a robust
speech-to-text system that can ultimately answer the 'who spoke when, and what'
question. In fact, speaker diarization can be a very useful preprocessing step
for other speech technologies like Automatic Speech Recognition, speaker
identification, speaker localization, speaker tracking, etc. For example,
ASR systems, which usually only aim to transcribe the spoken content in a given
speech, can use outputs from a speaker diarization step to concentrate on only
the segments with speech, and employ better speaker adaptation techniques (to
compensate for speaker specific variations in the acoustic model) with the help
of information about the speakers present in the recording.

Anguera /et al./ @anguera:2012speaker provide an excellent overview of various
approaches used for speaker diarization. In brief, the general architecture for
most systems (Figure \ref{fig:arch-diarization}) consists of the following
steps:

1) **Preprocessing** the raw audio data to suppress noise, extracting acoustic
   features (like MFCC, LPC, PLP, etc.), removing non-speech frames, and
   performing any other domain specific processing or augmentation (like
   acoustic beamforming when data from multiple microphones is available, etc.).
2) **Speaker Segmentaion** or speaker change detection to end up with speaker
   homogenous segments. When done separately, the most popular approach is to
   use a similarity metric like BIC, KL-divergence, etc. between two adjacent
   windows of relatively small size to determine if they belong to the same
   source. However, in order to avoid the propagation of errors introduced in
   this step any further, most state of the art systems optimize segmentation
   and clustering simultaneously.
3) **Clustering** the same speaker segments based on some acoustic similarity
   (e.g. BIC, KL-divergence, etc.) metric to (ideally) end up with the same
   number of groups as the total number of speakers in the recording. Since the
   total number of speakers is usually not known a priori, most systems rely on
   a heirarchial clustering algorithm, predominently by using a bottom-up
   (agglomerative) strategy where an over-clustered initialization is
   iteratively merged until a stopping criterion (like BIC, KL2, CLR, etc) is
   met. In approaches that unify the segmentation and clustering steps,
   iterative adaptation of speaker models based on current clustering and
   subsequent re-clustering of the assignments based current models is carried out,
   predominently using GMM/HMM based models, and BIC based agglomerative heirarchial
   clustering.
4) Final **labelling** of each frame/segment of the entire recording with the contributing
   speaker cluster (predominently using Viterbi decoding), with possible enforcement
   of minimum turn durations, and any other priors and constraints.

Nevertheless, a fundamental limitation of most of these systems is that they
only assign one speaker to a frame/segment. This leads to missed speaker errors
in segments where multiple speakers are active and, given the high performance
of some state-of-the are systems, can be responsible for a substantial of the
overall diarization error ~[]~. Anguera /et al/ in fact call overlapping speech
the 'Achilles heel' of speaker diarization for the meetings. Presence of such
segments can potentially also degrade the speaker clusters and models of the
involved speakers when they are not excluded in the pre-processing step ~[]~.

The most common approach available in literature employs a separate overlap
detection system whose posteriors about the presence of overlapping speech can
be used in the pre-processing step to remove such segments and then in the
labelling step to signal the need to look for a second contributing speaker. The
latter usually done by choosing more than one speakers based on the diarization
system's posterior probabilities. Other approaches that have been proposed,
which do not use a separate overlap detection system, either do the detection
and re-segmentation of the diarization system's output by adding to the derived
speaker models all the possible combinations of such models ~[]~, or, integrate these
combinations in the speaker modelling and clustering stage of the diarization
system itself ~[]~. These other approaches, however, have not been shown to improve
the overall performance, or require information about the total number of
speakers to be known beforehand, or have only been investigated in situations
where recordings from multiple microphones is present.

#+LATEX: \begin{figure}
# \centering
#+LATEX: \includegraphics[width=0.90\textwidth]{img/example-ovl-conv}
#+LATEX: \caption{General architecture of speaker diarization systems, and various ways to handle overlapping speech}
#+LATEX: \label{fig:arch-diarization}
#+LATEX: \end{figure}
#+LATEX: \end{small}

- One of the earliest approaches to solve for this ...
- use gmm-hmm for three classes : (0T, 1T, 2T)
- investigated various combinations of features and found ...

- other approaches with different feature combinations
  - CNSC
  - prosodic
  - long-term conversational
  - ...
- LSTM for overlap detection

- A common theme has been to prefer high precision over high recall
- because ....
- Another common theme, as evidenced above, is that MFCC's are not enough
  - weird dilemma of speaker agnosticity, vs. not
- Other literature, that focus solely on detecting overlaps suggest more features

*** Cocktail Party problem, and other speech technologies

- Teager-Kaiser

- Other's from that paper

- Nevertheless, limitation is that they are only shown to work good for overlaps
  longer than at least 2 seconds
- using these features to detect overlaps in natural conversations has not been reported
- Either an artificially generated data is used, or their objective task is the
  cocktail party problem.

- cocktail party problem?

- impact of overlaps in ASR systems.
- usually not challenged by overlap, intuitively, because only concerned with
  what is being spoken. A strong language model in addition to a strong AM can
  mitigate the disambiguation due overlap.
- Furthermore, when considered for contribution to WER, their impact is even
  more minimized, since each occurence usually has on a 1 or 2 words.
- Interesting to note that recent report of achieving human parity, including in
  similarity in case of errors, point out the exception in cases of backchannels
- backchannels are ... and often occur in overlapping condition, where the
  listener is not competing

** Deep Learning

- Deep learning technologies have proven to be this strong, and are still being
  investigated. Particularly interesting are end-to-end approaches.
- [ ] Deep learning and LVCSR
- CNN promising for the ability to learn appropriate features
- LSTM for long-term temporal patterns, so far done using HMMs

- Deep learning for diarization is being investigated actively
- DNN based approach proposed ....
- CNN based approach ....
- Still in initial stages.
- use lower level features and the let the Deep network learn the appropriate
  representation. a major motivation for us.
- LSTM based approach for detecting overlapping speech ...

- Make the assistants more natural to converse with, especially with back-channels, etc.
  + [ ] Search for continuous conversation type AI assistant research.

** WAIT [0/0] Objectives and expected challenges                      :cite:
The work done during this thesis is part of a larger project which aims towards
improving the conversational analysis workflows performed in the area of
linguistics. A diarization system, at least an ideal one, is nearly perfect for
the task of automating annotation of conversations which are otherwise done
manually and cost time and expertise. Furthermore, as noted earlier, this system
also assists the speech recognition system in it's automatic transcription task.
As discussed in Section [[Speaker Diarization]], an overlap detection system can
help improve the overall performance of a diarization system, especially when
applied to conversational scenarios containing situations with overlapping
speech. And finally, as motivated in Section [[Double Talk in Conversations]], a
system that can detect double talk situations itself is directly valuable to
conversation analysis. The ultimate goal of the work done in thesis, within this
context, is the automatic detection and temporal localization of double talk
(aka overlapping speech) that occur in natural conversations.

On the technical side, the objectives of this thesis include investigating the
use of deep learning technologies in realizing such an overlapping speech
detection system. As previously motivated, Deep Convolutional Neural Networks
(DCNN's), the particular deep learning technology proposed and investigated in
this thesis, can help avoid the need for manually engineering problem-specific
features (a common theme in the existing approaches), while promising ground
breaking and well generalizable results. The task of detecting overlapping
speech has proven to be extremely challenging and as yet un-solved ~[]~,
especially in case of the short duration ones (which are dominant in most
natural conversations), so it is worthwhile to investigate the powerful promises
of deep learning methods. If successful, the deep neural network could have
learned the appropriate representations from low-level acoustic features that
best discriminate and predict the number of speakers active at a particular time
in an audio recording (0, 1, or more).

Nevertheless, many (informed) decisions need to be made in designing such a
system, from the nature of low-level acoustic features to be used as inputs, to
the configurations of various layers in the deep neural network, how they are
trained and later fine-tuned, etc. Furthermore, the unique challenges posed by
naturally occurring double talk situations, that of short duration and the
consequent imbalance in the dataset with respect to their representation in the
dataset, need to be addressed using different possible approaches. These result
in a combinatorial explosion of avenues that should ideally be investigated in a
comprehensive study, and are especially warranted by the lack of (as of yet) any
existing published work that use Deep Convolutional Neural Networks (DCNN's), the
particular deep learning architecture proposed in this thesis, for the task of
remedying what has been termed the Achille's heel of speaker diarization systems.

Restrictions imposed to the duration of this thesis, which are further taxed by
the amount of computation (power and) time necessary for properly working with
deep learning technologies, limit the number of possible approaches that could
be investigated in the allotted time. Therefore, in addition to prioritizing
more straightforward approaches in the investigations, an important goal of the
work done during this thesis was to also implement an end-to-end pipeline for
speech segmentation research (and perhaps deployment) based on reputed open
source technologies which can be used for further research with related goals.
Work is expected to continue beyond this thesis's duration, and it is hoped that
it can build upon the learnings documented here and the implementation available
to the stakeholders.

The rest of this section formalizes the objectives alongside the expected
challenges that shape them. It is a fair summary of Section [[Methodology]] which
will go into the details of each aspect. These are then evaluted in Section
[[Evaluations]]. Finally, conclusions from this work and possible directions for
future work are discussed in Section [[Conclusions and Future Work]].
*** Setup and Assumptions
- **Acoustic Model:** :: The fundamental assumption of the work done in this
     thesis for detecting double talk situations is that it is a purely acoustic
     phenomenon, and hence, the underlying classification task will only use
     acoustic information (in the form of low-level acoustic features) extracted
     from the audio of a given recording. The audio data will be mono-aural
     where a single stream has speech from all speakers, and no other
     modalities, like spoken content, extra microphones, etc. will be used. This makes the task
     more challenging than a mult-channel/-microphone setup, but also makes the
     solutions more versatile.

- **Dataset Used:** :: All experiments (trainings and evaluations) will be
     carried out on the conversational telephone speech recordings from the
     **Fisher Corpus** ~[]~, as opposed to the standard NIST RT ~[]~ or AMI ~[]~
     datasets used by most other works on this task. The choice is motivated
     (Section ~[]~) by the fact that the Fisher corpus is a much larger dataset
     (necessary in general for deep learning technologies) which has a lot of
     /natural/ double talk situations. Nevertheless, this choice theoretically
     limits the maximum number of active speakers in a detected overlapping
     situation to 2, and further only proves the applicability of the proposal
     here to recordings of telephone based conversations.

- **Acoustic Features:** :: Inspired by recent studies ~[]~, the goal to
     circumvent the need for feature engineering done in related works ~[]~, and
     based on initial experiments, only low-level acoustic feautures will be
     used for training the acoustic model. These, for similar reasons, will be
     fixed to **64 dimensional $log_{10}$ Mel-Filterbank Coefficients /
     Vectors** extracted every **10 ms** over a window of **32 ms** (Section
     ~[]~). Furthermore, for all experiments, each such vector will be
     accompanied by ±10 neighboring frames as contextual information, resulting
     in the input to the classifier having a shape of $(21, 64)$ and the ground
     truth label from that of the frame in the center (Section ~[]~).

- **Supervised Learning:** :: Similar to other approaches ~[]~, the overlap
     detection system will classfy for every frame of the acoustic features
     extracted from an audio recording into 3 classes: /**(0 speakers, 1
     speaker, more than 1 speakers)**/ (Section ~[]~). Consequently, how many
     speakers are active in situations of overlap is not inferred. The
     classifier is to be trained in a supervised setting, employing ground-truth
     labels inferred for speaker activity from the transciption of the audio
     during training and evalution.

- **DCNN based Classifier:** :: The architecture of the deep neural network will
     be that of a Deep Convolutional Neural Network (DCNN), with multiple
     (stacked) convolutional layers at the input to learn appropriately low- and
     high-level patterns, and multiple (stacked) fully connected (dense) layers
     at the output to classify these patterns into the 3 classes mentioned
     above. The architecture of the network was fixed for all experiments to
     evaluate impact of other variables, and was inspired by recent research in
     acoustic modelling with DCNN's and other practical concerns (Section ~[]~).

# - **Evaluation Metrics:** :: As done in previous works ~[]~, the effectiveness of the
#      system will be measured with respect to overlap detection in terms of the
#      frame-wise precision and recall.
#      (Section ~[]~). Nevertheless, since the dataset used for these evaluations
#      is from the Fisher corpus which is not the standard dataset used by other
#      works, the results reported in this thesis cannot be directly compared to those
#      works. Adaptations and evaluations on such datasets are planned to be
#      carried out in the future, but were not included in the objectives of this
#      thesis due to time limitations.
#
# - **Fine-Tuning:** :: Where necessary and possible, any fine-tuning or
#      comparisons of overlap detection systems will be performed by
#      giving preference to higher precision over higher recall (Section ~[]~).
*** Variables
- **Normalization of Inputs:** :: Normalization is a standard step in preparing
     inputs to many machine learning algorithms. However, since the implicit goal
     is to learn appropriately discriminative features for the existence
     overlapping speech, the normalization of low-level features has to be done
     with some care. Impact of two standard approaches for normalizing speech
     signals will be investigated, and compared to the baseline approach without
     normalization (which is common in systems that want to perserve speaker
     discriminative information in the inputs). The two normalization approaches
     to evaluate are Mean Substraction, and further Variance Normalization
     (these and other possible approaches are discussed in Section ~[]~).

- **Presence of Silence:** :: To measure the impact of presence of silence in the
     training data, in addition to the baseline task of working with the 3
     possible classes /**(0 speakers, 1 speaker, more than 1 speakers)**/,
     configurations where such silence frames are removed and the task becomes
     that of a binary classification /**(1 speaker, more than 1 speakers)**/ are
     also evaluated. The silence frames will, however, be removed based on
     ground-truth annotations instead of by using an automated speech activity
     detection system in order to avoid the impact additional variables brought
     in by such an automated system (Section ~[]~).

- **Tackling Class Imbalance:** :: The most potent challenge in detecting
     naturally occurring double talk situations in a supervised machine learning
     framework is the inherent imbalance between the number of examples
     available in the training set to learn from. Dealing with imbalanced
     classes in deep learning technologies is unfortunately not a well
     researched topic. The most promising approach within this context, one
     involving the re-sampling of inputs from different classes while training,
     is compared against the baseline case where no such re-sampling is done
     (these and other possible approaches are discussed in Section ~[]~).

* DOIN Methodology
The essence of the methodolgy used in the work done for this thesis is related
to designing, implementing and evaluating an automatic pattern recognition
system.

The task of detecting and temporally localizing occurrences of double talk (or
overlapping speech) in a /mono-aural audio recording/ of a conversation is
performed by using a /Deep Convolutional Neural Network (DCNN)/ based classifier
trained on (a subset of) the /Fisher Corpus/ ~[]~ working on /low-level acoustic
features/ as inputs.

The choice of using only acoustic information for making such predictions comes
from the fundamental assumption of the work done in this thesis and in other
related works so far ~[]~. This assumption is that the presence of overlapping
speakers can be reliably detected from acoustic information alone. Such an
assumption is obvious to make since human subjects are capable of doing so.
Nevertheless, it is theoretically possible that other modalities like an
accompanying video recording, or a corresponding transcription, etc. could help
by augmenting the available information. But such approaches were not found in
published works, and are necessarily limited to application in situations where
these extra modalities are available. Furthermore, a system that can perform
well by only using the audio of a conversation, which is necessary for any
conversation analysis task, can definitely be applied to conversations that have
an accompanying video recording and/or a transcription available.

Similar arguments explain the restriction to /mono-aural/ audio recordings (and
the sample rate of the audios, discussed in Section [[Dataset]]). Although research
exists where the availability of multi-channel or multi-microphone (or high
sample rate) recordings has been shown to improve results for this task ~[]~,
extra modalities of such types are not available for many situations where the
system has to be ultimately deployed to. Merging multiple channels into one
could be done trivially (although sophisticated approaches exist ~[]~), and a
system that can work reliably under these lowest common denominator settings,
although could pose significant challenges during development, will nevertheless
be ultimately more versatile. However, it must be pointed out that the audio
recordings available in the Fisher Corpus are technically dual channel (1
channel per speaker), but these were merged into one channel following a trivial
and reproducible method before being used for acoustic feature
extraction. More detail on this is available in Section [[Data Preparation and Analysis]].

The choice of the Fisher Corpus was made necessary to appropriately train the
DCNN. As discussed earlier (Section [[Double Talk in Conversations]]), the total
number of examples (in terms of duration) could be very small in a given
conversation, whereas deep learning technologies typically generalize well on
unseen data only after being trained on a large number of samples. Furthermore,
the choice helps avoid various pitfalls involved in generating artificially
overlapped data by having a good amount of natural overlap situations.
Nevertheless, there are not existing works that have used this dataset for this particular
task, therefore the evaluations of the proposed system presented in Section
[[Evaluations]] are not directly comparable to any existing works.

The choice of using a DCNN based classifier itself is motivated by the ambition
to circumvent the need for manual feature engineering that has been prevalent
in previous works. DCNN's have been shown the ability of learning both low- and
high-level representations from minimally processed inputs and achieve
record-breaking performance on multiple occasions in recent years ~[]~. Acoustic
features were still extracted from the audio before being fed into the DCNN
proposed in this thesis, but they were kept to be fairly low-level ones, and the
impact of certain simple pre-processing methods have been experimented with.

All of the above aspects and related challenges are discussed in appropriate
detail in the following sections of this chapter, and some highlights from the
implementation perspective are provided at the end (Section [[Implementation -
Highlights]]). The final application of this work in a speaker diarization system
(introduced in Section [[Speaker Diarization]]) was however not within the purview
of this thesis and is therefore not discussed.
** WAIT Supervised Machine Learning for Classification
The ultimate goal of a /classifier/ is to map a new observation to a category
(or class) given what has been /learned/ from the categorization of perviously
seen observations, where the set of possible categories is finite and predefined
(e.g. whether, or not, more than one speakers are speaking simultaneously at a
given time in an audio recording). When the categorization of previously seen
observations (training data) is known, /supervised learning/ methods use this
information to /train/ the appropriate classifier, whereas /unsupervised
learning/ methods don't have this categorization available (or do not use them)
for such training (e.g. speaker clustering in Section [[Speaker Diarization]]).

In the formulation that was used in this thesis for double talk detection and
temporal localization, the proposed DCNN based classifier predicts the
likelihood of each acoustic feature vector extracted from the audio (at a
particular frame-rate in Hertz) for having /**(0 speakers, 1 speaker, more than
1 speakers)**/, after being trained in a supervised fashion on a representative
dataset with such classes being labelled for. The final decision was then made
by choosing the class that was assigned with the maximum likelihood.

The three classes mentioned above are more or less in line with the ones used by
other works ~[]~. An immediate possible extension would have been to include a
class for non-speech related acoustic events (e.g. bird sounds, etc.), however,
though present in the dataset that was used in this work, such events were not
annotated for at all in the available transcripts, and hence this extra class
was not used. Nevertheless, this means that the results from application of the
learned classifier on recordings with non-speech related events is undefined.

Another possible formulation could have directly predicted the number of
speakers speaking simultaneously at a given instance, instead of lumping all
cases of more than one speakers being active into one class. Such instances,
where more than two speakers are active at the same time, are very rare in most
natural conversations, and, furthermore, were not present in the dataset
(Section [[Dataset]]) that was used for training. Therefore, in a stricter setting,
the decision by the proposed classifier of presence more than one speakers being
active at the same time is only well defined for cases where there are utmost
two speakers active at the same time.

Now, the proposed classifier outputs the likelihoods of every acoustic feature
vector (aka frame) for belonging to one of these three classes. For a particular
frame however, the classifier does not exploit it's predictions for the
neighboring frames (the input to the DCNN classifier consists of neighboring
frames to provide contextual information, but the labels of such contextual
frames are not provided to the classifier while training; more in Section
[[Architecture]]). But, in a speech recording these three events occur as contiguous
segments in time. That is, ~FINISH~
- [ ] FINISH
*** WAIT Temporal Smoothing

*** Removing Silence
It can be argued that the presence of silence frames in the training input can
degrade the performance of the classifier with respect to discriminating between
single speaker frames and overlapping speech frames. Silence, or lack of speech,
can be much more easily discrimnable than speech from any number of speakers,
while discriminating between speech produced by a single speaker and that
produced by multiple speakers simultaneously can, even in isolation, prove
difficult. This may lead to the iterative gradient descent procedure used for
training a neural network getting stuck in a rather steep local minima where the
classifier's objective for detecting silence v/s speech from any number of
speakers could be so well met that moving on to other minima is too expensive.
This becomes even more challenging when the classes are as imbalanced as they
are in the present case (Section [[Double Talk in Conversations]], Section [[Dataset]]).
Even though their are more number of exemplary frames with double talk, these
are also much less discrimnable from single speech frames, which are by far the
dominant class.

In early experiments with simpler neural network architectures that had only fully
connected layers, it was indeed observed that the classifier achieved very high
precision and recall when discriminating between silence and the other two
classes, while the performance was not at all satisfactory in discriminating
between the other two classes.

It is possible, however, that a more powerful network, like the DCNN proposed,
with an order of magnitude more number of learnable parameters, will be able to
overcome this issue. Nevertheless, detecting silence could be performed by much
more easier and robust methods than such a complicated network. In fact, it is
part of the standard procedures for most state of the art speech technologies
(e.g. ASR, Speaker Diarization, Speaker Identification, etc.) to use a speech
activity detector in an early pre-processing step to remove segments with
silence before the audio is passed on to the next steps.

- [ ] Speech activity detection approaches

To study the possible impact that presence of silence can have on the
performance of the classifier, a set of experiments that were performed and have
been reported in this thesis included a configuration where the silence frames
had been removed from the input data during the training and evaluation
procedures. For this, ground-truth annotations were used in order to avoid any
impact on performance that may get introduced by using an automatic speech
activity detector. Under such configurations then, the task becomes that of
binary classification, i.e. /**(1 speaker active, more than 1 speakers
active)**/.

- [ ] Silence is ~not~ removed during evaluation, and is instead set to GT
  before calculating prec_rec
*** Tackling Class Imbalance
It was shown in Section [[Double Talk in Conversations]] that, even though
individual double talk situations can occur quite frequently in normal
conversations, their predominently small duration lead them to have a much
smaller share of the final number of acoustic frames. This imbalance in
representative number of examples available for each class, especially when
detecting the disadvantaged class is the primary goal of the exercise, could
prove devastating when the total amount of training data available is too small.
And can be further exacerbated when there can be significant variations within
the minority class or if the minority class is difficult to distinguish from the
dominant class.

This imbalance is arguably one of the most potent source of issues in detecting
double talk situations in natural conversations, at least when it is to be done in a
frame-wise manner. There are a few approaches that were considered to solve for
this problem during this thesis's work. The most important one was to choose the
Fisher Corpus over other datasets (like AMI, NIST RT, etc.) that have been used
by other studies on detecting overlaps in conversations. The Fisher Corpus is
sufficiently large (Section [[Dataset]]) and most of the telephone conversations in
it have naturally occuring double talk situations. The classes remain
imbalanced, but there are more number individual frames with overlapping speech
available in this dataset than there are /total/ number of frames in some other
datasets of conversations. Quantity has a quality all it's own.

Furthermore, since the dataset has natural conversations, certain pitfalls of
using artificially generated overlapping speech are avoided. For example,
certain vocal events like laughter, or certain utterances like those used as
backchannels (e.g. hmm, yeah, etc.), which often (if not almost exclusively)
occur in natural double talk situations, are difficult to account for while
generating artificially overlapped speech from single speaker utterances. ~There
have been studies ...~

Nevertheless, since the imbalance between classes still exists, other
approaches to mitigate the issue because of it were also investigated, and
are discussed next.

- [ ] Due to time limitations, not all methods could be thoroughly investigated,
  but when done, such approaches formed a yet another set of configurations for
  the experiments, with a baseline experiment where none of these measures were
  used.

***** Rebalancing Training Data
One of the most widely considered approaches to tackle imbalanced classes is to
use a biased sampling strategy for choosing examples from the dataset such that the
classifier sees a balanced representation from each class during training. Such a
goal can be achieved by either under-sampling the examples from the majority
class(es), or over-sampling those from the minority class, or by doing both
simultaneously. Several algorithms exist to carry out such under- or
over-sampling, and in situations where the total size of the dataset is small
more complicated methods might be warranted.

Over-sampling from the examples of the minority class is the more popular
approach taken by many studies where such imbalance in classes exist ~[]~. A
naive argument in favor of this technique is that doing this does not introduce
more information versus undersampling where potentially useful information is
being thrown away. However, an easy counterargument against this technique, at
least when implemented naively by simply duplicating random sets of examples,
and made worse in case of significant imbalance, is
that the variables associated with such an over-sampled class can appear to have
lower variance than they do, and can lead to overfitting of the classifier to
the training set which will not generalize well later on. There exist many
techniques that can solve for this (like SMOTE, etc.) where instead of naively
duplicating examples, new examples from the minority class are created
artificially following some procedure which should not impact the ultimate
classification task. In computer vision tasks, for example, new examples can be
created by flipping, rotating, etc. existing examples, and have shown to improve
performance of the classifier ~[]~.

Nevertheless, over-sampling from speech samples is usually not trivial. Popular
transformations include warping of the feature vectors, etc., but for the task of
detecting overlapping speech, such transformations can potentially negatively
impact the speaker-discriminative information available in the example. A
different approach would have been to simply create artificially overalpped
speech frames, but the problems associated with this approach have been discussed
earlier. Furthermore, given the limitations of hardware and time,
, doing such over-sampling to satisfactorily reduce the disparity between the
classes involved in this thesis would have increased the size of the already
large dataset to impractical proportions, especially in terms of the amount of
training time required. Consequently, taking this approach for
rebalancing the training examples remains a task to investigate in future works.

On the other hand, availability of a large dataset can make the decision to
under-sample from the majority class a more comfortable one. Often times, it is
recommended that such under-sampling should be done in regions of the feature
space which can lead to the most confusion between the majority and the minority
class, as opposed to naive uniform skipping of examples. These will be
boundaries regions between the two classes, and in case of overlapping
situations, the frames near the transition between the segments of single
speaker activity and multi-speaker activity. Unfortunately, due to inaccuracies
in ground truth annotations in most datasets of conversations, sampling solely
from such transition regions could have resulted in the under-sampled class
being represented by bad examples.

Furthermore, a mirrorring argument from earlier can also be put forward against
naieve strategies for under-sampling from all examples of the majority class,
where the under-sampled class can appear to have higher variance in it's
variables than the actual distribution. In large conversational speech datasets,
the biggest source of variance arguably stems from the individual speakers'
characteristics. Care should therefore be taken where each of the speakers in
the dataset are proportionally represented in the results of the under-sampling
procedure that is used. Unfortunately, there is no way to identify a speaker
uniquely in the Fisher corpus (Section [[Dataset]]). What is obviously guaranteed,
however, is that the two speakers within a particular conversation will be
different.

Therefore, the under-sampling that was performed in the experiments in this
thesis were performed on a per-single-speaker-segment basis. All segments with a
only a single speaker speaking were used, and within each, frames were skipped
with uniform probability. This procedure at least approximates the goal that
each individual speaker is proportionally represented over the entire dataset.
Furthermore, the probability of picking a sample was so chosen such that final
subsampled dataset has a ratio of 2:1 between single-speaker and
overlapping-speaker classes. This decision, as opposed to targetting a 1:1
ratio, can be intuitively explained as, given that the Fisher Corpus has a
maximum of two speakers in every conversation and that individual speakers can
be propotionally represented by the above procedure, the final ratios being
1:1:1 between the first speaker, second speaker, and their overlap,
respectively. This is, again, an approximation, and future works should
investigate different parameters to achieve such goals if they worthwhile.

The implementation done to this for this thesis's works performed such
under-sampling on the fly while preparing the inputs for each epoch of training
the DCNN. The uniform probability of keeping a sample from a single speech
segment was fixed at 0.2 for each epoch. No such under-sampling was performed
while choosing samples from overlapping speech class in any experiments.
Furthermore, afforded by the reduced number of total training examples, and
supported by the desire to avoid any impact on convergence, in experiments where
such under-sampling was performed, the neural network was trained for at least
twice as many epochs than experiments where none of the classes were
under-sampled. Lastly, in all configurations where such undersampling was
performed, silence frames were skipped, to avoid the impact of such frames as
has been discussed earlier (Section [[Removing Silence]]).

It must be noted that such under-sampling was only performed while preparing the
training examples, and were not performed on either the validation or testing
examples during evaluations. Silence frames, as noted earlier, were nevertheless
removed during both the training and evaluation phases.
***** Cost Sensitive Objective
While training a classifier, it's parameters are tuned with the objective of
minimizing it's misclassification rate, which is based on measures of the
errors the classifier makes in assigning categories to the inputs.

In cases where misclassifying instances from a particular class can be more
costly, the error measures can be biased by some fixed or derived cost so that such
misclassifications can have a larger impact on the tuning of the parameters of
the classifier. This approach can also be employed in tackling class imbalance,
where the error measures for misclassifying the minority class can be scaled with some
cost (based on some priors) that is higher than the scaling done for the
minority class.

In implementation, for the categorical cross-entropy loss function used for
training the DCNN (Section [[Deep Convolutional Neural Networks]]), experiments were
performed with a fixed cost for overlapping speech class of $2 \times $ the cost
for non-overlapping speech, and also costs based on priors derived from the
training set. In either case, however, the classifier did not converge even
after many times more the number of epochs for other experiments. It is possible
that there were some issues with the particular implementation that was used, or
that both choices of the cost used were inappropriate. One can also argue that
since overlapping speech necessarily consists of speech anyway, associating such
high cost to misclassifying overlapping speech could have misdirected the
classfier's optimizer towards learning simpler patterns and getting stuck in a
steep local minima, resulting in loss of generalizability. Lastly, it is also
possible that the adverse impact of inaccuracies in the ground-truth labels
could have been magnified by using such costs.

Nevertheless, more experiments could not be performed within the time
limitations. The results of many attempts at taking this approach for tackling
class imbalance were considered inconclusive and have not been reported. It will
thus be an open avenue for systematic research in future works where other
objective functions that might be more appropriate ~[]~ should also be
experimented with.
***** Scaling Likelihoods
- http://www.academia.edu/8472416/Tackling_Class_Imbalance_with_Deep_Convolutional_Neural_Networks
- Scale the likelihoods by priors, effectively shifting the threshold.
*** Evaluation Metrics
The imbalance between the classes makes using simple summary metrics for
evaluating an overlap detection system less informative. The mean accuracy, for
example, of a classifier that always assigns the priors of three classes
(typically $(0.20, 0.72, 0.08)$) as the likelihoods for a given sample and then
chooses the maximum likelihood class, will predict every sample to belong to
single-speaker class and still achieve overall $72\%$ accuracy score.

The two types of errors that an overlap detection system can make are: the total
duration of missed overlaps $T_{miss}^{(ov)}$, and total duration of falsely
detected overlap (aka False Alarms) $T_{false}_^{(ov)}$. The summary
metric used for the reporting these errors, made by a system that assigned a total
duration of $T_{sys}^{(ov)}$ as overlaps to a given set of inputs known to have a
total duration of $T_{ref}^{(ov)}$ overlaps, are /precision/ and /recall/.

/Precision/ ($P^{(ov)}$) is the proportion of times that the overlap detection system
was correct in it's decision with respect to all of it's decisions of existence
of overlap, calculated as:
$$P^{(ov)} = \frac{T_{sys}^{(ov)} -
T_{false}^{(ov)}}{T_{sys}^{(ov)}} = \frac{T_{ref}^{(ov)} -
T_{miss}^{(ov)}}{T_{sys}^{(ov)}}$$.

/Recall/ ($R^{(ov)}$) is the proportion of times that the overlap detection system was
correct in it's decision with respect to known amount of times overlaps actually
were present in the input, calculated as:
$$R^{(ov)} = \frac{T_{sys}^{(ov)} -
T_{false}^{(ov)}}{T_{ref}^{(ov)}} = \frac{T_{ref}^{(ov)} -
T_{miss}^{(ov)}}{T_{ref}^{(ov)}}$$.

Some related works on overlap detection system ~[]~ also report the $F-measure$,
which is the harmonic mean of precision and recall defined above, but it seemed
redundant and has not been reported in this thesis.

Precision and recall, being ratios with value between 0 and 1, will be reported
as percentages ($\%$) in this thesis. A perfect overlap detection system will
then achieve $100\%$ on both $P^{(ov)}$ and $R^{(ov)}$, while the classifier
from the toy example above will achieve a $0\%$ on both the metrics. Real
classifiers, however, are not this perfect in being good or bad.

In most practical scenarios, there will be direct tradeoff between being able to
detect most overlap situations (recall) versus being precise about these
detection (high precision). In situations where the classifier can be tuned to
prefer one over the other (e.g. by moving the decision threshold), almost all
studies ~[]~ that use an overlap detection system in a speaker diarization
system prefer higher precision (with possibly low recall) over higher recall
(with possibly low precision). False alarms (low precision) directly impact the
diarization performance of the system since the extra speakers that will be
predicted in such situations (Section [[Speaker Diarization]]) will certainly be
errorneous decisions. Missed overlaps (low recall) will result in missing chance
to predict extra speakers in a segment of speech, but then the system
performance in such a situation will be atleast equivalent to one that does not
use overlap detection at all.

Furthemore, for conversation analysis, a high precision detection of overlapping
situations can at least detect the time points that a linguist can later
concentrate on to annotate manually. Too many false alarms may prove frustrating
in this situation. Therefore, where necessary and possible, a system (operating
point) with high precision will be preferred.

Similar precision and recall metrics were also used for the silence class
(as $P^{(no)}$ and $R^{(no)}$) and the single speaker speech class (as
$P^{(sp)}$ and $R^{(sp)}$) when all three classes were being classified for, and
will be reported appropriately (Section [[Removing Silence]]).
** WAIT Dataset
The dataset that should be used in training (and evaluating) a classifier should
be representative of goals the of the task.

As has been mentioned earlier, for
the task of building an overlapping speech detection system, datasets like AMI,
NIST RT, ~[]~ ~and others~ have been used by most of the previous works ~[]~.
These corpora are made up of annotated audio (and sometimes also video)
recordings from different meeting scenarios where the number of participants in
a particular recording can be between 4 (most common) to 11 (maximum). The
audios are usualy recorded from multiple microphones, placed near each speaker
and/or on a table shared by the participants. These corpora represent the
flagship scenarios in which state of the art diarization systems are evaluated.

Pertinent to the task of detecting overlapping speech, since these recordings
are of natural conversations, there are a fair number of examples of the
relevant situations available in most such recordings (in fact, in some cases
there could be as many as 4 speakers active simultaneously). These corpora are
thus suitable for training an overlapping speech detection system.

Nevertheless,
- [ ] ~Explain why Fisher was chosen.~
*** WAIT [0/9] About
The Fisher English training corpus ~[]~ was made available by ~LDC~ in two parts
in 2004 and 2005, catalogued as **LDC2004S13** and **LDC2005S13** containing
speech data and **LDC2004T19** and **LDC2005T19** containing the corresponding
transcripts. Taken as a whole, the corpus is made up of $11,699 (= 5850 + 5849)$
recorded telephone conversations, each given a unique 5-digit ~CALLID~, starting
from ~00001~. The corpus is predominently used in conversational speech
recorgnition systems in literature ~[]~. For this thesis, only data from the
first part (**LDC2004S13** and **LDC2004T19**) were used.

Each conversation is upto 10 minutes long, and is carried out between two
participants in English on a provided topic. Over 12,000 participants were
initially recruited, including both native and non-native speakers of English,
and each were assigned a unique ~PIN~. However, due to the procedures used while
collecting these recordings, it is not guaranteed that the same PIN in different
conversations represent the same speaker. Therefore, it is also not possible to
determine exactly how many unique speakers are present in the entire dataset.
Nevertheless, it is obviously guaranteed that the two speakers within a
conversation are not the same.

The audios are available in NIST SPHERE format containing two channels (one
channel dedicated to each speaker's side in the telephone conversations) sampled
at 8,000 Hertz. The corresponding transcripts are available as plain text files
(example in Figure ~[]~) alongside a separate database with information about
the recording situation and the speakers for each conversation.

The transcripts were created by first performing automatic speech detection on each
channel (independently) of the audio data to identify start- and end-points (in
seconds) of utterances in that channel, and then the spoken content of these
utterances were transcribed manually. The final transcript file then has one
line per utterance, with start- and end-time stamps and the corresponding
channel/speaker as "A:" for channel 1, "B:" for channel 2 (Figure ~[]~). The
maximum resolution (theoritically minimum utterance length) of these time stamps
is 10 milli-seconds, which govern the frame-rate of the acoustic feature
extraction process (Section [[Acoustic Features]]) and how various statistics are
duration related statistics are reported in this thesis.

It has been explicitly pointed out in the documentation that no manual attempts
were made to modify the automatically derived utterance boundaries, leaving the
possibility that the start- and end-points may not be as precise as would have
been possible if done manually, or even by a more recent speech detection approaches.
The characteristics of the automatic speech detector that was used in the
transcription process have unfortunately not been discussed in the
documentation. Such lack of precision does not impact speech-to-text systems,
the primary intended user of this dataset, since the outputs of such systems do
not need to be localized in time. But for systems that do want to localize
different events in the audio, these imprecisions could adversely impact the
final evaluation results. It was indeed observed for the systems proposed in
this thesis that the predicted segment boundaries for overlapping speech were
sometimes more accurate than the boundaries derived from ground-truth,
especially in cases where the derived ground-truth labels were longer than 4
seconds (Section [[Evaluations]]). It would have been impractical to perform speech
detection properly again on the entire dataset to get more precise boundaries due to time
and resource limitations, and thus such a process was not performed during this thesis.
*** Data Preparation and Analysis
Each NIST SPHERE format audio file was first converted to two-channel WAV
format using the ~sph2pipe_v2.5~ utility provided by LDC ~[]~. These two channels
contain speech from one speaker's side in the conversation, and were merged
into a final single-channel WAV file using ~FFMPEG~ by giving both channels
equal weights. The sample rate of the audio files during all these steps were
kept to the original value of /8000 Hz/.

The class label for speech from 0 speakers, 1 speaker, or more than 1 speakers
for a given time stamp in the audio was derived based on if utterance from none
of the channels, only 1 of the channels, or both the channels were,
respectively, present at the particular time stamp in the corresponding
transcript.

The entire dataset of 5850 calls (~960.3 hours) was then split to make training,
validation and testing sets. The validation set was only used during the
training phase for monitoring or experimentation, but were not used for training
or evaluation. Final evaluations that have been presented in this thesis were
done on the testing set. Since there were no obvious correlations between the
~CALLID~'s (which order the conversations in the dataset) and any properties
pertinent to the task of overlap detection, for simplicity, these sets were made
in a sequential order as follows: the first 99 calls (~16.71 hours, 1.74%) were
assigned to the validation set, next 4,000 calls (~652.85 hours, ~68.00%) to the
training set, and the last 1,751 calls (~290.74 hours, ~30.26%) to the testing
set.

#+LATEX: \begin{small}
#+LATEX: \begin{table}
|------------+-------------------------+----------+------------+---------|
| /Set/      | ~CALLIDs~               | /#Calls/ | /Duration/ | /Ratio/ |
|            |                         |          |  /(hours)/ |   /(%)/ |
|------------+-------------------------+----------+------------+---------|
|------------+-------------------------+----------+------------+---------|
| Validation | ~{00007, 00013, 00028,~ |       99 |       1.34 |    0.48 |
|            | ~00062, 00065, 00069,~  |          |            |         |
|            | ~00086}~                |          |            |         |
|------------+-------------------------+----------+------------+---------|
| Training   | ~{00100, 00101, ...~    |     1200 |     188.22 |   66.97 |
|            | ~..., 01298, 01299}~    |          |            |         |
|------------+-------------------------+----------+------------+---------|
| Testing    | ~{05300, 05301, ...~    |      551 |      91.48 |   32.55 |
|            | ~..., 05849, 05850}~    |          |            |         |
|------------+-------------------------+----------+------------+---------|
|------------+-------------------------+----------+------------+---------|
| Total      |                         |     1850 |     281.04 |  100.00 |
|------------+-------------------------+----------+------------+---------|
#+LATEX: \caption{Final list of calls used to form different sets, their total durations and ratios.}
#+LATEX: \label{tab:splits}
#+LATEX: \end{table}
#+LATEX: \end{small}

However, due to hardware and time limitations, the final sets that were actually
used during the works in this thesis were only subsets from the initial
assignment above. The final calls that were used in each set are shown in Table
\ref{tab:splits}. The particular choice of validation calls was made to control
for gender ratios and certain other properties, while the first 1,200 calls from
the originally assigned training set, and the last 551 calls from the originally
assigned testing set were chosen for the respective final sets. It is noteworthy
that the even this subset of training and testing data used is almost $20
\times$ larger than such sets used in other studies ~[]~, affording a good
amount of training examples and, hopefully, more thorough evaluation.

Table \ref{tab:actspk-val} shows the ratios of the segments and the total
durations with different number of active speakers for the final validation set.
Figure \ref{fig:actspk-hist-val} shows the histogram of propotional (weighted by
duration) contributions of segments of different lengths to the total set of
segments with respective number of speakers active in the validation set. Table
\ref{tab:actspk-trn} and Figure \ref{fig:actspk-hist-trn} do the same for the
final training set, and Table \ref{tab:actspk-trn} and Figure
\ref{fig:actspk-hist-trn} do it for the final testing set. It can be seen that,
as discussed in Section [[Double Talk in Conversations]], 0.5 seconds to 1.5 seconds
long segments contribute the most the overlapping speech situations. Particular
peaks could depend on the nature of the dataset, but overall characteristics of
the data matche what has been reported for other conversational datsets ~[]~.

#+LATEX: \begin{small}
  #+LATEX: \begin{table}
    |-------------+-----------------+---------------------|
    | /#Speakers/ | /#Segments (%)/ | /#10 ms Frames (%)/ |
    |-------------+-----------------+---------------------|
    |-------------+-----------------+---------------------|
    |           0 |           15.41 |                6.25 |
    |           1 |           50.59 |               77.77 |
    |           2 |           34.00 |               15.98 |
    |-------------+-----------------+---------------------|
    |-------------+-----------------+---------------------|
    |       Total |          100.00 |              100.00 |
    |-------------+-----------------+---------------------|
    #+LATEX: \caption{Overall ratio of active number of speakers at a time in terms of segments and frames in the final \textbf{validation set}.}
    #+LATEX: \label{tab:actspk-val}
  #+LATEX: \end{table}

  #+LATEX: \begin{figure}
    #+LATEX: \includegraphics[width=\textwidth]{img/actspk-whist-val}
    #+LATEX: \caption{Histogram of durations of segments (truncated to < 4 second long) with different number of active speakers in the final \textbf{validation set}.}
    #+LATEX: \label{fig:actspk-hist-val}
  #+LATEX: \end{figure}
#+LATEX: \end{small}


#+LATEX: \begin{small}
#+LATEX: \begin{table}
|-------------+-----------------+---------------------|
| /#Speakers/ | /#Segments (%)/ | /#10 ms Frames (%)/ |
|-------------+-----------------+---------------------|
|-------------+-----------------+---------------------|
|           0 |           29.18 |               14.26 |
|           1 |           50.44 |               76.04 |
|           2 |           20.37 |                9.70 |
|-------------+-----------------+---------------------|
|-------------+-----------------+---------------------|
|       Total |          100.00 |              100.00 |
|-------------+-----------------+---------------------|
#+LATEX: \caption{Overall ratio of active number of speakers at a time in terms of segments and frames in the final \textbf{training set}.}
#+LATEX: \label{tab:actspk-trn}
#+LATEX: \end{table}
# #+LATEX: \end{small}

# #+LATEX: \begin{small}
#+LATEX: \begin{figure}
# \centering
#+LATEX: \includegraphics[width=\textwidth]{img/actspk-whist-trn}
#+LATEX: \caption{Histogram of durations of segments (truncated to < 4 second long) with different number of active speakers in the final \textbf{training set}.}
#+LATEX: \label{fig:actspk-hist-trn}
#+LATEX: \end{figure}
#+LATEX: \end{small}

#+LATEX: \begin{small}
#+LATEX: \begin{table}
|-------------+-----------------+---------------------|
| /#Speakers/ | /#Segments (%)/ | /#10 ms Frames (%)/ |
|-------------+-----------------+---------------------|
|-------------+-----------------+---------------------|
|           0 |           17.48 |                4.81 |
|           1 |           51.58 |               81.20 |
|           2 |           30.93 |               13.99 |
|-------------+-----------------+---------------------|
|-------------+-----------------+---------------------|
|       Total |          100.00 |              100.00 |
|-------------+-----------------+---------------------|
#+LATEX: \caption{Overall ratio of active number of speakers at a time in terms of segments and frames in the final \textbf{testing set}.}
#+LATEX: \label{tab:actspk-tst}
#+LATEX: \end{table}
# #+LATEX: \end{small}

# #+LATEX: \begin{small}
#+LATEX: \begin{figure}
# \centering
#+LATEX: \includegraphics[width=\textwidth]{img/actspk-whist-tst}
#+LATEX: \caption{Histogram of durations of segments (truncated to < 4 second long) with different number of active speakers in the final \textbf{testing set}.}
#+LATEX: \label{fig:actspk-hist-tst}
#+LATEX: \end{figure}
#+LATEX: \end{small}

** DOIN Acoustic Features
- [[http://recognize-speech.com/][The Speech Recognition Wiki]]
- [ ] The values are normalized at the time of feature-extraction to be in
  range (-1, 1) and mean 0 when getting input to feature extraction. Done by =librosa.load=.
- [ ] Mention that we only export parts of the audio that are within =min-start=
  and =max-end=, although we actually do it before feature extraction on the
  read =numpy-data= later on.

*** Acoustic Features
- 64 dim MFB also allow for using different orig samplerate audio since all will
  end up as 64 dim anyway ...
- [ ] will have to argue on why not spectrogram, so cite recent works by
  @deng:2013recent ... maybe
  + [ ] /maybe/ and why log
- [ ] add to thesis plot of mel-frequencies
**** Others
- [ ] look at links on MFCC, iVectors, prosody, pitch, CNSC, PLP, etc. for inspiration.
  + [ ] Look at these links from Todoist
    * [[https://en.wikipedia.org/wiki/Pitch_(music)][Pitch (music) - Wikipedia]]
      - [[http://www.fon.hum.uva.nl/praat/manual/Intro_4_1__Viewing_a_pitch_contour.html][Intro 4.1. Viewing a pitch contour]]
      - [[http://librosa.github.io/librosa/generated/librosa.core.piptrack.html#librosa.core.piptrack][librosa.core.piptrack — librosa 0.5.0 documentation]]
      - [[http://www.let.uu.nl/uilots/lab/courseware/phonetics/basics_of_acoustics_1/praat_pitch.html][praat_pitch]]
    * [[https://github.com/timmahrt/ProMo][timmahrt/ProMo]]
    * [[http://www.nature.com/articles/ncomms13654][Rapid tuning shifts in human auditory cortex enhance speech intelligibility]]
    * [[https://www.kaggle.com/primaryobjects/voicegender][Gender Recognition by Voice | Kaggle]]
    * iVectors : [[http://pydoc.net/Python/bob.spear/1.1.8/spear.utils/][Python bob.spear package v1.1.8, spear.utils module source code :: PyDoc.net]]
    * [ ] Fractal Dimensions, check zotero
- [ ] See if the argument that 'spectrogram' is the mother of all features
  still holds.
*** Chunking (?)
*** Normalization
- Main reason: compensate for mismatch in training and testing conditions,
  especially related to channel distortions.
- Also: Make it more suitable for the machine learning algorithm.
** Deep Convolutional Neural Networks
The use of a deep learning based classifier was motivated in Section [[Deep
Learning]]. Ground breaking results have been achieved using these methods in
almost all areas of maching learning. For detecting overlapping speech in
conversations, Geiger /et al/ ~[]~ used Long Short-Term Memory based Recurrent
Neural Networks to achieve comparable performance to the traditional method
using GMM-HMMs, and improved the results further when they were used in a tandem
setting (Section [[Speaker Diarization]]).

As discussed earlier, almost all previous works on overlap detection, including
the work by Geiger /et al/ ~[]~ mentioned above, have concentrated on finding
the right set of acoustic features that give the best performance. Deep learning
technologies, especially Deep Convolutional Neural Networks (DCNN), promise the
capability to /automatically learn/ robust representations from low-level features
that are most appropriate for a given task. And recent results ~[]~ have
demonstrated that these promises are being satisfactorily fulfilled.
Particularly interesting are the studies where neural network architectures
originally designed for one problem domain (e.g. computer vision) have shown to
perform surprisingly well in other domains as well (e.g. speech recognition)
~[]~.

It was for these reasons that a Deep Convolutional Neural Network (DCNN) was
used in this thesis to build the classifier. No works that have used DCNN for
overlap detection in conversations have been published, so it was made necessary
that the investigation be a comprehensive one. The basic approaches that need to
be evaluated in building an overlap detection system are already numerous
(Section [[Supervised Machine Learning for Classification]]), and the long training
times that any sufficiently deep neural network requires would prove taxing to
the limited time allotted for this thesis. It would have been impractical to
then also investigate different DCNN architectures, or even the impact of
different hyper-parameters or the ordering of individual layers in even a single
one. The DCNN architecture that was used then, therefore, was fixed for
evaluations performed in this thesis, while fine-tuning and experimentation with
other variations would continue in the works beyond this thesis's submission.

This DCNN architecture is presented in the next section where each of it's
components are also briefly discussed. Most of these components are fairly
standard in any DCNN, and since almost each of them can warrant a scholarly
article, their discussion in this thesis has been kept brief. The reader is
encouraged to follow many freely available resources for deeper discussions ~[]~.

*** Architecture
Figure ~[]~ shows the final DCNN architecture that was used in this thesis. It
was decided upon after some initial experiments, and was limited by what seemed
reasonable for the given task and by available hardware and time. There are a
total number of 572,035 trainable parameters, which were trained using the
/adam/ optimizer (a popular variant of batched stochastic gradient descent) on
the /categorical cross entropy/ (objective function) between the output
likelihoods and the categorical ground-truth labels.

This architecture is a heavily simplified version of VGG-net ~[]~ ~which won the
...~ and a recent investigation of using VGG-net for speech recorgnition by Deng
/et al/ ~[]~. The network was simplified to have fewer layers and hence much
fewer number of learnable parameters than the original ~85 million. This
decision was made since, in addition to other practical reasons, the network was
to be trained for only 3 classes (Section [[Supervised Machine Learning for
Classification]]) as opposed to in the order of 1,000's in the case of the
original purposes of VGG-net. Early experiments with even smaller architectures,
or even those without any convolutional layers, had not shown satisfactory
results.

The DCNN consists of three /convolutional blocks/ at the input, followed by 3
/dense blocks/ before the final /softmax/ output layer for the three
classes /**(0 speakers, 1 speaker, more than 1 speakers)**/. No padding was
performed on inputs or outputs of any layers, so the depth of the network (i.e. the
number of blocks and layers) is also partly constrained by the shape of the /input/.

The **inputs** are formed for a given frame by attaching 10 frames immediately
before and 10 frames immediately after as contextual information. This leads to
a 2D input of the shape $(21, 64)$ where the two dimensions are /(time,
mel-frequency)/, and, since these frames were calculated every 10 ms, represent
a total contextual information of 210 ms available to the DCNN for each
classification. The **class label** for each such input is decided by the frame
at the center, and is provided to the network in categorical form (3 dimensional
vector with value 1 for true class and 0 otherwise). Other works with DCNN in
speech recognition prepare their inputs in a similar manner to capture a typical
phoneme length, with the amount of context added between 5 to 9 from both sides
being popular ~[]~. The decision to choose 10 contextual frames was made after
determining that lower number of such frames lead to relatively worse results in
initial exploratory experiments, while larger numbers would have imposed severe
penalty on training times as well as, given the typically small duration of
segments with overlapping speech, would have made the decision to choose label
from only the center frame questionable. Within a context size of 210 ms, it is
hoped that enough information can be captured also about acoustic events which
could last longer than a typical phoneme (like laughter, backchannels, etc.) but
nevertheless occur in overlap situations, and also to somewhat mitigate for any
inaccuracies the ground-truth annotations.

Each **convolutional block** consists of a /2D convolutional layer/,
/batch-normalization layer/, non-linear /activation layer/, /drop-out layer/,
and finally a /2D max-pooling layer/, in that order. The max-pooling in the
final convolutional block is performed globally so that the inputs to the
following /dense block/ are vectors of the same size as the number of filters
learned in the final convolutional block. Each of the **dense block** then has a
/densely connected layer/, followed by an /activation layer/ and then a
/drop-out layer/. The final outputs in the form of per-class likelihoods are
obtained from the appropriately sized /dense layer/ with /softmax/ as the
activation function. In total, then, there are 7 layers (excluding
batch-normalization layers) with learnable weights. These layers are discussed next.
*** Layers
**** Convolutional Layer
The use of /convolutional/ layers gives a Deep /Convlutional/ Neural network
it's name. Within this layer, a set of filters (of size smaller than the inputs)
are /convolved/ over the inputs ~[]~. Each filter of a given size (called it's
receptive field) is moved over the entire input with some striding ratio (aka
step-size), and the output at a position is the weighted sum of the overlapped
region of the input at the corresponding position. These multiplicative weight
and additive bias parameters are learned during training after having been
initialized with some appropriate values. In this way, each filter produces a
feature map from a given input, which, in crude terms, represents the presence
of the /local patterns/ that the filter has learned to distinguish from others.
The number of such filters learned in a layer are increased as the position of
the convolutional layers goes deeper (Figure ~[]~), with the intuition of
learning fewer lower level features (e.g. utterance specific) at the beginning
which are combined to form more number of higher-level features (e.g. speaker
specific) as the network goes deeper.

All convolutional layers in the proposed architecture used filters with
receptive field of shape $(3, 3)$, i.e. 2-dimensional convolution, and also used
a striding ratio of $1$ in both dimensions. The hope is that these filters will
learn the temporal and spectral patterns that occur during the three situations
where different number of speakers are active, while also being invariant to the
particular characteristics of what is being spoken, by whom, and when.
Furthermore, since none of the layers performed any padding to their inputs
before convolution, the final size of each feature map will be reduced by an
absolute value of $2 \times \lfloor{\frac{3}{2}}\rfloor = 2$ in each dimension.
No weight or bias regularization (often done to improve stability) were done.

**** Batch-Normalization Layer
In recent works using DCNN's ~[]~, the output of a convolutional layer is often
normalized to have zero-mean and unit-variance. During batch-wise training, this
/batch-normalization layer/ performs this in an online manner, learning the
mean and variance of the entire dataset at the particular position of the layer
for each component of it's input. During evaluation, the learned mean and
variance are used to normalize the inputs to the next layer. The goal is to
standardize the internal representations inside the network (similar to how the
inputs were normalized in Section [[Normalization]]), and have been
shown to help the network converge faster and generalize better ~[]~. To enable
such a layer to learn the mean and variance statistics more robustly in a
batch-wise online manner, care should be taken when the batches are constructed.
For this, the batch-size was kept relatively large and it was made /sure/ (upto
a random decision function) that consecutive batches did not come from the same
recording (while being shuffled within themselves anyway).

It is a subject of many informal debates within the deep learning community on
whether such a normalization should be performed on the outputs of the
convolutional layer, or the /activation layer/. Unfortunately, there have not
been any thorough investigations into the impact of the two decisions. It is the
personal view of the author of this thesis that using batch-normalization before
activation may lead to loss of potentially useful information especially when
using Rectified Linear Unit (ReLU) as the activation function. This is because
ReLU is inactive for values $<= 0$, and batch-normalization will necessarily
center all it's inputs around zero. Nevertheless, the decision was made to honour
the original architecture that inspired this work's DCNN, and
batch-normalization was always performed before activation. Further works are
planned to investigate the other options, including ones skipping this layer altogether.

**** Activation Layer
The /activation layer/, essentially, applies a non-linear function to it's
inputs while sometimes promising certain properties for it's outputs as well.
The application of a non-linear activation function in the neural units
(neurons) to the weighted sum of it's in coming values is what lies at the heart
of what makes a neural network capable of learning universal functions, both
theoretically ~[]~ and, with recent ground-breaking results, probably also in
practice. Such an activation function should be non-linear, bounded, and
monotonically increasing, preferably continuous functions. Traditionally,
functions like the logistic (sigmoid) function $f_{sigmoid}(x) = \frac{1}{1 + e^{-x}}$, or the
hyperbolic tangent function $f_{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$, etc.
have been used. Recently however, the Rectified Linear Unit (ReLU) function
$f_{relu}(x) = x^{+} = max(0, x)$ has become a very popular choice for most applications, and neural
networks using these have been shown to learn much faster than with others.
However, being non-continuous, weights of the neuron should be initialization
with some care ~[]~. Furthermore, they are also known to lead to overfitting, but
certain regularization layers have been shown to mitigate such issues ~[]~.

All weights in the proposed network here were initialized using the Glorot
Uniform distribution ~[]~ based on other recent works ~[]~. /Drop-out layers/
were used during training for regularization (discussed later).
All activation layers in the proposed network used the ReLU function for
activation, except the final layer that produced the likelihoods for the three
target classes. For this layer, the Soft-max activation function was used, which
is a generalization of the logistic function above, but promises the
$C$ -dimensional output vector for a $C$ -dimensional real-valued input vector
$\textbf{x}$ to be real valued in the range $[0, 1]$ and sum up to $1$,
simulating a discrete probability mass distribution over the target classes:
$$f_{softmax}(\textbf{x}) = \frac{e^{x_j}}{\sum_{k=1}^{C} e^{x_k}} \text{ for }
j=1, 2, \ldots, C$$.
**** Drop-Out Layer
Randomly dropping out (setting to zero) the inputs (with some fixed probability)
before being passed on to the next layer was introduced as an elegant
architectural "hack" to prevent overfitting in neural networks, and also for
approximating the ensemble training (with model averaging) of exponentially many
"thinned" networks within the same neural network architecture ~[]~. It acts as
a regularization method by preventing complex co-adaptation of the neural units
to fixed, simple patterns. When applied to the inputs themselves, it can also
simulate training on an exponential number of augmented datasets, although this
configuration is rarely used and reserved for situations when the original
training dataset is small and meets certain criteria ~[]~.

In the proposed architecture, all ReLU activation layers were followed by a
drop-out layer with $10\%$ of the inputs to such layers being randomly set to
zero. It is arguable that a higher probability should be used in the /dense
layers/ (the classifier) to make them more robust, but this has been left for
future investigations.
**** Max-Pooling Layer
Another hallmark of many DCNN's is the use of pooling layers that perform a type
of non-linear down-sampling on their inputs. They are implemented in a very
similar fashion to the convolutional layers, except for two key differences: the
filters are applied to non-overlapping receptive fields instead of overlapping
ones in case of convolutional layers (i.e. the step size is equal to filter size
instead of $1$), and the output of the filters are not weighted sums of the
inputs within the receptive field, but rather the result of a non-linear
function, popularly the max or the average. Furthemore, the number of such
filters is kept equal to the number of filters (output feature maps) in the
earlier convolutional layer. The intuition behind using such a layer is that the
exact location of a recognized pattern in the inputs is less important than its
rough location relative to other recognized patterns, leading to a form of
translation invariance. Furthermore, with such a high striding ratio (or step
size), the size of the learned representation is significantly reduced, leading
to fewer computations in subsequent layer while also helping against
overfitting in the previous layer ~[]~.

When applied globally, the chosen non-linear function is applied to a receptive
field of size equal to the shape of the input feature maps, effectively
replacing each feature map by a single value.

In the proposed architecture, all pooling layers applied the max function.
Except for the final global pooling layer, all other pooling layers use a 2D
filter of shape $(2, 2)$, applied with a striding ratio $(2, 2)$ (equivalent to
a step size of $(2, 2)$). The final global max-pooling layer of course used a
filter size equal to the shape of the input feature maps, making the striding
ratio irrelevant. The other popular option of using the average function, or a
different combination/location in the network, is alas left for future works.
**** Dense Layer

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
c1_3_64_1 (Conv2D)           (None, 19, 62, 64)        640
_________________________________________________________________
c1_bn (BatchNormalization)   (None, 19, 62, 64)        256
_________________________________________________________________
c1_relu (Activation)         (None, 19, 62, 64)        0
_________________________________________________________________
c1_d_10 (Dropout)            (None, 19, 62, 64)        0
_________________________________________________________________
c1_mxp2_2 (MaxPooling2D)     (None, 9, 31, 64)         0
_________________________________________________________________
c2_3_128_1 (Conv2D)          (None, 7, 29, 128)        73856
_________________________________________________________________
c2_bn (BatchNormalization)   (None, 7, 29, 128)        512
_________________________________________________________________
c2_relu (Activation)         (None, 7, 29, 128)        0
_________________________________________________________________
c2_d_10 (Dropout)            (None, 7, 29, 128)        0
_________________________________________________________________
c2_mxp2_2 (MaxPooling2D)     (None, 3, 14, 128)        0
_________________________________________________________________
c3_3_256_1 (Conv2D)          (None, 1, 12, 256)        295168
_________________________________________________________________
c3_bn (BatchNormalization)   (None, 1, 12, 256)        1024
_________________________________________________________________
c3_relu (Activation)         (None, 1, 12, 256)        0
_________________________________________________________________
c3_d_10 (Dropout)            (None, 1, 12, 256)        0
_________________________________________________________________
gmxp (GlobalMaxPooling2D)    (None, 256)               0
_________________________________________________________________
f1_512_relu (Dense)          (None, 512)               131584
_________________________________________________________________
f1_d_10 (Dropout)            (None, 512)               0
_________________________________________________________________
f2_128_relu (Dense)          (None, 128)               65664
_________________________________________________________________
f2_d_10 (Dropout)            (None, 128)               0
_________________________________________________________________
f3_32_relu (Dense)           (None, 32)                4128
_________________________________________________________________
f3_d_10 (Dropout)            (None, 32)                0
_________________________________________________________________
sfmx (Dense)                 (None, 3)                 99
=================================================================
Total params: 572,931
Trainable params: 572,035
Non-trainable params: 896
_________________________________________________________________

*** Training
- adam and backpropagation
- Categorical cross entropy
  - using cost sensitive objective
- No early stopping, no learning rate scheduling
- 20 passes at least, then choose best based on overfitting
- Keras and Tensorflow, with custom data provider
  - ~5 days for full, 3 to 4 days for skipping / subsampling
- monitored for by the class-wise precision and recall using a custom implementation
** Implementation - Highlights
* Evaluations
+ Initial experiments were done on simple FFNN, but were not successful
* Conclusions and Future Work
- Context helps. Single frame is not enough for DT.
- Deep learning relies heavily on the optimizer.
- Summary statistics can definitely be misleading.

** Future Work
- Evaluate on different dataset
- More context?
  + Hardware limitations.
- More/better features.
  + If Fbanks do in fact work better, then, more hand-tuned ones as well.
- More complicated neural networks.
  + Bigger size ones.
  + Heirarchial model.
  + LSTM.
- Use language model.

* Bibliography

* Workflows                                                        :noexport:
[[https://bitbucket.org/motjuste/masters][This repository on BitBucket]]
** org-mode setup

Look at all the fiddling I have done, and there is bound to be more.

We have some example thesis.org files in `Documents` if you ever need
inspiration. Also checkout the references.
*** References
- [[http://bastibe.de/2014-11-19-writing-a-thesis-in-org-mode.html][Writing a thesis in org-mode]]
- [[http://www.macs.hw.ac.uk/~rs46/phd-thesis.html][Rob Stewart's PhD thesis]]
- [[http://orgmode.org/manual/In_002dbuffer-settings.html][Summary of in-buffer settings]]
- [[http://orgmode.org/manual/Export-settings.html#Export-settings][Export settings]]
- [[http://orgmode.org/manual/Embedded-LaTeX.html][Embedded LaTeX in orgmode]]
- [[https://www.gnu.org/software/emacs/manual/html_node/emacs/Specifying-File-Variables.html][Specifying File Variables]]
** DOIN [17/38] Finale Planne whatever
Most of this is going to have to be talked about in the [[Approach]] Section of the
thesis, and maybe also in the [[Introduction/Preliminaries]] where the concepts are general.
*** WAIT [0/4] Data Analysis
**** WAIT [0/2] Fisher
***** WAIT [0/9] About
- [ ] Where does the data come from, with reference to paper
- [ ] What does the data have
  + [ ] From the main readme of the dataset, all the params
- [ ] Why use this dataset
  + Real Double Talks, similar to KA3
    + [ ] Some examples
  + Not a laboratory dataset (?)
  + SNR (?)
  + Giant, may help the models generalize better
  + [ ] How have others used it?
- [ ] What part was used
- [ ] How is double talk inferred
- [ ] What are the limitations / problems
  + Only Telephone conversations, and only in English
  + Designed for speech recognition for conversations
  + VAD done automatically, not manually, only transcription done manually
  + No way to explicitly determine unique number of speakers over the dataset
  + Segmentation not as fine as TIMIT
  + Some parts are not annotated, and have to be taken out carefully
***** WAIT [0/13] Analysis of segment lengths : General, 0T, 1T, 2T
- [ ] *Do all analysis in a notebook, either here or `rennet-x`*
- [ ] *Do All analysis at /frame level/*
- [ ] *Use consistent colors*
- [ ] What is the annotation length + histogram
- [ ] What are the inferred segment lengths for 0T, 1T and 2T + histograms
- [ ] When do 2T segments occur? Check also @heldner:2010pauses
  + [ ] S1 -> DT -> S1 (back-channel)
  + [ ] S1 -> DT -> S2 (turn)
  + [ ] S1 -> DT -> No (back-channel)
  + [ ] No -> DT -> Sx (overlapping-start and takeover)
  + [ ] No -> DT -> No (overlapping-start and backing down)
- [ ] What is the gender distribution for different segment lengths, 1T and
  2T, + pie-chart of n-frames + /maybe/ histograms
- [ ] /maybe/ What are the distributions for other params, like topic-id,
  dialect, etc.
*** WAIT [0/2] Data Preparation
**** TODO [0/6] Split into train, val, test/eval
- [ ] Which groups were added to which split, and possibly why.
  + [ ] replicate the same labels data on myrmidon as planned on unumpu, even
    though we do not have the audio, so that we can do some analysis while at home
- [ ] Check the distributions of different statistics
  + [ ] segment lengths : general, 0T, 1T, 2T + histograms
  + [ ] gender distributions for 1T, 2T + pie-chart + /maybe/ histograms
  + [ ] /maybe/ the distribution of other params
**** WAIT [0/7] Convert all to merged, mono, 8kHz, wav files
- [ ] Mention that we only export parts of the audio that are within =min-start=
  and =max-end=, although we actually do it before feature extraction on the
  read =numpy-data= later on.
- [ ] Check how it is being done in =pydub= and document
- [ ] expected to be =int16= files, without compression, and equal weights for
  all channels.
  + [ ] The values are normalized at the time of feature-extraction to be in
    range (-1, 1) and mean 0 when getting input to feature extraction. Done by =librosa.load=.
  + [ ] Check for each split to confirm.
- [ ] this is where the model hyperparameters have already started to
  accumulate, although it is arguable if using only Telephone conversations
  should be made part of that, especially since we are working with Deep Learning.
  - [ ] how to account for robustness?
*** WAIT [5/7] Feature Extraction
**** DONE [2/2] Load audio using =librosa.load=
CLOSED: [2017-08-04 Fri 22:12]
- [X] make sure that they are in the range (-1, 1) and mean close to zero.
- [X] Take only the slice between =min-start= and =max-end= calculated with =samplerate_as(audio_samplerate)=.
**** DONE [7/7] Calculate the fbanks_64
CLOSED: [2017-08-04 Fri 22:10]
- [X] use params:
  + win_sec = 0.010
  + hop_sec = 0.032
  + samplerate = 8000
  + window = 'hann'
  + power = 2
  + n_mels = ~{40, 64, 96}~ = 64
    + [X] check that audio-classification-keras guy's explanation for 96
      * I can't find anything about his work.
    + Going with =64=. It serves a nice middle ground of serving the purpose of
      fbanks and keeping more information as far as I am concerned. Refer
      [[file:img/fbanks-v-spect.png]]
    + Yes, this will mean that the training will be slower than that for 40.
    + I am hoping that it will keep enough info about the speakers as well, more
      than 40 would.
    + I just can't find justification for 96, except if my samplerate was really high.
- [X] Use =librosa.features.stft= with =center=False=, and implement wrappers
  + [X] Save simple log10 of the mel-scaled spectrogram
  + [X] *Make sure that the final shape is in terms of (time, frequencies).*
    * [X] *Make sure that the shape in time dimension matches =samples_for_labelsat=.*
    * [X] Have to write at least my own spectrogram just to set center-ing to
      False. Damn you librosa!
**** DONE [2/2] Make 16k equivalent long chunks per-file and save as single dataset in master h5
CLOSED: [2017-08-04 Fri 22:12]
***** DONE [11/11] Dry run with a single file from validation set
CLOSED: [2017-08-04 Fri 22:12]
- [X] Make overlapping chunks with =strided_view=
  + win_shape = =2**14 = 16384=
  + step_shape = 10 seconds = 10 * 100 = 1000 ~= =1024=
- [X] Concatenate them either using =dask=
- [X] when reading into dask, make sure that chunk-size is win_shape, aka 16k equivalent.
- [X] Create one hdf5 dataset per file.
- [X] Make sure that all chunks for a file are stored in the same dataset in h5.
  + [X] Make sure that the chunking value is the same as the 16k equivalent we created.
  + [X] Check that reading all chunks do give the expected results.
- [X] Use compression
- [X] Use Checksum
- [X] Add fft-frequencies as attribute or whatever =h5py= provides, to each dataset.
  + [X] Check [[http://docs.h5py.org/en/latest/high/dims.html][Dimension Scales]] in h5py
  + Couldn't, and wouldn't ... h5py was not helping
***** DONE [5/5] Final notebook for all splits
CLOSED: [2017-08-04 Fri 22:11]
- [X] Keep to and from location for data configurable.
- [X] Run on *myrmidon*
  - [X] Remove old data for new space.
- [X] Run on *unumpu*
  - [X] copy results to *nm-raid*
**** DOIN [1/6] Normalization
- [X] see if the log-mel-spec values are in good range
- [ ] Normalize on chunk (== utterance) level at the time of feeding into the network.
  + Don't worry about skipping vectors with silences. Fuck it.
- [ ] Do dimwise-MN
- [ ] Do dimwise-MVN
- [ ] Do pixel-MN
- [ ] Do pixel-MVN
***** Why?
- Main reason: compensate for mismatch in training and testing conditions,
  especially related to channel distortions.
- Also: Make it more suitable for the machine learning algorithm.

**** IDEA [0/3] Add other possible features for future investigations, mainly as text
- [ ] look at links on MFCC, iVectors, prosody, pitch, CNSC, PLP, etc. for inspiration.
  + [ ] Look at these links from Todoist
    * [[https://en.wikipedia.org/wiki/Pitch_(music)][Pitch (music) - Wikipedia]]
      - [[http://www.fon.hum.uva.nl/praat/manual/Intro_4_1__Viewing_a_pitch_contour.html][Intro 4.1. Viewing a pitch contour]]
      - [[http://librosa.github.io/librosa/generated/librosa.core.piptrack.html#librosa.core.piptrack][librosa.core.piptrack — librosa 0.5.0 documentation]]
      - [[http://www.let.uu.nl/uilots/lab/courseware/phonetics/basics_of_acoustics_1/praat_pitch.html][praat_pitch]]
    * [[https://github.com/timmahrt/ProMo][timmahrt/ProMo]]
    * [[http://www.nature.com/articles/ncomms13654][Rapid tuning shifts in human auditory cortex enhance speech intelligibility]]
    * [[https://www.kaggle.com/primaryobjects/voicegender][Gender Recognition by Voice | Kaggle]]
    * iVectors : [[http://pydoc.net/Python/bob.spear/1.1.8/spear.utils/][Python bob.spear package v1.1.8, spear.utils module source code :: PyDoc.net]]
    * [ ] Fractal Dimensions, check zotero
- [ ] See if the argument that 'spectrogram' is the mother of all features
  still holds.
*** DOIN [12/18] The Classifier and Configurations
**** DONE [0/0] Fixed number of =steps per chunk=, {=8= or whatever runs (likely one with 2k examples)}
CLOSED: [2017-08-10 Thu 20:52] DEADLINE: <2017-08-06 Sun> SCHEDULED: <2017-08-05 Sat>
- This is to make sure we can predict exactly how many steps are required for a pass
- The same number of chunks are to be used with the same number of steps per chunk
  + For skipping/sub-sampling, do it before feeding into the stepper.
**** TODO [0/5] Number of epochs / passes over the dataset - =steps_per_chunk epochs per pass= * {=5=, =10=, =20= =passes=}
DEADLINE: <2017-08-06 Sun> SCHEDULED: <2017-08-05 Sat>
- [ ] Make atleast 5 passes over the entire dataset for every model.
- [ ] upto 21 = (5 + 5 + 10) total passes for the best/most promising/relevant models.
  + 2 types of promising results both working with atleast the same features,
    and other such input of same parameters, like, context, etc.:
    1. Excellent 0T v/ {1T + 2T}
    2. Best 1T v/ 2T, with 0T skipped
  + [ ] build upon saved checkpoints from earlier runs.
  + [ ] pass starting epoch as a parameter to Keras.fit_generator?
- *Fixed number of keras epochs per pass anyways* == 8
- Since there is a fixed number of steps per pass, irrespective of skipping or
  subsampling, the number of steps per keras epoch is also fixed.
  + equivalent to (total_steps // 8) + 1 for a keras epoch.
  + equivalent to ((nchunks * 8) // 8) + 1
  + Or, just use nchunks as the nsteps for keras epochs, lel
  + [ ] Make sure these invariants hold
- It is okay if we pass over a little more due to rounding, but we don't want to
  pass less than the entire dataset.
**** WAIT [2/5] The Neural =Network= - {~c3~}
- There is essentially just one model based on the code below.
- There is one output per-sequence, as in, we do sequence classification, but
  not at utterance level.
- The configurations will decide:
  + The input shape, and hence the context per frame.
  + The number of classes.
- We use BatchNormalization *BEFORE* Activation, to follow the original paper.
- We use Categorical crossentropy, and categorical accuracy.
- [X] We use adamax as optimizer, but this can change
  + Nah .. we're sticking with it ... too many other things waiting
- [ ] Use Average Pooling instead of Max Pooling?
- [ ] Check more conv nets for speech and decide one final that we can run.
  - [ ] @deng:2013deep
- [X] Move this to =keras_utils= or =models= or =model_utils=, and actually see
  the model output. Too much time getting wasted in making it work in spacemacs.
  + moved to =keras_utils=

#+BEGIN_SRC python :results output
  from keras.models import Sequential
  import keras.layers as kl

  def c3(input_shape, nclasses=3):
      model = Sequential(name='conv3')

      # first conv2d layer
      model.add(kl.Conv2D(
          64,
          3,
          strides=1,
          data_format='channels_last',
          input_shape=input_shape[1:],
          name='c1_3_64_1',
      ))
      model.add(kl.BatchNormalization(name='c1_bn'))
      model.add(kl.Activation('relu', name='c1_relu'))
      model.add(kl.Dropout(0.1, name='c1_d_10'))
      model.add(kl.MaxPool2D(2, name='c1_mxp2_2'))

      # second conv2d layer
      model.add(kl.Conv2D(
          128,
          3,
          strides=1,
          data_format='channels_last',
          input_shape=input_shape[1:],
          name='c2_3_128_1',
      ))
      model.add(kl.BatchNormalization(name='c2_bn'))
      model.add(kl.Activation('relu', name='c2_relu'))
      model.add(kl.Dropout(0.1, name='c2_d_10'))
      model.add(kl.MaxPool2D(2, name='c2_mxp2_2'))

      # third conv2d layer
      model.add(kl.Conv2D(
          256,
          3,
          strides=1,
          data_format='channels_last',
          input_shape=input_shape[1:],
          name='c3_3_256_1',
      ))
      model.add(kl.BatchNormalization(name='c3_bn'))
      model.add(kl.Activation('relu', name='c3_relu'))
      model.add(kl.Dropout(0.1, name='c3_d_10'))

      # max globally
      model.add(kl.GlobalMaxPool2D(name='gmxp'))

      # first FC
      model.add(kl.Dense(512, activation='relu', name='f1_512_relu'))
      model.add(kl.Dropout(0.1, name='f1_d_10'))

      # second FC
      model.add(kl.Dense(128, activation='relu', name='f2_128_relu'))
      model.add(kl.Dropout(0.1, name='f2_d_10'))

      # second FC
      model.add(kl.Dense(32, activation='relu', name='f3_32_relu'))
      model.add(kl.Dropout(0.1, name='f3_d_10'))

      # output layer
      model.add(kl.Dense(nclasses, activation='softmax', name='sfmx'))

      # Compile and send the model
      model.compile(
          loss='categorical_crossentropy',
          optimizer='adamax',
          metrics=['categorical_accuracy'],
      )

      return model

  input_shape = (None, 21, 64, 1)
  c3(input_shape).summary()
#+END_SRC
**** DONE [2/2] Features to use - {=fbanks_64=}
CLOSED: [2017-08-05 Sat 16:20]
- [X] choose one between ~{40, 64, 96}~, and stick to it.
  + [X] We wait on the final decision any way from [[Workflows/Finale Planne whatever/Feature Extraction]]
**** WAIT [1/2] Making sequences to input with =context= - {=±10=}
SCHEDULED: <2017-08-05 Sat>
- There are multiple options, and adding more context has helped results.
- I have decided to choose and evaluate only on ±10 frames (±100 ms).
- The decision comes from @ryant:2013speech
- We can go for ±20 or ±30 as in @xiong:2016achieving, but why not:
  + hardware limitations
  + Run time limitations
  + [ ] Add this to future works
- [X] Add `[..., None]` at the end to make it `channels_last` for conv2d
**** DONE [1/1] =Skipping= class(-es) {=0T=, =None=}
CLOSED: [2017-08-10 Thu 20:54] DEADLINE: <2017-08-07 Mon> SCHEDULED: <2017-08-05 Sat>
- *We only experiment with skipping 0T when we choose to, and it is preferable*
  + skipping 1T does not make sense, use subsampling instead
- We *still* maintain the same number of steps per chunk, even though the
  batches now may be of different sizes
- [ ] should check it out offline first to see that there are no unforseen
  circumstances where the batches may end up being empty.
- We want to avoid making copies of giant arrays, so the convoluted algo below.
- *Skipping will be done on validation/test data as well*
***** DONE [0/0] How to skip, the algo
CLOSED: [2017-08-10 Thu 20:54]
- Do normal strided data_prep
- Do normal strided label_prep.
  + This is the final decision of labels to skip or not is made.
- The label prepper returns two things
  1. the prepped_labels
  2. keep, which is:
     + True, if to keep all
     + np.array of booleans of size of prepped labels, indicating which example
       to keep
- The packaging method that sends data to stepper forwards all three
- The stepper looks at keep
  + if not is nd.array and True (check beforehand that this is never False)
    * return step-wise in nsteps_per_epoch
  + if nd.array of booleans
    * do cumsum of keep
    * nexamples_per_step = cumsum[-1] // step_per_epoch
    * ends = searchsorted(cumsum, arange(steps_per_epoch)  * nexamples_per_step,
      side='right')
    * starts = [0].extend(ends[:-1])
    * return data[keep[start:stop]] and label[keep[start:stop]] for start, stop
      in zip(starts, ends)
- Make sure that we never go out of bounds in our calculations, and never reture
  empty batches/steps, and always return the same number of steps per chunk.
**** DONE [1/1] =Sub-sampling= class(-es) {=1T_0.2=, =None=}
CLOSED: [2017-08-10 Thu 20:54] DEADLINE: <2017-08-07 Mon> SCHEDULED: <2017-08-05 Sat>
- *We only ever sub-sample 1T, cuz it is the majority class*
- Since the contexts would have been already added by now, we can subsample all
  1T, whether or not it is near 2T or 0T
- *Sub-sampling is never done on the validation/test set*
***** DONE [0/0] How to subsample, the algo
CLOSED: [2017-08-10 Thu 20:55]
- Do data and label prep as is, and also intercept the keep from label_prepper
  + Therefore, subsampler will be a sub-class of skipper, /maybe/
- make a var keep_list = []
- do a groupby on labels, key of label == class (1T)
- if key == False:
  + make array of all trues of the size of group
  + Append to keep_list
- if key == True:
  + make array of all False of the size of group
  + Set every nth (=5) to True, starting at first
  + Append to keep_list
- np.concatenate keep_list, which should be the same size as keep returned earlier
- set keep = keep && keep_list
  + this ensures that any skipping is carried along as well.
**** WAIT [0/1] Class-/Sample- =Weights= {=clsw_1= =clsw_2=}
DEADLINE: <2017-08-06 Sun> SCHEDULED: <2017-08-05 Sat>
- *We only use class weights, set them to 1 for both 0T, and 1T*
- we prefer to set class weight for 2T as 1, and at most 2
  + prefer 1 especially when skipping and/or sub-sampling
  + using 2 to perhaps support the argument that adding a cost matrix doesn't
    help much. I hope the results support it.
- *We use all ones as clsw when either skipping or subsampling*
  - [ ] Or do we?
- Why not more:
  + because, 2T is very similar to 1T.
  + It hardly ever gets confused with 0T.
  + Too much clsw for it has been shown to make the network results less confident.
    - Elaborate, with examples.
  + Slows down training in some ways.
  + We are also not 100% sure about each and every label. There is a collar.
- Why not based on data, entire or per batch:
  + the class weights become even more skewed.
  + Experiments were performed, things went wrong
**** WAIT [1/5] Choosing =label= for a sequence - {=center-frame=, =max-mid-±5-frames=}
SCHEDULED: <2017-08-05 Sat>
- priority is to choose the center frame with the idea that we are doing frame
  wise prediction, and the context is just there to ... well, put the frame in context
- The other choice of choosing the max of the mid ±5 frames, as per our
  inspiration, is to say that there is 2T happening somewhere near the center.
  + in terms of time, it means that we are adding a collar to the boundary at
    training time. This collar basically makes the boundaries fuzzy, in priority
    of 2T > 1T > 0T.
  + What is the collar size? The boundaries get fuzzy by ±5, depending on which end.
  + [ ] Hence, the post-processing should make sure that segment lengths are atleast
    11 frames (=110 msec) at the end.
  + [ ] The validation data to keras and confusions callback should have this
    collar applied, if the training data does so
  + [ ] We may even apply this collar to the val/test data on trainings not
    done with this collar
  + [ ] Finally, there should also be an evaluation step on the pure,
    un-collared val/test labels as well.
- Both of these can be implemented in the same code, cuz we only have to max,
  and both types involve the center frame anyway, with a var label_ctxt, which is:
  + 0 : for center-frame
  + 5 : for max-mid-±5-frames
- [X] maybe be smarter about creating strided_views based on data_ctxt and label_ctxt
  - There is not much smart way around it, since we need to know what frames
    need to be in the center. Have to create the strided views with the same params
**** DONE [2/2] Save model =checkpoint= on every keras epoch - {=per-keras-epoch=}
CLOSED: [2017-08-06 Sun 19:49] SCHEDULED: <2017-08-05 Sat> DEADLINE: <2017-08-05 Sat>
- [X] decide on file name formatting.
  + should reflect the true epoch and sub-epoch number
  + [X] Add a convenience function to =keras_utils= that accepts activity path
- Save checkpoints every keras epoch
**** DONE [2/2] Save =Tensorboard= events {=per-keras-epoch=}
CLOSED: [2017-08-06 Sun 19:52] SCHEDULED: <2017-08-05 Sat> DEADLINE: <2017-08-05 Sat>
- [X] Is the images and stuff not showing up an issue from my side?
  - Very likely that this is due to using generator for val data, because that
    is not mentioned to lead to histogram generation.
- [X] Is there a way to append to existing events file, instead of adding a new one?
  - if nothing else, if keras reflects what epoch we are training on, maybe that
    will help.
  - Skipping this ... cuz we can plot our plots from the accuracy in the log
  - Plus ... since we plan to pass =initial_epoch= to keras.fit ... there is a
    likelihood that the tensorboard reflects that.
**** DONE [0/0] Part of =training data= to use - {=all=}
CLOSED: [2017-08-05 Sat 16:21]
We use all the data we have for training. We'll see that we train each model for
atleast one pass. Of course, we pass it via the data provider.
**** DONE [3/3] Part of =validation data= to use - {=('00007', '00013', '00028', '00062', '00065', '00069', '00086')=}
CLOSED: [2017-08-10 Thu 20:58] DEADLINE: <2017-08-07 Mon> SCHEDULED: <2017-08-07 Mon>
- We use the same data for *all* validations while training, the keras one and
  the confusions one as well
  + so the confusions are printed for the same predictions/loss
- [X] *Find 1 or two calls after data-analysis over the extracted frames*
  + We have the validation data on myrmidon, so can be done at home
  + Choose for large ratio of 2T, long/good 2T
  + Sorry, we are, for now, choosing very many more than just 2, hoping it will perform
    + chosen based on gender, ratio of 2T, length of 2T, etc.
    + More explanation, for now, not required
  + If not, we'll reduce the number
  + Added that list of calls to =datasets.fisher= for easy access.
- [X] give as generator
  + We can also predict the nstep size as it is with nchunks * 8
- [X] Do not do any sub-sampling in the validation data provider.
  + skipping will be done, however, if done on training data.
**** DONE [11/11] Chatty =Confusions= in callback - {=init+per-keras-epoch=}
CLOSED: [2017-08-06 Sun 19:55] SCHEDULED: <2017-08-05 Sat> DEADLINE: <2017-08-05 Sat>
- [X] Add to a new =keras_utils= file.
- [X] requires path to export the h5 to.
- [X] Make a confusion calculation on init, on 0th batch of 0th epoch
  + this is to make sure that any errors due to the size of the batch are caught early.
  + Save the true labels
  + Save the output and confusions, and print the prec rec, to give idea of if
    the network has learned anything at all, calling it 'init' in the file/logs
- [X] After the init one, Only do per keras epoch, and we have 8*npasses any way.
  + [X] how to calculate preds? =predict_generator=?  =Yes=
  + [X] print out the precision and recall for all classes being trained on, with correct
    epoch and sub-epoch number.
  + [X] Save the full confusion matrix as well.
- [X] print full confusion after training ends.
- [X] Make sure that there is no code being called that needs packages not on
  the GPU.
- [X] Structure of the h5?
  - basically: [/init/x, x/pass/epoch, /final/x, /true] where x = [preds, confs,
    precs, recs]
- [X] use the same val_dp as one given to keras for it's validation
**** DONE [2/2] =Predict= over validationat the end of training, in the script - {=val-all=}
CLOSED: [2017-08-10 Thu 20:57] SCHEDULED: <2017-08-07 Mon>
- [X] predict only on the last epoch? What if it starts overfitting?
  + We explicitly predict at the end of training
  + Hopefully, the geepu thing will make it run fast enough, and
  + give an idea of run time on test data, if we ever decide to run on that.
- Prioritize saving the model first before this. We can do it offline, and
  probably will have to anyways.
  + Want to do on GPU cuz there is so much data, and inference takes that long.
- [X] Have to come up with a better /loop/ to save the predictions, cuz
  =predict_generator= works on returning one giant numpy-array.
  + We're doing explicit predict on batch on every step from flow.
  + No multi-processing, but atleast, it is supposed to run predictably
  + We save the trues and preds and the confusion matrix based on the same
    structure of labels in the chunking info
**** IDEA [0/1] Adapting a model for KA3
- [ ] This depends on whether or not the models will be evaluated on the KA3 dataset.
- Looks time consuming, and not much promising.
*** WAIT [0/0] The Table of Experiments
**** Fixed
- Values between '~~' may change before final models
|----------------------------+------------------------------------------------------------------------------|
| What                       | Value                                                                        |
|----------------------------+------------------------------------------------------------------------------|
| features                   | fbank 64                                                                     |
| data_ctxt                  | ±10                                                                          |
| steps_per_chunk            | 8 or whatever doesn't crash                                                  |
|----------------------------+------------------------------------------------------------------------------|
| val_dp_shuffle_seed        | None                                                                         |
| val_dp_callids             | =('00007', '00013', '00028', '00062', '00065', '00069', '00086')=            |
|----------------------------+------------------------------------------------------------------------------|
| trn_dp_shuffle_seed        | 32                                                                           |
| trn_dp_callids             | all                                                                          |
| epochs_per_pass            | steps_per_chunk                                                              |
|----------------------------+------------------------------------------------------------------------------|
| nclasses                   | 3                                                                            |
| input_shape                | trn_dp.inputdatashape                                                        |
| model                      | ~c3~                                                                         |
|----------------------------+------------------------------------------------------------------------------|
| trn_steps_per_epoch        | trn_dp.nchunks                                                               |
| val_steps                  | val_dp.steps_per_pass                                                        |
| max_q_size                 | 2 * trn_dp.steps_per_chunk + 1                                               |
| verbosity                  | 2                                                                            |
| pickle_safe                | True                                                                         |
| model_checkpoints          | per-epoch ='w.{epoch:03d}-{val_loss:.3f}-{val_categorical_accuracy:.3f}.h5'= |
| confusions                 | init + per-epoch on val_dp + final                                           |
| tensorboard                | per-epoch                                                                    |
|----------------------------+------------------------------------------------------------------------------|
| predict_on_inputs_provider | val.for_callids('all')                                                       |
|----------------------------+------------------------------------------------------------------------------|
**** Variables, and order of experiments

*** WAIT [0/1] Post Processing / Smoothing / Inference
- [ ] What was that thing where the likelihoods were multiplied by something
  before feeding into the HMM?
*** WAIT [0/0] Evaluations and Comparisons
** WAIT [0/1] Finale Palnne - Remainers
*** WAIT [0/7] Feature Extraction
- [ ] Extract the same features for KA3
- [ ] will have to argue on why not spectrogram, so cite recent works by
  @deng:2013recent ... maybe
  + [ ] /maybe/ and why log
- [ ] add to thesis plot of mel-frequencies
- [ ] /maybe/ document what is being done to maintain audio b/w (-1, 1) (look at =librosa.load=)
- [ ] Normalization ~Section above~
- [ ] /maybe/ other possible features for future work, ~Section Above~



* [50/61] Logs                                                     :noexport:
** DONE [3/3] 27-Jul-2017
CLOSED: [2017-08-04 Fri 21:39]
*** DONE [0/0]  5:53 PM : Setting up.
CLOSED: [2017-07-27 Thu 18:43]
I think I am going to be wasting a lot of my time fiddling with org-mode and
spacemacs. Add to that my perversion for using [[https://normanlayout.info/][Norman layout]] for typing, and I
am not sure how my numbers for productivity will look like.

And it is stupid, especially in the current context. There is a lot of stuff to
write and there is lot of stuff that will need to get done before a lot of stuff
gets written. And don't even get me started on the amount of back and forth that
will inevitably take place until the final document is ready to submit.
**** Why choose org-mode?
***** Pros
+ Pure text is easy and convenient to write, and adding $$\LaTeX$$ formatting is
  pretty easy towards the end.
+ Text files are easy to put in git.
+ There are many handy tools available for exporting, formatting, task
  management, etc.
+ I can run code from within the the org file, potentially making this repo a
  single file one.
+ I have some helpful reference usages available for using org-mode to write theses.
+ The experience can result in a life-long competency.
***** Cons
- Too many opportunities to fiddle with, especially considering I don't have
  much exprience of working this seriously, at least not with success, in
  org-mode beforehand.
  + I don't have enough experience with $$\LaTeX$$ either, but it is likely that I
    would have used Atom and some hacky, possibly inefficient process to make it
    work, just like I did for my seminar report.
- Too many opportunities to get distracted by, including making this my one-file
  repo idea, where this file holds other, non-thesis related, stuff as well,
  like these loggings.
- Can only use emacs to make best use of this file.
  * Frequent exports may be necessary.
**** Why do [[https://normanlayout.info/][Norman keyboard layout]]?
***** Pros
+ I type faster in it.
+ It is overall more comfortable for me.
+ I have some practice of using this layout while using org-mode, so not very many keys to relearn.
***** Cons
- Not very comfortable while using VIM keybindings, but not absolutely abysmal either.
*** DONE [5/5]  6:40 PM : First incision.
CLOSED: [2017-08-01 Tue 13:55]
If you don't believe me, I have writing the above log entry till *now!*

I have a bunch of things to do in order to even call all these hours to not have
been a waste. Those things shall be, at least for today:

- [X] Create an outline of the possible chapter headings.
- [X] Add some outlines in [[Introduction/Motivations]].
  + Added to a bunch of other headings too, main points that is.
  + There is still a lot of literature review kinda things needed.
  + I can keep on going, but ... hey ... good start eh!
- [X] Add links to pages that helped setup org-mode this far as references in
  [[Workflows/org-mode setup/Refrences]].
- [X] Test a preliminary export. Make sure git doesn't find it interesting.
- [X] Sync Google Drive.
*** DONE [0/0]  8:31 PM : After first incision
CLOSED: [2017-07-27 Thu 20:35]
I hope I can do this. I am finding this interesting, so that is a positive sign.
And I am talking about writing, not just fiddling with org-mode. In fact, it is
very likely that I never close this window of emacs, unless something forces me to.

I have done my things till syncing with Google Drive. It is a nice Checkpoint.

Next changes at hand are not exactly here, but in Todoist, essentially a
complete overhaul. That is definitely daunting and time-consuming, and I am
already hungry.

I hope that the next update is today, and I hope it comes with good news.

Back to the writing experience, I need to read a lot of papers again, if I have
ever come across them at all. That ... is ... scary.

Hope Allah Helps.
** DOIN [6/7]  1-Aug-2017
*** DOIN [2/3] 11:46 AM : Final Planning
No, I have not reorganized Todoist yet. Fuck!

But today, We do it!

After brainstormings and experimentation in the past days, I have come to a
conclusion which means that I basically have to start over ... from scratch.
That is definitely a daunting task. And I have to finishe writing this thing in
the meantime as well. I am very much screwed, and that will be mild to say.

And, since I have less than a month to do all of that (for buffer, we see why
later), not only does it demand excellent efficiency, but also aggressive
pruning and perhaps compromises. There is a small buffer to accommodate any
unforseen emergencies, but don't rely on it. There will be emergencies, the
first of which has been that I may have been calculating my spectrograms all
wrong till now!

Therefore, the plan, the final plan. Also, moving updating todoist to today as well.
- [ ] Make a final-ready plan in Workflows for all the things that need to be run.
- [X] Reorganize Todoist ... please ... dude ... it is unusable ... cluttered
  with outdated and/or impossible ideas and tasks.
  + [X] Find paper about `fe_03_p1` and add to zotero. :todoist:
*** DONE [3/3]  1:56 PM : Progress ... is slow
CLOSED: [2017-08-02 Wed 14:28]
After quite a bit of unnecessary waffling, I have finally started writing the
[[Finale Planne whatever]]. I started from the very beginning, hoping to make sure
that I don't miss any thing, and to organize my thoughts anyway.

I have only reached till feature extraction, although there is still a section
left for melspectrogram and normalization. There are more todos here that I am
not sure what I should do with them, and will multiply the [[The Classifier and
Configurations]] set of todos even more.

But, I am making progress. One idea that I got in the middle was to save the
chunks as overlapping by 10 seconds. That will help solve the issue of making
appropriately overlapping context frames. The choice of 10 seconds is to set the
upper-bound of the context frames I will be using later. I know that I will
actually only need like 100ms, but ... the repetition will hopefully not be an issue.

Furthermore, I may then settle to do CMN or even CMVN on the chunk level,
treating it as an utterance that is more than 2 minutes long.

Finally, I have had a few other ideas for aggressive sub-sampling.
- [X] Make sure to make this parametrizable, and skippable for validation data provider
- [X] In order to remove silences, and train only on 1T and 2T
  + Read the chunk for audio and labels
  + Remove the audio and labels where label == 0
  + Calculate the mean and variances on these
  + group the audio and labels based on label == 0 or not
  + If label == 0, keep a (context_len) amount of data, initially set to None, to_prepend
  + if label != 0, and if to_prepend is not None, prepend it to the current data
  + Make strided views for each group
    * No need to keep data from from here, cuz we are only going to group if/not silence.
  + Concatenate the strided views ... will need to make copies and increase memory need.
  + Give this to the stepper
- [X] In order to aggressively subsample 1T and 0T
  + If we have groups (as in, skipping silence)
    * create strided views with step_size = int(win_size / (larger_factor))
    * concatenate, and give to stepper
  + If not working with groups
    * still make groups based on label == 0 or not
    * Repeat above for grouped case only if all labels in the group are 1T (and/or 0T)

It is already 2:37 PM! Actually, I finished the normalization and melspectrogram
sections as well, for now. There are a LOT of todos, just for today, and many of
them are decisions and explorations. I am gonna go shower and pray. Lunch only
after all the planning has been done. Damn, the hardest parts are still to come.
*** DONE [1/1]  5:05 PM : I am late now am I not?
CLOSED: [2017-08-01 Tue 19:01]
Can't deny that I predicted this 'not being back before 5pm' thing. It's a
tragedy, and I didn't spend the time on eating. And all that while knowing that
the next sections to work on are by far the most crucial, and probably will give
me the most peace of mind. And also the fact that *I am only planning*, and all
those todos need to actually get done, and then written about in the thesis.

Hours are passing by in minutes, and I am waffling.

Here's the thing... I know that when I say that the tasks ahead are the most
important and difficult and what not, my heart starts beating like crazy and I
get stressed, and that makes decision making even more difficult and scary.
Therefore, I am not going to be too angry about this whole thing, because, one
way or another, I have to make sure all the things get done, and breaking down
will be a disaster. Calm down.

Here's an idea ... start tracking time. To the minute I say. The hope is that it
will pull me back to work when I am wasting it away, and push me away from it
when I am waffling too much and spending too much time on some thing. Like right
fucking now!. I can't plan to the minute, but that shouldn't stop me. And hey,
may be the tracking and looking at the actual time being spent will help me make
better plans!

- [X] Start tracking time, granularly, for *EVERYTHING*
*** DONE [1/1]  7:01 PM : Starting with what is kinda fixed.
CLOSED: [2017-08-01 Tue 20:07]
Have setup the trackers, tracking *EVERYTHING*, and then took a break for prayers.

The main thing is, there are so many options for the networks and the configurations.

I started with what I know, it is going to be a CNN classifying sequences, where
sequences are essentially single frames provided with left-right context.

At first I was thinking of adding one with starting filter size 3, but keep the
one with 5 as my top priority. Then, something stupid happened while adding the
source for the model and I had to restart. Then I decided to fuck it, and go
with a single network config, just the 5, and wrote the whole code.

But ... once done, I decided to make that 3. You see, now I will be working on
10ms hopped data. Furthermore, there are very few (64) filters in the first
layer (limited by hardware, per experience), so ... made that 5 into 3. /sigh/.
And, it makes sense that it fewer local patterns, and then more and more global
patterns going up. I hope it is fine. It looks a lot like the conv from that
@xiong:2016achieving paper (actually, more like VGG net) but with much fewer layers.

So, I have only one architecture to train on. I hope it works.

Furthermore, even though I may look into using configs with different context
sizes, I believe I am going to stick with (-10, +10) frames ((-100, +100) ms),
ending up in size 21 frames per sequence (210 ms). I don't want to test out the
other one for one obvious reason that I don't have the time, and there are far
more number of other hyperparameters to test out. Also, it makes sense to add
more context, and was shown to be good in that paper doing SAD on YouTube @ryant:2013speech.

And you know what ... there is still a lot of work to get done, and it is
already past 7:00 PM. 💩.

- [X] Prioritize models with ±10 frames context, and maybe skip the others.
*** DONE [0/0]  8:07 PM : Aggressive pruning ... slowly
CLOSED: [2017-08-01 Tue 20:13]
Added that section on options for context. I am adding the set of options
available for each 'hyperparameter' at the end of the heading. So far, the
features to use has more than 1 option, but I have to keep in mind that I have
not even begun to add a lot of other things.

They will depend on the arguments I want to make. The fewer, and clearer
arguments I want to make, the better I can design my experiments, and definitely
the more time I will have ... if I don't decide to go too overboard with nepochs.

I will, however, also have to keep in mind that I show-off some novelty in my
approach. Just CNN on context is an okay bet, but you know how much I want to
use the max-in-center-label approach. The only problem there is post-processing,
and that is another can of worms I have not opened yet, and it is past 8:00 PM.

Wasn't the plan that I get done by 12:00 PM. /sigh/
*** DONE [1/1]  9:19 PM : Couldn't take a break, updated network
CLOSED: [2017-08-02 Wed 14:27]
I wanted to take a break, and have dinner and what not, but in the cooling down
period and looking at how much still had to get done, I was ... waffling about
making myself something.

I realized that my earlier and complicated idea to do silence-skipping was
overly complicated, and could be done by simply removing the silence labels
/after/ they have been made into sequences. Yes, there will be forced copying in
=numpy=, but my earlier solution also involved that. Furthermore, this solution
is much less complicated, and doing sub-sampling of 1T is pretty similar, except
that I will already have the labels for the sequences, and can choose create my
boolean for keeping things by working only on the final sequence labels.

- [X] Add the approach of filtering away /after/ sequences have been made, in
  addition to the one brainstormed previously, and choose one.

After that, I actually got back to running the network and looking at the number
of params. I stopped fiddling with org mode to run my source block, and simply
copied and pasted the code in ipython. And lo-and-behold, I had missed max-pool
layers in between conv layers. Because, my number of trainable params were
beyond 1M. Now, that could definitely improve our model, but the problem in that
case would be training times. Plus, how many params do I need to learn for 3
fucking classes?

Anyway, I can still fiddle with it a little more, and change things. Like I did
by making the final FC layer tapered, instead of square. Too many params.
*** DONE [0/0] 11:32 PM : There are only so many hours ... I am awake
CLOSED: [2017-08-01 Tue 23:41]
The dinner break was a forced and unrewarding one. And then came in my nightly
routine. I don't feel very accomplished today. The biggest things are still left
to do in the planning task itself, let alone to start implementing on those
plans, which could definitely prove a lot more ominous, if not filled with
distractions and what.

If history is worth trusting, I may not even finish this planning thing. And,
also, I may not even end following a lot of it. There are definitely things that
can go wrong which may force me to abandon all hopes and get the minimum done.

The only reason I decided to get through with this was that I believed that I
had more knowledge now than ever. And that the deadlines will force me anyway.
And, otherwise, I can stop my brain from having incoherent thoughts.

The sad thing is ... a lot of the tasks are like the 'figure this out' kind, and
there are definitely more to come I am sure, my first deadline of having getting
this planning done today has been horrendously overspilled, and, just look at
how many hours it took me to write that much, I was that incoherent with my ideas.

Tomorrow is another day. Just like today.

I hope having finished a few of the easy sections will not come back to bite me
and make it more difficult for me to get started tomorrow.
** DOIN [4/5]  2-Aug-2017
*** DONE [0/0] 12:04 PM : Half the day left, after the other wasted frivolusly
CLOSED: [2017-08-02 Wed 12:08]
Well, it is officially more than 12 hours since I did something. Literally
anything useful, except probably sleep, but feeling the way I am right now, I am
not sure I even did that correctly.

We finish the plan in the next 6 hours, InshaAllah. It is very likely that I may
not have very good plans about the post-processing and evaluation sections, but
we can leave them with a few todos that can be figured out while our models are
training and we are writing.

Oh fuck ... yes ... I have been relying on writing the thesis while the
trainings ran in the background. I am screwed ... but that is not new.

Let's get started. Remember to sit properly, last night's back pain was not good.
*** DONE [0/0]  2:28 PM : Been making progress, better skipping algo now, I hope
CLOSED: [2017-08-02 Wed 14:34]
I have been making progress on certain decisions and plans. For example, I now
have it decided that only clsw are going to be used, and that too only for 2T,
and only upto 2.

I also made a lot of other adjustments and additions, that frankly I don't
remember much about right now. But, at least, I am in the zone.

Given the amount of time that has been spent, I am still going slow, but the
decisions are making me feel good, even though it is only an iceberg.

I will continue working on this, especially because I am in the zone, and don't
want to get it interrupted by anything. Not really hungry.

Furthermore, the next major sections on post-proc and evaluation can either be
straightforward, or require a lot of research. (FML). But, I can add some
desirables and todos for now and take them up later.

There is a lot of dev work waiting for me, and quite an uncomfortable few of
them will need careful research as well. Hence, I am pushing for finishing the
planning before I put something energizing (more like sleepy-making) in my stomach.

I really hope that there are no big surprises waiting for me. I really hope I
haven't been something really obvious.
*** DOIN [1/2]  4:29 PM : More progress, but shit there's so much still left.
Took a small break after the last update and push, but the sub-sampling thing,
and later the labelling thing kept distracting me (!). So, got back to work.

Yes, all of it just "planning", but I am significantly more detailed in my
decisions and algos today. I hope that works in my favor. There is one todo though.

- [X] Decide on that validation call(s) to be used while training, based on analysis.
  + choose based on amount, length, etc of 2T

There is still one giant task left, that is the table of experiments to perform.
That was /the/ main goal of this entire exercise, to end up with the final list
of experiments to perform, and the final list trainings to run. Quite a few of
them will remain tentative, especially beyond the first pass, but, let's see.

- [ ] Make the table of experiments with the configs to use

I am gonna go shower and pray. Eating, even though my stomach has started to
make sounds, will have to happen later. Hopefully, I will be done by 7:00 PM.
*** DONE [0/0]  9:29 PM : Well that break stretched for too long
CLOSED: [2017-08-02 Wed 21:35]
I had that shower and prayers break, and was actually back around 6:15 PM, but
... I just couldn't make myself to do anything apart from some tweaking here and
there. Wagering that it was the lack of food issue, I decided to take care of that.

Once done, I was still not in the mood, physically, to restart work. Followed by
a confusion between whether it was 8 PM or 9 PM, and I lost another hour.

I am still not in the mood to work, and God help me, because I just want to go
to bed, and maybe take a nap. But ... I don't want to ruin my delicated sleeping
routine which, even though I agree hasn't been paying off very well yet, but
will come in really handy soon. Actually, there might be one coming just around
the corner. And I am more confident of having one of those and sticking to the
tasks better when I am actually coding, and not researching, and definitely not
when I planning. Nevertheless, the coffee, as cold and old it may be, at this
time of day ... may end up doing something bad anyway.

In lieu of all the problems that I am having, I believe I am gonna grind a
little more, until I can't. Having finished the planning, and only being left
with develpment work might have worked, but ... well atleast I have a full
stomach and clean kitchen, and some quick snacks ready.
*** DONE [0/0] 11:35 PM : Well, got rid of the spectrogram bullshit
CLOSED: [2017-08-02 Wed 23:47]
I first started by making a nice table of the hyperparameters that going to be
fixed anyway, mainly from the scripts' point of view.

Then I used =itertools.product= on the rest of variables.

I can't lie but I was very surprised to see that total number of total
combination of variables led to atleast 32 different experiments. More scared
than surprised. I tried to reduce the list, but ... it was just not budging.

Then, I saw that using pure spectrograms was not promising much, and was
definitely going to need more epochs to settle down anywhere. Add to that that
these features are giant (129 dim). And ... all that ... for one agument that
while the networks can learn features for themselves, but helping it with better
features makes it easier. And possibly the other one that some features can end
up taking away too much speaker information away.

It was not worth it, and I remembered that Microsoft paper @deng:2013recent that
showed that fbanks worked better than not only spectrograms, but also MFCCs.

So, heck with it ... skipping spectrograms. Plus, now, the export size of the h5
files will be smaller. I am not going to add more calls to any splits though.
And, it will hopefully speed up the training times, all with the on-demand
log-ing and fbank-ing done beforehand.

It is a good decision, and halves the number of experiments I will have to run.

But that still leaves me with ... at least 16.

Aggressive pruning, I have to do that. They will depend on the set of arguments
I want to make. Best Wishes.

I am going to take a break.

Fuck ... I also have to decide on the n_mels ... FUUCK!
** WAIT [4/5]  3-Aug-2017
*** WAIT [0/1] 11:24 AM : n_mels = 64
Yes, the day started a little bit early today. Well ... I will be awake at this
time on earlier days as well, but today ... I could start working. The trick was
to start setting up the feat-ext script/notebook while watching (more like
listening, nay, hearing) YouTube.

I got in pretty easy into tackling the decision for n_mels, and ... even though
I still not sure about this, I am gonna stick with 64. It looks better than 40,
and I am not sure why I would choose 96, except if I already had a lot of
frequency bands already available (which would have needed a higher samplerate
audio in the first place).

- [ ] mention how the use of features need to be updated and a thorough study
  conducted in light of deep learning models now being prevelant.

Now, off to the next part ... looking at how the fbanks and the log fbanks look.
What values do they have, and in what range. How should I go about normalizing
them later (Most likely gonna stick with CMN).

I can't lie but, the chance than I may not be able to get my models running this
weekend was a pushing force today. Not because I am not confident in my
abilities to finish something as complicated as the training scripts in only
like 18 hours, but administrative reasons.

FML. Both ways.
*** DONE [0/0]  4:35 PM : Damn this crashing!
CLOSED: [2017-08-03 Thu 16:41]
At 1:00 PM, looking at how plotting histograms was taking time, I decided to
capitulate on my early start to have lunch. All was finished in proper amount of
time, but after that, I just did not want to return to work. I wasted another
hour here and there, and when I returned, all I have done since then is just
write some code.

And that's the thing. I making decisions about what features to use, and what
normalization to use, and why. That decision making is dangerous especially when
it is such a crucial part of the system. It is the darn fucking features that I
am talking about.

I realize that I don't have to worry too much about normalization right fucking
now and can afford to work on just the feature extraction, the log-fbanks. And I
can do the normalization debates when I am actually preparing for the models.

But the thing is *there* on my mind, and just not letting me have peace or anything.

There is another impending break any time now for Prayers. May be a shower will
help as well.
*** DONE [0/0]  6:29 PM : I hope the break works
CLOSED: [2017-08-03 Thu 18:33]
After the last update, I realized that I was wasting my energy on the wrong
thing at the wrong time. I don't have to worry too much about feature
normalization at the stage of feature extraction. I just need to make sure that
the values that I am saving make sense.

So, that is my goal for today, and, after finishing this log, I am gonna be
enumerating my tasks/checklist in Todoist for that. I will be extracting
log-mel-spectrograms, and have already written some wrapper functions for them.

I will start a fresh notebook, aggressively copying stuff where I can, and not
spending too much time in that notebook on frivolous stuff.

I will come back to planning later, likely towards the end of the day, after
another break. But, my train should not stop before I have a working script for
feature extraction.
*** DONE [0/0]  9:16 PM : Damn! is this taking time
CLOSED: [2017-08-03 Thu 21:19]
I have been working on the final feature extraction scripts, and I am only done
till setting up sources and sinks! There are pre-flight checks before I even
begin the new stuff I have to code for chunking and what not.

Damn!

It took me an hour to just put the feature extraction todos in Todoist! And the
rest nearly 2 hours ... and I am not even done with pre-flight checks!

What can I say, I like doing things right ... at the wrong fucking times!
*** DONE [0/0] 11:11 PM : Good boy pre-flight checks, no sleep before done!
CLOSED: [2017-08-03 Thu 23:18]
Yes, I am still in the pre-flight checks state, but what did you expect.

And good that I did the checks, because there are audio files that are smaller
than the labels available for them. I can't do anything about it, except that
while loading audio and extracting labels, I should trust my =AudioMetadata= for
the right number of samples available, and choose the minimum between that and
what is the =SequenceLabels.max_end= in order to determine the right endings for
the labels, and ofcourse for chopping the audio data itself.

I am kinda feeling a little smug about how elegantly the
=SequenceLabels.min_start_as= method solves the problem of missing labels beyond extrema.

I am gonna have to update a few things here in the workflows, but only after I
am done with the script. There is still the pre-flight check for how the
features are calculated, and later the monster of a task of making chunks.

For now, I have some nightly routines and Prayers to take care of.
** DONE [1/1]  4-Aug-2017
CLOSED: [2017-08-04 Fri 22:01]
*** DONE [0/0]  9:45 PM : Phew, done with feature extraction
CLOSED: [2017-08-04 Fri 22:01]
I know that I am bascally a week late on this. I tried doing it on 31-Sep-2017,
but I didn't have many of things figured out, and had to make the plans.

I was obviously successful in finishing the training scripts last night, but at
least I went to sleep with a pretty solid plan for how to implement dasking the
striding thing and concatenating such that the results were with appropriate
chunking size, overlaps, etc.

The morning today, which started pretty early I must say, especially relative to
my going to bed time, was basically implementing that idea, with a nice surprise
helping solve a problem for me as it is, thanks to some test cases I had had
running last night.

What I didn't expect, I should be scolded for it, especially because I was so
smug last night about it, was that my 'just shift the min_start to zero and
everything will work out' had a serious flaw, especially in the case that I
wanted to use it to solve my problem of having last labels beyond the length of
the audio, which itself was caught in a pre-flight check, like this problem was.
Thankfully, the fix was easy, and relatively elegant, and there was some smiling
and dancing involved.

The hard deadline of needing to be in office definitely helped speed things up,
but I was, of course, not done till the last minute. At office, the scripts (I
keep saying scripts when it is a jupyter notebook) worked almost flawlessly.
Took less than half the time for nearly 50% more data. However, I did notice
that there were a lot of empty CPU times in the bokeh plot of the dask progress
report, even though the features I was saving were 1/3 the earlier one. Most
likely, the striding thing for making chunks of constant size was messing things up.

In fact, come to think of it, there was no need to make all the chunks of the
same size, because h5py would have returned to me the right amount of data. Yes,
skipping could have made things a little messy, but I should not forget that the
main issue with the previous data provider was that I was skipping entire steps,
leaving me with no guaranteed estimates of what frames would be used, except
when I used the exact steps per chunk.

But, I hope I have saved myself time and (definitely) space while training cuz I
will be reading chunks of essentially 1/3 the size, essentially only needing normalization.

Oh you normalization ... I am confused like heck about you. I am not going to
touch you before I am done updating and implementing the training helpers, which
I plan to do this weekend. They will perhaps also make the analysis easier.

I am probably only going to update the done tasks for today ... and continue tomorrow.

This week has been tiring ... there is more to come ... I can't afford to rest
... much.
** DONE [3/3]  5-Aug-2017
CLOSED: [2017-08-06 Sun 19:46]
*** DONE [5/5]  4:05 PM : I certainly rested a bit too much
CLOSED: [2017-08-06 Sun 19:46]
Well ... let's not talk too much about it. Last night, I only went to sleep
after having made the updates I was not in the mood of making. So ... I thought
I had earned an extended relaxation session in the morning. Plus ... it is the weekend.

I have, nevertheless, tried to use the relaxation time to actually rejuvenate
myself ... and I hope I have succeeded on it. I ate ... read ... watched ... had
a long and deep sleep ... etc. Time to get back to work because I don't have
many more excuses and distractions left.

In that regard, I am most likely going to work on the trainings related utility
functions and classes, like the confusions calculations and the data providers.
Not the normalization part ... but that is rife with decision making ... which
scares me to no end, but also ... need me to have a reliable data provider ready anyways.

So, starting on those are my goals for today ... even though my planning has
essentially not even reached the most important parts of what needs to actually
shine in my thesis.

And, not just start ... I need to finish some things as well ... today! And, for
that ... I really don't know which one tackle first ... the easy one and risk
feeling like thursday past when my entire day was spent things that could have
been done by essentially copying pasting ... and ... the harder and more
involved one and risk having a yet another string of days spent on something
with no progress on other fronts whatsoever, like the finale planne.

I am gonna start with the easy one .. God help me ... but also prepend it with
some planning in Todoist (mainly because I am not finding task tracking in org
very intuitive yet ... and I just want to start with something even easier than
the easy task one.


- [X] Setup training utils project in Todoist with related tasks from here
- [X] Finish Confusions callback in =keras_utils=
- [X] keep model =c3= in the =model_utils= file. /maybe/ give a name.
  + Actually ... just added it to =keras_utils=
- [X] rename/replace confusing =training_utils.py= files.
  + There will be none now ... heh
- [X] Finish a simple =setup_callbacks= in a new =training_utils.py=
  + Make it a one-stop-shop training utils file inspired by the existing
    =rennet.training_utils= file, in the utils file.
  + =import * from rennet.utils.model_utils= to make all models available.
  + The dataprovider (or not) has to be setup by the user in his/her training /
    evaluation script ... because only they know what data they are using.
  + Everything is in =keras_utils=, and the user has to make sure the s/he is
    using the correct inputs provider, just like caring about providing the
    correct h5 files.
*** DONE [0/0]  5:55 PM : Setup Todoist ... yet to start real work
CLOSED: [2017-08-05 Sat 17:59]
Since the last log ... I have been setting up Todoist for the tasks for today
... and kinda also reorganized the outdated stuff there while at it ...

There are definitely some more possible tasks and ideas coming here later.

It is prayers time ... and I have to then get on with the real coding work for
today. I am hoping for smooth sailing ... especially because a lot of the things
are pretty straightforward, especially after having taken out a lot of decision
making from all of it ... and also because there are existing implementation
that I have written earlier that I know work.

woops ... dejavu?
*** DONE [0/0] 10:34 PM : It is taking sooo much time ... again!
CLOSED: [2017-08-05 Sat 22:38]
Damn Boy! Just printing stuff is taking so much time! What are you gonna do
about the real stuff???

It is nearly 4 hours that I have spent on this shit ... I am not finished yet
... at least not the hero tasks of printing things ...

What is causing the delay? It is taking me way too long to make even the most
trivial decisions ... and I am trying to make good ones ... I believe that most
of the code ... except the printing thing is there ... and I got distracted a
lot in the time between ... especially in stupidly trying to set some flag so
that the confusions history would be inactive ... INACTIVE?? What the hell was I
thinking?

Late start is making me pay ... and I am starting feel hungry again.
** DOIN [1/2]  6-Aug-2017
*** DONE [0/0]  3:44 PM : Long Break again huh?
CLOSED: [2017-08-06 Sun 15:50]
It was too damn easy to not have finished confusions history yesterday itself. I
was actually pretty much done by midnight. But I didn't commit and push because
I wanted to put some thought into it, plus, I hoped that it would be an easy
start and early win for today.

I actually woke up pretty early today ... and even with my shitty morning
'routine', I was kinda near starting work by 0930, but ... Fuck

Only good thing I did with my time till now is have 'some' lunch, although it is
arguable if what I had for lunch should be counted at all.

I will start by finishing yesterday's tasks ... not many left anyway ... and are
pretty straightforward ... but those are some very ominous words ... so ...
expect me to only start on something new only after 6.
*** TODO [1/2]  7:09 PM : Pretty much on time ... but ...
I am running pretty much on time ... finished a bunch of the planned things,
moved the model actually only to =keras_utils= ... and there is likely not going
to be any =training_utils=. The user will have to make sure that they are
importing the right stuff from their =dataset=, and the rest will be found in
=keras_utils=, if, of course, they are using keras at all.

Only left is committing some of the things ... but I have been distracted a little.

Distracted by ... data-preppers and data-provider related changes. Those are the
next set of tasks on my list you see.

The thing is ... it is kinda bugging me that when I implement a stepper as I
have been doing earlier, with the only change that I don't skip any step ...
that is ... there is a fixed number of steps per chunk ... that is all fine and
dandy for predictability ... but ... it is bugging me that for those consecutive
number of steps, each batch will be coming from the same 'utterance'/chunk and
hence the same call.

I will still have the issue where a batch comes from the same utterance ... but
... is there a way that the consecutive batches that keras sees ... they come
from different calls?

Apart from re-extracting the features with smaller chunking size, I can actually
simulate that where the stepper makes multiple chunkings out of the existing
one. But, I will then have the problem where my entire chunks could be
skippable, essentially negating the original purpose of keeping steps per chunk
and steps per epoch constant and predictable. I have analyzed the training h5
that I have on myrmidon, and can see that continuous segment lengths could go as
long as an expected input size / step / batch, leading to them getting skipped
if one uses the skipper.

The existing idea avoids skipping any potential batches by creating them /after/
the skipping has been done. And, as a reminder, we need stepped provider because
at full size, GPU will run out of memory, because keras essentially considers
each input as a single batch.

I can atleast shuffle the order in which the batches are provided ... even
though they will still belong to the same chunking.

- [X] Shuffle (if asked) the order of steps, and maybe also the samples as well,
  since we will be copying the data within the batch/step anyway.
- [ ] There is also the issue with when to normalize ... cuz that will result in
  copies if I do it after the striding ... and will be impacted by skipped
  classes if I do it before striding ... FUCK

I hope eating something will help ... but first ... some updates here!
** DONE [3/3]  7-Aug-2017
CLOSED: [2017-08-07 Mon 23:02]
*** DONE [0/0]  1:28 PM : Too many unsolved problems !!!
CLOSED: [2017-08-07 Mon 13:42]
Last night, I did not know how to proceed with many things regarding the data
providers. Plus, I had to attend to some other personal things. I decided to use
the time to create the KA3 dataset on myrmidon, because why not. Let's just say
that it was not as smooth as I had imagined it to be ... but not that rough
either. I had forgotten that KA3 can have more than 2 speakers as well! But ...
ultimately, the problems were solved, and I stuck to my earlier decision of
using Lisa_David as my validation data, because TBH, it sounded to best.

Nevertheless, I woke very late today ... and ... with my shitty morning routine,
was not having my coffee before 1130. But, I avoided the rest of the tragedies
by just opening up a few papers from zotero to help decide on the convolutional
network I am going to train on. But ... I got distracted by a paper on how large
batchsizes lead to bad generalizations. And that got me thinking, maybe that was
an issue with my trainings so far, because my batchsizes have been usually very
big, except probably when I was skipping 0T, when the batchsizes, in addition to
be unpredictable ... were rather small.

That got me into brainstorming whether I could ensure small batchsizes with my
new steppers, skippers and preppers. I hope I can ... given my steps_per_chunk
are big enough. Because, even if none of the chunk is going to get skipped, 8
steps_per_chunk will lead to 2048 examples per batch ... which is still 4x the
size in that paper for a batch to be the biggest small, but ... oh well ... I
could also use a different batchsize.

In fact, given that, I will have to settle on earlier decision of making batches
only after skipping has been done (i.e. by choosing from =keeps=). Consecutive
batches will still end up coming from the same chunk, but at least I can
randomize the order in which they do, and even within!

Finally, the normalization thing is going to kill me. I have no idea what to do
about it, with only the dynamic range normalization being slightly easier
looking than any other, especially considering the skipping and what not.
Nevertheless, I can't do any analyses until I have my data providers ready ...

So, those are my today's goals ... update the data providers and associated
classes, including the data preppers that, at least for now, only work with the
structure of the data, like, adding context, stepping, skipping, sub-sampling,
etc. I will probably need a notebook for the devving, for quick debugging. This
may still take more time than it should, and late start to the day is already
not helping.

I have ran out of real coffee ... /sigh/
*** DONE [0/0]  4:53 PM : I am so predictable
CLOSED: [2017-08-07 Mon 16:58]
Basically, I started from zero again ... although I did make heavy use of
copying and pasting. I am re-writing the inputs/data provider but this time
trying to keep the inheritance tree relatively more linear ... that is, there
is, so far, only parent per subclass ... but ... I am sure ... as I start adding
multiple different aspects to like stepping, etc ... there will have to be
multiple parents.

Rewriting ... in addition to bringing into the right zone so that I know what is
going on ... has also resulted in better (albeit still pretty much the same)
shuffling and initializations ... hope that speeds me up ...

... speeds me up to a place where I have no idea what to do ... like
normalization and model hyperparameters ... etc

Gonna go on a break now ... need food, need to go get coffee ... today will be a
late nighter.

BTW ... Hans Zimmer ... Dunkirk ... muah muah muah
*** DONE [0/0] 10:56 PM : I may go back to the old ways
CLOSED: [2017-08-07 Mon 23:02]
So ... I made good use of the break ... prayers, dinner, groceries, ... and was
back by 8:00 PM. I have a working Stepped Provider ... but ... I am kinda
feeling the need to go back to the multiple inheritance thing ... Smaller legos
are easier to manipulate, and have some good contraints built in ... having one
giant class like the one I have implemented which tries to do everything ... is
... well ... good for auto-completion ... but opens up doors to mistakes where
things get called and updated by some class that is not supposed to do it.

I am probably gonna go back to the old ways ... but this time ... I don't have
to rewrite anything ... just copy in the stuff. And then, I will implement the
specialized preppers and inputs providers ...

But ... I am feeling exhausted for some reason. I will go take a break for
prayers. It is very likely I don't come back till tomorrow.
** DONE [2/2]  8-Aug-2017
CLOSED: [2017-08-08 Tue 21:40]
*** DONE [0/0]  2:11 PM : Been working since early ... slowly but deliberatively
CLOSED: [2017-08-08 Tue 14:17]
Yeah ... I went to sleep early last night ... I was surprisingly tired.

Anyway, it helped me waking up early, and even with the shitty morning routine,
I was here and working around 9:00 AM.

I went with last night's resolution to keep the classes as they are ...
chunkings reader and Prepper required to make inputs provider.

Since morning, I have finished stepper, sub-sampler (and skipper when some ratio
is given to be zero, I might add a convenience wrapper, maybe), and I am
currently on the context-adder ... which is proving to be a little painful to
elegantly implement label_context ... especially the function to be applied to
choose the right label from a label_subcontext.

Been working for ~5 hours ... I think I will take a lunch and Prayers break.
Hopefully back by 1600, preferably, but unlikely, earlier.
*** DONE [0/0]  9:31 PM : =BaseWithContextClassSubsamplingSteppedInputsProvider=
CLOSED: [2017-08-08 Tue 21:39]
That break went into the 1800's, quite a bit later than 1600. Actually, I was
done with the eating and praying in the 1600's, but I was still so exhausted and
sleepy that I decided to take a small lie down in bed.

Once back, I was still sleepy, and may be with a vague headache. I decided that
since going to bed early was now almost certain, I should atleast finish
everything till Normalizers in =h5_utils=.

And that's where that long-ass named hero class comes in the title.

Anyway ... I have not tested most of the things at all, since for that, I will
have to implement them using =fisher.H5ChunkingsReader=, which I haven't looked
at yet. And it is important that I do that, because, I am playing with a lot of
multiple inheritance bullshit, and things can go wrong anywhere.

Anyway ... I might do that before bed ... if I am up for it... else ... hope
that tomorrow starts early, and ... that there are no giant bugs ... and that I
able to make quick decisions about normalization quickly, maybe because I may
have a shot at going to office and starting my first model training.

~700 lines ... damn you =rennet.utils.h5_utils= ... and barely any tested
outside some manual ones. However, since the hero class basically calls
everything, if my first test on that passes ... then ... I don't think I have to
worry too much about anything else. Otherwise ... definitely tomorrow.
** DONE [2/2]  9-Aug-2017
CLOSED: [2017-08-10 Thu 20:34]
*** DONE [0/0]  8:09 PM : Last double digit day and look at me wasting time
CLOSED: [2017-08-09 Wed 20:25]
I had started to get the headache tomorrow night which would go on to ruin my
falling asleep at the appropriate time. Nevertheless, I had myself setup to
start testing all my work on training utils as the first thing to do when I wake
up in the morning. Things didn't go so well after that.

I fell asleep late due to that headache, which was still there when I woke up
multiple times in the middle. The final wake up was too late (albeit with no
headache) and my shitty morning routine ate up my time till past 1400, even
though I tried to use a little of it to eat something.

Nevertheless, once I started, I could immediately find bugs. I started with the
simpler no-context one first ... and the bugs were easy to spot and fix. It
kinda made me happy and all.

But then came testing with-context one ... and that was just stupid. I mean ...
it was not working as expected to the point I was wondering how is Python
calling super ... because it was ending up in infinite recursions!

I finally decided to implement the context adder as a data-prepper instead of a
full on data-provider while still having to implement things in duplicates (with
tiny changes).

All in all ... sub-sampling ... implemented using =groupby= just slows things
down ... and I am in no mood to implement anything else.

Or ... I might ... using the =np_utils.groupby_value= ... because ... the next
tasks on my list are ... the ever dreaded ... normalization.

I have noted it earlier how I it looks to be far more complicated ... especially
when trying to get it done properly. Especially in the case of subsampling,
where I have to make sure that I only normalize based on the statistics of the
data that will be kept ... And, don't even get me started on normalizing on flat
or dimension wise normalization.

Lastly, I also have to choose some appropriate validation files ... based on
analyzing the validation set I have with me.

Then, ... setup the table of experiments ... FUCK!

And considering that this is the last day of the month with single digit date,
and I have not started writing my thesis yet ... it is safe to say that I am
royally FUCKED ...

... and I managed to waste 3/4 of the day today. /sigh/
*** DONE [0/0]  9:17 PM : Totally did that ... potentially 10x faster
CLOSED: [2017-08-09 Wed 21:23]
Yes ... in the time I had to wait for the Prayers, I just basically implemented
groupby based on =np_utils=, and the subsampling provider is almost on par with
the non-subsampling one, ~10x faster that original. However, it does assume
categorical labels, and I have no time to come up with something generic.

Anyways ... I even thinking about flunking normalization ... maybe ... but the
fact is that I don't want to do it now, even though it might potentially improve
my results. It needs a lot of researching and what not ... and for fuck's sake,
I don't want to do it. Maybe the last thing on my list before I run my models.

The other tasks on my list are also decision making ones. I might start on them
soon, and ... if I decide to run something without normalization anyways, then I
might have a model training running tomorrow! Well ... atleast ... submitted.

We'll see how it goes. I am anxious enough already to want to stop working for
today and go to bed, so ... please forgive me for trying to make things a little
easier for me to keep myself in the game.

I having serious doubts about going into research heavy career. I love to code.
** DONE [2/2] 10-Aug-2017
CLOSED: [2017-08-10 Thu 22:31]
*** DONE [0/0]  8:34 PM : I may be ready ... but I am not
CLOSED: [2017-08-10 Thu 20:50]
Last night, I started on choosing the validation calls to use, but was getting
no where. My code was sloppy, and things were ugly. So, I stopped.

Today didn't start very early, but I restarted with analyzing the labels for
validation set, and ended up choosing them based on genders, total ratio of 2T,
the number of such segments, and the average length of such segments (because,
you see, the annotations are not very granular, in fact I didn't choose the top
two wrt ratio of 2T for this reason). I was basically done with my decision by 1
PM, and, if I wanted to, could make the push to go to office and get them running.

However, that would definitely been an exercise in frustration, because there
were many other bugs, especially in the ConfusionsHistory callback that I only
discovered while writing a function to make predictions on end (instead of a
callback). I fixed those, and implemented the latter by just using
with_chunkings, which also I had to implement. Finally, I had to add the extra
channel dim to the with-context prepper.

All those bugs could have been fixed easily, and probably quickly, even if I had
gone to office with the plan to start a model training on unnormalized data.
However, it would certainly have been exhausting for the rest of the day.

I didn't save myself any grace however, because, even after getting done by
around 4PM with all of that, I didn't use my time till now in figuring out the
normalization, or making the scripts. The normalization thing is looking scarier
than ever, especially considering how it probably needs to get tackled in cases
of class skipping or subsampling. And I have no idea what to do.

I have been pushing my code basically, and just biting my time to reach night
time, so that I can go to sleep in this nice and chilly weather. But, there are
still enough hours left that at least working on the scripts can be started,
even if I decide to go with unnormalized data as it is.

And, who knows, it might even work out! There are way too many changes in my
current implementations from the previous ones that I just can't predict the
results.

Nevertheless, I will still then have to setup the table of experiments, the as
yet another dreaded task that I was able to avoid in the name of doing all this
dev work.

I will go through my tasks here and make sure that I haven't missed anything
stupid. I can definitely choose to waste my time in trying to explain in the
workflows why certain calls were chosen for validation (they are =['00007',
'00013', '00028', '00062', '00065', '00069', '00086', ]= BTW), but I hope I
don't, and instead, take up the setting up of the table of experiments, and
start on the scripts so that I have something running on the geepu when I return
tomorrow.

Yes, setting the scripts will require them to be tested beforehand as well. So,
my work is far from done. So much for the hopes of enjoying a sleep in the chill
weather. Nevertheless, I am still not sure about my conv2D model ... and FUCK is
this shit hard.
*** DONE [0/0] 10:27 PM : I might have a very heavily pruned set of experiments to run tomorrow
CLOSED: [2017-08-10 Thu 22:31]
As you can easily imagine, even with the configs that were heavily pruned, even
with unnormalized training looking like my only path, I still have double-digit
number of experiments to perform, if I want to do this correctly.

In this moment of fear, I have heavily pruned even further, most likely not
using clsw_2 at all, and keeping room for normalization later on, maybe.

I have setup the tasks for the necessary scripts I will have to prepare, and
hopefully start tomorrow at office. I will do them as first thing in the
morning, hoping that it will pull me out of my shitty morning routine earlier,
and make the deadline sort of like work for me.
** DONE [1/1] 11-Aug-2017
CLOSED: [2017-08-11 Fri 21:24]
*** DONE [0/0]  9:10 PM : They are running, and they're running fast!
CLOSED: [2017-08-11 Fri 21:24]
I was hoping for an early start today, but didn't really make sure of that last
night by having late dinner and adamantly finishing the new Dan Carlin's
Hardcore History episode.

But, the presence of a virtual deadline kinda pushed me to skip my shitty
morning routine and get started with the training scripts pretty much as soon as
I was out of bed. But ... I had woken up pretty late, and there were looming
clouds of doubt on whether I will have to postpone starting the training
sessions till tomorrow, not the least of which was just my difficulty in
comprehending some of the most basic variables in my code.

I had to perform some quick fixes towards the end, therefore the dry runs on
myrmidon were just immensely helpful. Nevertheless, albeit an hour later than
planned, I had the scripts kinda ready, and an hour or so later, I was trying to
make them run geepu-d.

And you know what, apart from some easy to spot mistakes, the first script
worked flawlessly. And, to my surprise, by the time I duplicated it to make 5
other configurations, the running one had already finished an epoch! Which is
like 1/8th of the first pass, but dude ... it was running fast! All thanks to
the new setup at the geepu-d main node.

Quickly, yet carefully, I got the other ones submitted, and by the looks of it,
I may have them finished by tomorrow! Yes, I only ran them for 5 passes (10 in
case of subsampling), but ... that possibility just exhilirated me. Which was
important given how anxious I was while preparing the first one, and how much I
lacked confidence in getting even one script started today just before leaving
my room.

That comes with a small caveat too ... because now, not only is it possible for
me to run for many more epochs, but also, for multiple possible configurations,
including maybe spectrograms, mfccs, etc not forgetting different
normalizations. I know that if I start to increase the number of epochs, my
training time will still go into weeks, and that will be dangerous, but ... hey
... I can see that light, after I have been in so much darkness.

However, I should still be careful, because it is far more important for me to
start writing my thesis document now than it is to start finding excuses not to.
And ... if history has taught me anything about my working, even the simplest
things can take a lot of time and effort. They may as well end up paying back,
but I have nowhere the luxury of time as I had even a week ago.

I'll see what I do ... and I'll see into it tomorrow, hopefully early in the
morning. I will be going to office to check on the finished jobs, and restart
them with for longer number of epochs. So, I hope, I will have the table of
experiments with those that are running up there tomorrow ... at some point at
least. Till then ... I really want to enjoy some deep sleep in this nice and
chilly weather.
** DONE [2/2] 16-Aug-2017
CLOSED: [2017-08-17 Thu 02:58]
*** DONE [0/0]  2:20 PM : Fuck My Life
CLOSED: [2017-08-16 Wed 14:34]
I have essentially not been working since the last entry. If you want to be
generous and count the model trainings going on in the background as a single
task that *I* am doing, then you are missing the point of this being the last
month to finish my thesis.

I tried, but my sleeping routine has gotten fucked, and my shitty morning
routine, which therefore happens much later in the day, has become worse. Plus,
I also had a bad headache yesterday, rendering me useless.

But ... let's stop degrading ourselves, and try to focus on some other possible
whys.

The thing is, the next tasks ahead of me ... are those that I had deferred
earlier because they were taking too much time because they needed analysis,
research and, by far the most dreaded, *decision making*. They include,
normalization, post-proc, and evaluations. Yes, there is this another one, which
is *actually writing the damn thesis*, but ... we'll come to that a bit later.

I need a plan for those difficult tasks, and a bounded one at that, and
definitely explicit in all regards. That is what I am going to do next, at least
for the normalization step, because that is going to define perhaps the next set
of models that I want to train, which looks like not starting before Friday. (FML).

The post-processing thing will have to be compromised a little bit, and the
evaluations will have to stick with frame level statistics, and /maybe/
segmentation metrics, if I plan to use the iFinder system for it. They both will
involve significant development effort if I don't act smartly, let alone faithfully.

Back to writing the thesis. /sigh/. Let's make one thing clear. I know that
writing the thesis will definitely help guide my decisions and steps better and
put things in context. Moreover, it is *the* the most important thing to do with
respect to my thesis. All the experiments will be for naught if I don't have a
proper written document. And, considering that I have lost half a month has
passed already, it is becoming more and more urgent.

We'll, unfortunately, have to defer it to a little later in the week. I *have*
to decide on normalization first, because I expect to be talking about it in the
thesis, and because it will perhaps improve my results. So, that's the plan for
starting today, till I go to sleep. Finish normalization things. /gulp/
*** DONE [0/0]  9:17 PM : What the hell am I supposed to do?
CLOSED: [2017-08-16 Wed 21:29]
I spent roughly 4 hours or more trying again to find out a way to normalize my
data. My initial plan was to just write down the why's and possible how's, but
then I started looking at other papers on google and what not for their
normalization strategies.

From VGG-net, the one that my network is the closest relative to, I saw they
were only doing dataset level mean normalization over all pixels in a channel,
and were definitely using it in their tests and evaluations, but I can't use
that as it is ... not at the frame with context level ... because it doesn't, so
far, make sense to me, except if I am trying to copy their method as is, without
any explanation.

So, I tried to come up with an explanation. The one thing that was haunting me
about their approach was that they have an explicitly limited range of possible
values for each pixel [0, 255]. In our case, we do have an explicit lower bound
on the values, they would have been 0 for the power-spectrogram, and -8 when
logged with amin of 1e-8. But ... the maximum ... that has been proving to be
elusive. Even though all the operations are very well known, and my input has an
explicit normalization of (-1, 1) range, I can't seem to find and neither come
up with that max limit of the final value. That ate up significant portion of my
time and energy, and was the last thing I tried doing before taking a break for
dinner.

In the middle though, before I went on doing that, I looked at more refrences
for overlapping speech detection, and lo and behold, there was an entire thesis
that I had never seen. It is explicitly on the topic of overlapping speech
detection, with more focus on improving diarization, and using GMM-HMM. I
skimmed through that quickly as well, mainly trying to find their approach to
normalization ... but all that they are doing is CMN, and that too on MFCC. I
can definitely, however, use the thesis to get ideas about Viterbi decoding ...
but that comes later.

In essence ... I am back to where I was, and I blame, in addition to the task
being scary, the lack of direction and appropriate planning. I know that writing
it down would help, especially in thesis form ... but hey ... I still don't know
about a lot of stuff.

Nevertheless, I am going to start on that planning and working late night today
... especially considering the date, and my shitty sleeping and waking-up rituals.
** DOIN [2/3] 17-Aug-2017
*** TODO [2/12]  3:03 AM : Alright ... CMN it is ... was it so hard?
Alright ... I believe I have some direction to go in. Reading that thesis of the
guy from RWTH was very helpful and introduced me to different possibilities of
normalization, and a summarization and sets of experiments with results. Very
very useful indeed.

I have decided to try at least mean normalization per-dim on the filter-banks. I
am not sure if I will try out MFCCs, but it doesn't look like it, especially
after reading how it's goal is to separate the source and vocal tract level
information from the extracted features.

The other method that I wanted to try out earlier was to do dynamic range
normalization but ... for some reason, I don't think it is a good idea, even
though I can somehow match it to what is usually done in computer vision
(scaling). My main concern is not knowing the bounds of the values, especially
the max. From the looks of the histograms, I may be fine with choosing
log10(n_fft), but ... I can't / won't explain it ... plus ... it can go fuck
myself. Actually, if I calculate mel-scaled values on a power spectrogram that
has all values equal to 2**14 (what I got as pow-spec for constant signal), then
... log10(/of that/) is almost 7 ... resulting in the data being centered
already ... !!! which is definitely not the case.

If I want to do something with the histograms, then it makes sense to do the
histogram equalization thing from that thesis, which ... God help me ... seems
to complicated and I don't want to them.

Talking of copyting computer vision kind of steps, that VGG-net paper, and many
others, actually only do channel wise pixel level mean subtraction (over the
entire dataset, and later use it on test as well). I may think about it, but
TBH, I don't think it will make any sense in speech. However, looking at the
plots for each dimensions, they don't look so independent to the neighbors, and
seem to lie in similar ranges, so .... /maybe/

Variance normalization alongside mean normalization makes sense ... a little bit
... but ... well ... it is a single step ... and increases the number of
experiments that I can run and evaluate ... without needing too much explanation.

Lastly, I am thinking about dropping the max-5-lctx configs altogether. From the
last results, they were worse than the per-frame ones, and require jumping
through a lot of hoops to explain them.

- [X] Implement Mean and variance normalizer that only acts per chunk
- [X] Final scripts to run the 0-lctx for mean, and mean+var normalization
- [ ] Offic:
  + [ ] start these scripts,
  + [ ] decide final if mx-5-lctx is dead
  + [ ] collect and bring the results.
- [ ] Other CNN changes
  + [ ] Average pooling
  + [ ] More context (±20)
  + [ ] Bigger first filter size
  + [ ] Deeper network with more convs before pooling
  + [ ] clsw???
*** DONE [0/0]  3:14 PM : The scripts are ready ... too much time here and there
CLOSED: [2017-08-17 Thu 15:22]
I went to sleep quite a bit later than I was done writing all of that above. I
was actually reading more about from that TUM speech recognition wiki, and also
found a paper which concentrated very heavily on analyzing the data imbalance
problem, and kinda skimmed through it all ... and could see that there is a
chance that my networks have not been learning very well, especially when I
don't help it. The idea of clsw has therefore risen again ... as a method of
modifying the objective function and make the loss function reflect the cost of
misclassifying the minority class.

Of course, there are also ideas of using more powerful network configurations,
and testing filter sizes, and using deeper networks ... etc.

But ... since I have woken, which I did at the same usual time, even when I had
gone to bed only after the sun had already come up, I have been working on mean
and variance normalizer, mainly testing the impact on the histograms, because
the code itself was pretty easy (because I am chunk level normalization,
irrespective of whether there is any subsampling or class skipping going on
later on). Then, I prepared the scripts, and now they are all ready. It is
almost the end of the day ... but I also don't have to do a lot ... I may leave
in this hour hopefully.

And when I come back, I will have the results of the previous trainings, and ...
even though it will be very tempting to take the rest of the day off ... no! I
will either start analyzing the results in a bit more detail ... or start
writing the thesis. Both, I agree, and dangerous looking tasks, but ... hey ...
we ain't got time for nothing.
*** DONE [0/0]  5:45 PM : No ... I didn't
CLOSED: [2017-08-17 Thu 17:53]
I actually prepared, put everything in the bag and went out to go to office.
But, by the time I had left, it was past 1600, and there was no way I was going
to reach office before 1700. Plus, the weather was / is this weird and bad mix
of post-rain humid and warm and very little wind.

I turned halfway on my way and got lunch/dinner and came back. Gonna go run
those things tomorrow.

An important reason for skipping office today was what I hinted at earlier what might
have happened to the rest of the day. I would have most likely gotten really
tired, and taken the rest of it till tomorrow morning ... off. /Oh yes ... we
were going to instead work on the thesis weren't we ... / nope ... definitely
not.

On my way, I was thinking about what I would write about anyway if I succeeded
in making myself do it. The goals/scope section made the most sense, because it
is relatively easy, and may even help in decisions about the various experiments
I plan to run later on.

So ... that is the plan for today ... or at least till I go to sleep. Finish the
Goals and Scope section. I may ... after that ... even start data-analysis
section, because certain arguments will need backing from the data. /Just .../
don't get too hooked up about making the plots and charts look pretty. Not right now.

For some reason ... the internet is not working ... and ... honestly, I don't
need it ... but there are a few more minutes I wanted to chillax after lunch,
which ... for good or for worse ... may have to get cut short.

Nevertheless, before I have even started on it, even the most trivial of the
sections of the thesis promise to be very difficult to write, if not only time
consuming. There is also the question of whether or not I should go into the
details of how a DCN works, and what is deep learning, and all the associated
algorithms and what not. If I have to inflate the number of pages, I can
certainly do that, or leave it as an appendix item with refrences.

Oh God the refrences ... I can no longer just skim the papers ... I'll have to
read them not only to make sure what I am referring to from their work does make
sense, but also to probably get some proper ideas to write them in my own words.
There is a lot of reading that is going to be involved. And I am not even
mentioning how many tons of fucking papers I have already.

Lastly, equations ... metrics, etc ... they will have to be written down as
well. I may wing it in the beginning but they will not be painless either.

Last night's work was surprisingly focused and I remembered a lot of it. I was
also relatively quick. The peace and serenity ... may be those are worth fucking
up my sleeping schedule, considering especially that my shitty morning routine
ends up fucking things up any way. This way, chillax in the evening at least comes after
something has already been achieved ... and so does going to sleep.
** DONE [1/1] 18-Aug-2017
CLOSED: [2017-08-19 Sat 20:21]
*** DONE [0/0]  8:23 PM : No ... I didn't ... again
CLOSED: [2017-08-18 Fri 20:32]
Wait ... regarding the running of trainings thing ... /that/ I got done today
... except that the cluster is pretty busy ... so I might end up taking more
time than just this weekend. That damn predit-on-end thing still didn't work ...
this time due to a stupid typo bug that I introduced in the last commit.

What I didn't do is ... writing anything in the thesis ... let alone anything in
the goals section or any other section ... although I had brainstormed a lot
about it and the no-internet situation was like a perfect opportunity, and I did
not go to sleep early either. I don't have an excuse ... just that I was stupid
and over-slept today as well ... and all I could do was go to office, get the
trainings running ... and because it was raining so heavily ... get some
preliminary and of course surprisingly underwhelming analysis of the results of
the trainings that had finished.

I will be doing more detailed analysis later (I hope), but as it is ... it looks
like the precision for double-talk just went very high while the recall went
into single digit percentages ... which essentially means it didn't work out. I
might look at the histograms more deeply and find a way to exploit any patterns
if there are any ... otherwise ... I have no idea other than just try smoothing
or ... choose a checkpoint before the last one. It is surprising, however, that
the model did not seem to have converged in any case, although I have a feeling
that that convergence would have come at very low recall and very high precision
for double talk ... since it is such a small fraction of the entire training set.

I know that there is barely one month left now for the real hard deadline, and
barely a week for my personal one. I am going to start on the thesis ...
especially considering that I have not much else to do for the next three days
which have more impact promised.
** DONE [1/1] 19-Aug-2017
CLOSED: [2017-08-19 Sat 20:29]
*** DONE [0/0]  8:21 PM : Shit
CLOSED: [2017-08-19 Sat 20:28]
Wait ... don't take ou the pitchforks so soon. I did keep the promise and
started writing the thesis last night. I started with the introduction section
because that was the one that was attracting my thoughts the most. Plus, it was
supposed to be the easiest one to handle.

Unfortunately, I have essentially wasted the nearly 7 hours that I worked on the
thing. I got distracted in a different way. For some reason, and definitely
without appropriate foresight, I started by talking about the Turing Test for
some reason, and on top of that ... made my sentences too complicated. The later
could have been fixed, but the path that I started on ... was going to get me
stuck in a corner later on. I do not have to debate machine intelligence v/s
human intelligence, and whether or not there is a potential of some elegant nods
and winks is not only irrelevant, it is also potentially dangerous.

I have to write the whole thing in a dry scientific and straightforward, clear
manner. Not only it is what is expected from me, it will also make my life a lot easier.

Nevertheless, those 7 hours gave me a taste of how hours can fly by in doing
this simple task without much progress to show for them. It is scary, and I have
to, again, be ruthless and, hopefully, responsible.

Nevertheless, I am still going to start with the introduction section, leaving
some sub-sections as merely outlines. The need for this approach stems from the
benefit of maintaining flow of thoughts and ideas.
** DOIN [1/1] 28-Aug-2017
*** DONE [0/0]  2:23 AM : I can only wish
CLOSED: [2017-08-28 Mon 02:38]
I can only wish that I would have been ruthless ... or at least responsible.
With my time, with my energy, with my focus, with my task in general and in
particular this thesis.

A week has passed, and I have only hastily been able to move on to chapter 2 by
skipping the most important section in the Introduction chapter, that on
relevant works, as mostly just outline peppered heavily with 'JIT FIO'
(Just-In-Time Figure-It-Out) tasks. Even the sections that have been closed are
definitely incomplete ... and the ones coming next are more difficult by an
order of magnitude ... with respect every metric one can think of, except
probably one which I am sure I should not exploit, and probably even can't.

It has been disappointing, my performance that is. I could definitely have made
a lot more progress if I had acted more responsibly. Forget about the days and
months that were wasted earlier ... the hours wasted in the past week are just
inexcusable. At this point in time? Abmoniable!

And here I am writing into the logs wasting my late night hours on words that
will perhaps never be read by a living soul.

Nevertheless, expect fewer updates in the upcoming week as well. I hope that the
next entry is titled ... FINALLY! ... but I cannot trust myself enough to be
sincere in hoping that.

A miracle would work just fine ... maybe ...
