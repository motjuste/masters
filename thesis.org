# -*- fill-column: 80; eval: (auto-fill-mode: 1); eval: (zotxt-easykey-mode 1);
# mode:bibtex; eval: (bibtex-set-dialect 'biblatex); -*-
# #+latex_header: \documentclass[a4paper, parskip=half, BCOR = 6mm]{scrbook}
#+TODO: IDEA TODO DOIN WAIT | DONE CANC
#+PROPERTY: COOKIE_DATA recursive
#+STARTUP: overview
#+STARTUP: indent
#+STARTUP: align
#+STARTUP: inlineimages
#+STARTUP: latexpreview
#+OPTIONS: toc:nil creator:nil todo:nil stat:nil tags:nil inline:nil
#+OPTIONS: H:6 ':t ^:{} tex:t
#+MACRO: NL @@latex:\\@@ @@html:<br>@@
#+MACRO: L @@latex:\LARGE@@ @@html:<br>@@


#+latex_class: scrbook
#+latex_class_options: [a4paper, oneside, parskip=half]
#+latex_header: \addtokomafont{sectioning}{\rmfamily}
#+TITLE: {{{L}}}Detecting Double-Talk (Overlapping Speech){{{NL}}}in Conversations{{{NL}}}using Deep Learning{{{NL}}}   {{{NL}}}   {{{NL}}}
#+AUTHOR: Abdullah
#+DATE: September, 2017
#+latex_header: \subject{\large{Master's Thesis}}
#+latex_header: \publishers{\vspace*{4em} \normalsize\textbf{Rheinisch-Westfälische Technische Hochschule Aachen}\\\small Faculty of Mathematics, Computer Science and Natural Sciences\\Department of Computer Science}

# #+latex_header: \usepackage[citestyle=authoryear-icomp,bibstyle=authoryear, hyperref=true,backref=true,maxcitenames=3,url=true,backend=biber,natbib=true] {biblatex}
#+latex_header: \usepackage[backend=biber]{biblatex}
#+latex_header: \addbibresource{thesis.bib}
#+LATEX_HEADER: \usepackage{amsmath}
#+latex_header: \usepackage{fancyhdr}
#+latex_header: \usepackage{afterpage}
#+latex_header: \usepackage{pdfpages}
#
# #+latex_header: \usepackage{chngcntr}
# #+latex_header: \counterwithout{figure}{chapter}
# #+latex_header: \counterwithout{table}{chapter}
#
# #+LATEX_HEADER: \usepackage{subcaption,subfig}
# #+latex_header: \numberwithin{figure}{section}
# #+latex_header: \numberwithin{table}{section}

#+latex_header: \DeclareMathOperator*{\argmax}{arg\,max}

#+latex: \includepdf[pages={1}]{affidavit.pdf}
\clearpage
#+latex: \pagenumbering{gobble}
#+latex: \addchap*{Abstract}
The work presented in this thesis aims to automatically detect double-talks
(overlapping speech) in audio recordings of natural conversations using a Deep
Convolutional Neural Network. In doing it so, manual engineering of problem
specific acoustic features prevelant in classical approaches is avoided. The
characteristic challenges arising from the ephemeral nature of natural
double-talks, in addition to the standard issues faced in development of a
pattern recognition system, are handled using different methods. In particular,
careful rebalancing of the training data for tackling the inherent class
imbalance, pre-removal of silence, and two standard normalization procedures for
reducing the mismatch in training and testing conditions, are all scientifically
evaluated for their respective impacts. Furthermore, the shortcoming of the
proposed neural network in modelling long-term temporal dependencies is
documented, and the attempt for fixing it with Viterbi decoding is reported.
Satisfactory results have been achieved on a large and representative testing set,
while multiple avenues have been paved for future works.

\clearpage

#+latex: \vspace*{\fill}
#+latex: \begin{center}
To Ammi, Abbu, Gudiya, Bushra and Khushi

/To Happiness Indeed/

#+latex: \begin{center}
#+latex: \end{center}
#+latex: \vspace*{\fill}

Thank you Dr. Köhler and Alex for playing invaluable roles along the way
#+latex: \end{center}
#+latex: \vspace*{\fill}
\clearpage

#+latex: \renewcommand{\headrulewidth}{0.5pt} %obere Trennlinie
#+latex: \fancyhead[R]{\small \textbf{\thepage}} %Kopfzeile rechts
#+latex: \fancyhead[L]{\small \textbf{\nouppercase{\rightmark}}} %Kopfzeile rechts
#+latex: \pagestyle{fancy} %Stilangabe
#+latex: \fancyfoot{}

#+TOC: headlines 2
\clearpage
#+latex: \pagenumbering{arabic}
* DONE [6/6] Introduction
CLOSED: [2017-09-17 Sun 00:23]
Conversations are the dominant form of social interaction between two or more
people. While text-based messaging has seen a meteoric rise in recent years,
speech is still the primary medium for holding conversations. These
interactions, especially face-to-face ones, are often enriched by both verbal as
well as non-verbal acts by the participants in the facilitation of the exchange
of ideas, information, thoughts, feelings, etc. The enthusiastic adoption of
emojis in text-based messaging by almost all vendors perhaps was to help users
at least simulate such acts in a concise way under the constraints of the medium
of text.

The study of such verbal and non-verbal acts are key to conversation analysis in
interactional linguistics. In addition to the choice of words, the use of
intonation, pauses, feedback signals, turn management, and other conscious or
unconscious phatic expressions, which in isolation may not hold any information
of value, can still provide fascinating insights into various aspects of a
particular conversation. One can analyse their use and frequency to determine
the scenario, the topic, the structure of the interaction, pre-existing or even
ad-hoc relationships between the participants, their cultural backgrounds and
predispositions, their levels of engagement and comfort, etc. To the inquisitive
mind, such analyses and related discoveries are the means to kindle the fire
that it is. However, the practical aspects of their study can inspire richer
and more satisfying social interactions, and, where relevant, help the
participants in achieving specific goals of such exercises better.

For their studies, linguists collect data in the form of video or audio
recordings, which are then transcribed in detail for the different aspects of
the conversation. The process, even with the use of digital technologies,
consists of painstakingly manual and time-consuming tasks, and often requires
expert knowledge and training. Using software tools like ELAN or PRAAT, they
often have to go through a recording multiple times to finely annotate
different events in the conversation. It involves disambiguating not only what
is spoken when and by whom, but also other verbal and non-verbal phenomena like
intonation, prosody, gestures, etc. When necessary (e.g. in bad recording
conditions), and when possible, this transcription is carried out by multiple
parties before final adjudication. The process gets increasingly expensive on
multiple metrics very quickly. Furthermore, high standards need to be maintained
since these transcriptions will essentially be the /ground truth/ for related data
driven academic and applied research later.

The work presented in this thesis is part of a larger project with the ultimate
goals of using machine learning to speed up, if not fully automate, the processes
involved in such researches in linguistics. Specifically, this thesis focuses on
detecting and temporally localizing /Double-Talk/ (also known as /Overlapping
Speech/) situations which occur when more than one speakers speak simultaneously
during a conversation. Its occurrence can be used as an incredible
meta-conversational metric by linguists. Depending on context and several other
factors, such overlaps can be quite frequent yet be characteristically
ephemeral, especially in spontaneous conversations. For linguists, this
phenomenon can be very difficult to precisely annotate for, and requires expert
ears and eyes.

For the objectives of a yet another group of researchers, double-talk situations
in conversations can actually prove to be a major source of errors. For systems
that aim to automatically transcribe or annotate speech signals, high
performance on spontaneous conversations represents their flagship task.
Hesitations, overlaps, and other disfluencies are typical in such unplanned
settings, and pose significant challenges to these speech technologies. In
speaker diarization systems, for example, which want to automatically annotate
'who spoke when' in a conversation, overlapped segments of speech contribute as
one of the main sources of errors in their state of the art performance, and
have even been referred to as their /'Achille's heel'/
\cite{anguera_speaker_2012}. Similar degradation is also observed in Automatic
Speech Recognition (ASR) in conversations where the systems fail at
disambiguating the contents of the spoken terms in regions with overlapping
speakers \cite{cetin_speaker_2006}. Appropriate detection and handling of
overlapping speech in conversations can play an important role in improving such
existing speech technologies, in addition to being directly helpful in
interactional linguistics.

The phenomenon of double-talk is discussed in more detail in the next section,
which is followed by putting it in perspective of research in speech
technologies and existing work on the task of detecting them. The technology
behind this thesis' proposed approach, Deep Learning, is introduced next. Aided
by this background, the objectives of this thesis are made concrete at the end
of this chapter.
** DONE [0/0] Double-Talk in Conversations
CLOSED: [2017-09-12 Tue 03:30]
Double-talk (also called as overlap or overlapping speech, throughout this
thesis) can be straightforwardly defined by the physical phenomenon where more
than one speakers speak at the same time. During conversations, their
charactristics, like frequency of occurrence, typical duration, content, etc.
may vary widely based on several factors. For example, competitive
conversations, like those of argumentative nature, may have more frequent and
typically longer double-talks involving a wider vocabulary, in contrast to
conversations in more cooperative or relaxed environments. Similar types of
differences can also arise when the setting is formal vs. informal, involves two
vs. more participants, or is carried out face-to-face vs. using a telephone,
etc.

#+LATEX: \begin{figure}
#+LATEX: \includegraphics[width=\textwidth]{img/example-ovl-conv}
#+LATEX: \caption{Examples of most common types of double-talk situations (shaded white) in informal conversations.}
#+LATEX: \label{fig:example-ovl-conv}
#+LATEX: \end{figure}

Double-talks become interesting in the study of turn-taking management in
conversation analysis. In the absence of any strict guidelines, participants in
a conversation have to manage when they should start speaking in order to avoid
speaking at the same time as others. There are two popular theories that try to
model how the participants find the appropriate moments to start speaking
\cite{heldner_pauses_2010}. When one speaker is active at a time, according to
the first theory, the next speaker /projects/ the possible end-time of the
current turn based on syntactic, prosodic, pragmatic or gestural
information. The other theory suggests the next speaker starts talking as a
direct /reaction/ to some signal indicating that the current speaker has
finished, or is about to finish. Overlaps can occur when there is a mismatch
between predictions made by the next speaker and the actual time the current speaker
stops talking. From both perspectives, it is expected that having contextual
information around a given time-stamp should benefit the approaches attempting
to detect double-talk occurrences. From the /projectionist/ point of view, the
contextual information should be extracted over longer durations, and it is
possible that the actual content of speech from the current speaker can also be
useful. From the /reactive/ point of view, given human reaction times can be 100
ms to 250 ms for different stimuli \cite{heldner_pauses_2010}, contextual
information from relatively smaller vicinity of a given time-stamp could be
enough. From implementation perspective, working with longer duration context
could lead to reduced temporal resolution of final detection results, so, at the
least, contextual information from ±100 ms around a given time-stamp should be
used for detecting double-talks.

Furthermore, according to both theories, most of the time during a conversation,
a single speaker is active for varying amounts of duration, and turns are taken
with (relatively) minimal gap or overlap with respect to duration. Additionally,
speakership changes occur regularly in conversations, so instances of
double-talk can be quite frequent. These can vary in different scenarios,
cultures, etc. and studying such variations is an active area of research
\cite{stivers_universals_2009,minna_stolt_many_2008}.

In Table \ref{tab:actspk-all} the ratios of number of segments of speech and
those of number of acoustic frames extracted every 10 ms (the temporal
resolution of the transciptions) over 5850 telephone based conversations in the
/Fisher English Corpus (**LDC2004T19**)/ are shown (more details in Section
[[Dataset]]). It can be seen that while segments with more than one speakers active
at the same time account for a significant portion, they contribute
proportionally much less to the overall number of individual acoustic frames.
This indicates, and is further illustrated in Figure \ref{fig:actspk-hist-all}
that the distribution of segment-lengths with overlapping speech is skewed
towards smaller durations. Overlaps longer than $4\,\text{seconds}$ are less
common and are often characteristic of the scenario of the conversation (e.g.
politically charged debates) if not otherwise a result of erroneous
transcription. For various conversational datasets, the reported median value is
typically around $0.5\,\text{seconds}$, and the distribution has a very thin
tail in longer durations \cite{heldner_pauses_2010}. Knowing this, a system that
aims to classify each frame for the number of active speakers in it will face
additional challenges due to the heavy imbalance between the classes.

#+LATEX: \begin{table}
|-----------------+---+-----------------+---+---------------|
|-----------------+---+-----------------+---+---------------|
| /# of Speakers/ |   | /# of Segments/ |   | /# of Frames/ |
|        /Active/ |   |           /(%)/ |   |         /(%)/ |
|-----------------+---+-----------------+---+---------------|
|-----------------+---+-----------------+---+---------------|
|               0 |   |           20.39 |   |          6.86 |
|               1 |   |           50.93 |   |         79.61 |
|               2 |   |           28.68 |   |         13.53 |
|-----------------+---+-----------------+---+---------------|
|-----------------+---+-----------------+---+---------------|
|           Total |   |          100.00 |   |        100.00 |
|-----------------+---+-----------------+---+---------------|
|-----------------+---+-----------------+---+---------------|
#+LATEX: \caption{Overall ratios of the number of segments and acoustic frames with different number of simultaneously active speakers in the Fisher Corpus \emph{LDC2004T19}.}
#+LATEX: \label{tab:actspk-all}
#+LATEX: \end{table}

#+LATEX: \begin{figure}
#+LATEX: \includegraphics[width=\textwidth]{img/actspk-hist-all}
#+LATEX: \caption{Histograms of durations of segments (truncated to $<4$ sec. long) with different number of active speakers in \emph{LDC2004T19}.}
#+LATEX: \label{fig:actspk-hist-all}
#+LATEX: \end{figure}

Literature in the area of linguistics classifies overlaps into further
categories. Some of the most common occurrences of overlap happen during
backchannels (listener feedbacks like "hmm", "mhm"), (mis-)anticipated turn
takings, complementary listener utterences, and laughter (Figure
\ref{fig:example-ovl-conv}). Another form of categorization is done based on
whether the overlapping speaker is competing for the turn with the active speaker, or is
simply providing additional information without any intention to take over.
These, and other categorizations are not discussed here further and the reader
should refer to \cite{minna_stolt_many_2008} for an extensive overview. Almost
all approaches for detecting overlapping speech, including the one presented in
this thesis, do not further categorize examples of overlaps, especially when
working with naturally occuring double-talk, mainly to avoid any further
decimation of what is already a minority class.

# What the analysis above motivates is to use information that can capture
# differences in the voices of the speakers involved.

A straightforward approach can be to have a system that can identify individual
speakers, as it is done in speaker identification systems. This system can then be
used to detect overlaps if it identifies multiple speakers with similar
confidence in a given segment of speech. Unfortunately, in addition to needing
prior information about all possible speakers (which is often not available in
the application phase), so far these systems do not work at high temporal
resolutions. A simple reason for this is that speaker identifying information can
only be reliably calculated over long durations.

#+LATEX: \begin{figure}
# \centering
#+LATEX: \includegraphics[width=\textwidth]{img/actspk-whist-all}
#+LATEX: \caption{Weighted histograms of durations of segments (truncated to $<4$ sec. long) with different number of active speakers in \emph{LDC2004T19}.}
#+LATEX: \label{fig:actspk-whist-all}
#+LATEX: \end{figure}

High temporal resolution in double talk detection is, nevertheless, necessary.
When the histograms in Figure \ref{fig:actspk-hist-all} are weighted by their
respective durations (Figure \ref{fig:actspk-whist-all}) to indicate the overall
proportional contribution of different segment lengths, double talk segments between 0.5 and
1.5 seconds long are seen to be the largest contributors to overlapping speech.
Applications where the detection and appropriate treatment of overlapping speech
can significantly improve the overall performance, should therefore be keenly
interested in working with such small segments. This has proven to be extremely
challenging so far, and the work for this thesis' objectives is not immune to
it either. Existing works on this and related problems are discussed in the next section.
** DONE [2/2] Double-Talk and Speech Technologies
CLOSED: [2017-09-12 Tue 02:54]
Natural conversations are one of the toughest scenarios where most of the
automated spoken language technologies have to prove their metal. In addition to
having multiple speakers, the spontaneous nature of the utterances and their
content, the presence of hesitations, self-corrections, and other disfluencies
(including double-talk situations), make conversations in meetings or other
informal settings much more challenging than planned or read speech (e.g. TV
or radio broadcast recordings). Due to the detrimental impact of overlapping
speech on their performance
\cite{cetin_speaker_2006,morgan_meeting_2001,shriberg_spontaneous_2005}, such
systems often remove them in a pre-processing step, or mitigate for such
situations with the help of extra information. In an automatic speaker
identification / verification system, for example, individual speaker models
must be learned on non-overlapping examples to ensure purity, and the system
should be robust against the presence of interfering speakers in real world
application, or at least refuse to assign a speaker by identifying presence of
competing speakers.

The poster child of research in speech technologies is Automatic Speech
Recognition (ASR), which aims to transcribe a given speech recording into text.
The systems to do this employ two major components: one to model the acoustics
(called the acoustic model), and another to model the semantics (called the
language model). While presence of overlapping speech can impact the results of
the acoustic model, these are often mitigated for when a strong language model
is used to find the best possible sequence of words being spoken. Furthermore,
since such systems are evaluated based on their Word Error Rate (WER), and knowing
that overlapped segments in speech are typically very short and typically
contain one to three words, over a long conversation, such overlaps can contribute
much less to the overall error of an ASR system. But they do contribute. In 2006,
Çetin /et al./ extensively analyzed the errors made by a then state-of-the-art
ASR system in meetings, especially with respect to locations of overlapping
speech \cite{cetin_speaker_2006}. They reported higher WERs in the final results
in regions with overlapping speakers than in clean or noisy single-speaker
regions. They also reported that the WERs were also (relatively) high in
single-speaker regions that surrounded a segment containing overlapping speech.

Such a study was not found for more recent state-of-the-art ASR systems. However,
it was interesting that in a recent paper \cite{xiong_achieving_2016}, where
Xiong /et al./ claimed achieving human parity in conversational speech
recognition, the authors pointed out that even though their system's errors
were /substantially equivalent/ to human errors, there was an exception in cases
of backchannels (e.g. "uh-huh") and hesitations (e.g. "um"). The authors did not
mention anything about overlapping speech, but it is well known that at least
most backchannels occur as overlaps. The argument then, that ASR
systems can perhaps benefit from detection and appropriate handling of such
overlaps, probably still holds, even with recent advances in ASR.
*** DONE Overlapping Speech Detection
CLOSED: [2017-09-12 Tue 02:54]
An extreme version of dealing with overlapping speech occurs in the /cocktail
party problem/. Human listeners are able to focus their attention on a single
speaker among a mixture of other conversations and background noises, for
instance, during a loud and crowded party. This perceptual /separation of
sources/ is not very difficult for humans, but is non-trivial for machines. Many
interesting approaches have been proposed under the categories of Blind Source
Separation (BSS) and Computational Auditory Scene Analysis (CASA). However,
these methods have various limitations. Nearly all of them assume that the
number of concurrent speakers is known beforehand, and BSS approaches in
particular cannot work on mono-aural (single-channel) recordings
\cite{zelenak_detection_2012}. Furthermore, these approaches assume that the
given recording /already/ consists of audio from overlapping sources in order to
separate them into the individual contributing ones, whereas the goal of overlap
detection is to detect the occurrence of such situations in the first place.
Source separation algorithms can be used after detection for appropriate
handling of such situations (e.g. to attribute the contributing speakers),
however, these approaches have been reported to face robustness issues in real
conversational settings \cite{zelenak_detection_2012}.

The challenges associated with overlap detection in real conversations were
hinted in Section [[Double-Talk in Conversations]] as: even though they are
frequent, double-talk situations are predominently of short duration, which
result in heavy imbalance between the examples for single-speaker and
overlapping-speakers classes. The conversations themselves are often recorded in
less than ideal conditions (e.g. with varying degrees of background noise).
Many studies have therefore chosen to develop and evaluate their proposed
approaches for overlap detection on artificially created overlapping speech.

In one of the more recent works \cite{shokouhi_teager_2017}, Shokouhi /et al./
proposed the use of a harmonically enhanced power-spectrogram based acoustic
feature called /Pyknogram/ for unsupervised overlapping speech detection.
Teager-Kaiser energy operators were used to estimate resonant frequencies in a
signal, and were then applied to power-spectrograms (Section [[Acoustic Features]])
in the form of a binary mask to obtain the Pyknograms. These were shown to have
relatively smooth patterns when the underlying speech was produced by a single
speaker, but had discontinuities in cases where there were multiple simultaneous
speakers. The authors proposed calculating the average distance between
neighboring units in time of the extracted Pyknograms, expecting that the
measure will be higher in segments with overlapping speech. They compared their
approach with other popular ones that used speech kurtosis, Spectral Flatness
(Section [[Speaker Diarization]]), or Spectral Autocorrelation Peak-to-Valley Ratio
(SAPVR) for overlap detection. The change in kurtosis (fourth order moment of a
random variable) of speech signal could be a consequence of increased signal
complexity, and has been used by several studies
\cite{boakye_audio_2008,wrigley_speech_2005,krishnamachari_use_2001} to indicate
the presence of overlapping speech, in particular because overlapped speech has
been shown to exhibit lower kurtosis compared to single-speaker speech (although
Zelenák reported \cite{zelenak_detection_2012} the opposite to be true when
evaluated on real conversations in meeting scenarios). In case of SAPVR, the
peak-to-valley ratios in the autocorrelation of the speech signal's spectrum are used with the
assumption that a single speaker should have a strongly periodic autocorrelation
whereas in the case of overlapping speakers, this autocorrelation should be
flatter due to overlapping harmonics.

Shokouhi /et al./ reported that their approach using Pyknograms outperformed the
other approaches under several (artificially created) conditions with different
main-speaker's to interfering-speaker's power ratios. Nevertheless, they
reported significant performance degradation when the overlapping speech
segments were less than 2 seconds long, which, setting aside issues related to
the use of artificially overlapped speech (discussed in Section [[Tackling Class
Imbalance]]), does not enthuse one to apply this method on real conversations
where such overlaps are predominently much smaller.

Earlier \cite{lewis_cochannel_2001}, Lewis /et al./ developed Pitch Prediction
Feature (PPF) for identifying whether one or two speakers were present in a
speech signal, with the basic principle that distances between successive
estimated pitch peaks are more regular in the single-speaker case than in the
two-speaker case. This feature was shown to outperform the standard acoustic
features like Linear Predictive Coding (LPC) coefficients and Mel-Spectral
Frequency Coefficients (MFCCs) (Section [[Speaker Diarization]]) when evaluated on
artificially overlapped speech.

However, Wrigley /et al./ reported \cite{wrigley_speech_2005} that MFCCs, PPF
and SAPVR did not give satisfactory results for detecting overlapping speech in
conversations. Approaches for detecting overlapping speech that have been
developed and evaluated on conversations are discussed next.
*** DONE [0/0] Speaker Diarization
CLOSED: [2017-09-12 Tue 02:37]
Over the past decade, most of the significant attention to detecting overlapping
speech in conversations has been motivated in /**speaker diarization**/ systems,
whose main task is to determine 'who spoke when' in a recording with more than
one speakers. This involves the unsupervised identification of each speaker
within a given audio stream and the intervals during which each speaker is
active. The methods are unsupervised due to the lack of prior information about
speaker identities in most application scenarios. These systems find utility in
many audio/video document processing tasks, and are integral to automatic rich
transcription of these documents for a variety of applications (e.g. indexing
and retrieval). For several conversation analysis workflows, an ideal
version of such a system is almost perfect, especially when combined with a robust
ASR system that can ultimately answer the 'who spoke when, and what'
question. In fact, speaker diarization can be a very useful preprocessing step
for other speech technologies like ASR, speaker
identification, speaker tracking, etc. For example, ASR systems, which usually
only aim to transcribe the spoken content in a given speech, can use outputs
from a speaker diarization step to concentrate on only the segments that contain speech,
and employ better speaker adaptation techniques (to compensate for speaker
specific variations) with the help of the information about the speakers present in
the recording.

#+LATEX: \begin{figure}
# \centering
#+LATEX: \includegraphics[width=\textwidth]{img/arch-diarization}
#+LATEX: \caption{General architecture of speaker diarization systems, and popular approach for handling overlapping speech.}
#+LATEX: \label{fig:arch-diarization}
#+LATEX: \end{figure}

Anguera /et al./ provide an excellent overview of various approaches used for
speaker diarization in \cite{anguera_speaker_2012}. In brief, the general
architecture for most systems (Figure \ref{fig:arch-diarization}) consists of
the following steps:

1) **Preprocessing** the raw audio data to suppress noise, extracting acoustic
   features (e.g. MFCC, LPC), removing non-speech frames, and
   performing any other domain specific processing or augmentation (e.g.
   acoustic beamforming when data from multiple microphones is available).
2) **Speaker Segmentaion** or speaker change detection to end up with speaker
   homogenous segments in time. When done separately, the most popular approach is to
   use a similarity metric like Bayesian Information Criterion (BIC) or
   Kullback-Leibler (KL)-divergence between two adjacent
   windows of relatively small size to determine if they belong to the same
   source. However, in order to avoid the propagation of errors introduced in
   this step any further, most state-of-the-art systems optimize segmentation
   and clustering simultaneously.
3) **Clustering** the same speaker segments based on some acoustic similarity metric
   (e.g. BIC, KL-divergence) to (ideally) end up with the same
   number of groups as the total number of speakers in the recording. Since the
   total number of speakers is usually not known beforehand, most systems rely on
   a heirarchial clustering algorithm, predominently by using a bottom-up
   (agglomerative) strategy where an over-clustered initialization is
   iteratively merged until a stopping criterion (e.g. BIC) is
   met. In approaches that unify the segmentation and clustering steps,
   iterative adaptation of speaker models based on current clustering
   and then subsequent re-clustering of the assignments based on current speaker models is
   carried out, predominently using GMM-HMM based models, and BIC based
   agglomerative heirarchial clustering.
4) Final **labelling** of each frame/segment of the entire recording with the
   contributing speaker cluster (predominently using Viterbi decoding), with
   possible enforcement of minimum turn durations, and any other priors and
   constraints.

Nevertheless, a fundamental limitation of most of these systems is that they
only assign one speaker to a frame (and hence, to a segment). This leads to
missed-speaker errors in segments where multiple speakers are active and, given
the high performance of some state-of-the-art systems, can be responsible for a
substantial fraction of the overall diarization error
\cite{anguera_speaker_2012, huijbregts_blame_2007, huijbregts_speaker_2012}.
Presence of such segments can potentially also degrade the speaker clusters
and models when they are not excluded in the pre-processing step
\cite{otterson_efficient_2007}. Anguera /et al./ in fact called overlapping speech
the /'Achilles heel'/ of speaker diarization for meetings.

The most common approach to solve for this employs a separate model-based
overlap detection system whose predictions about the presence of overlapping
speech can be used in the pre-processing step to /exclude/ such segments from
clustering, and then in the labelling step to signal the need to /attribute/ a
second contributing speaker
\cite{otterson_efficient_2007,boakye_overlapped_2008,zelenak_detection_2012,yella_overlapping_2014,charlet_impact_2013}
(Figure \ref{fig:arch-diarization}). The latter is usually done by choosing more
than one speakers based on the diarization system's posterior probabilities
\cite{boakye_overlapped_2008, zelenak_detection_2012}, or by choosing the two
speakers closest to the region with overlap
\cite{otterson_efficient_2007,yella_overlapping_2014,yella_overlapping_2014}.
Choosing more than two speakers was not found in literature, perhaps due to the
rarity of such situations.

Other approaches that have been proposed, which do not use a separate overlap
detection system, either do the detection by a second-pass re-segmentation of a diarization
system's output with additional (overlapping) speaker models derived from the
detected ones \cite{van_leeuwen_ami_2006}, or, integrate these combinations in
the single-pass speaker segmentation and clustering stage of the diarization system itself
\cite{lathoud_location_2003,lathoud_segmenting_2003}. These other approaches,
however, have not been shown to improve the overall diarization performance in
real conversations \cite{van_leeuwen_ami_2006}, or require information about the
total number of speakers to be known beforehand
\cite{lathoud_segmenting_2003,lathoud_location_2003}, or have only been
investigated in situations where recordings from multiple microphones are
available \cite{lathoud_segmenting_2003}.

Almost all of the proposed dedicated overlap detection systems use GMM-HMM based
framework for detecting three possible classes of speaker activity:
*/non-speech/* (silence, no speakers active), */speech/* (one speaker active),
and */overlapping speech/* (multiple speakers active). Geiger /et al./ explored
a /tandem/ LSTM-HMM and a purely LSTM based overlap detection system in
\cite{geiger_detecting_2013} for the same three classes. In some cases however,
non-speech parts are removed before the training phase by using a speech
activity detector (or by using ground-truth labels, for the sake of evaluations)
so that the models are learned for single-speech and overlapping-speech. Then
the results of the speech activity detector (or ground-truth labels) are
integrated into the decoding phase to produce the final segmentation of the
audio for the three classes.

In all GMM-HMM based frameworks, a three-state Hidden Markov Model (HMM) is
trained for each of the classes while the state emission probabilities are
modelled by using a multivariate Gaussian Mixture Model (GMM) learned from
various acoustic features. The posteriors for the classes are produced for each
frame of the acoustic feature, which are then decoded
to get the final segmentation using Viterbi Algorithm with an Overlap
Insertion Penalty (OIP) for transitions to overlapping speech (Section [[Temporal
Smoothing]]). OIP is always tuned so as to reduce the number of false detections
of overlaps since labelling the second speaker in these regions would result in
errors in dirarization performance. Such tuning often results in increased
number of missed overlaps, but in that case, the diarization performance will be
the same as that of a baseline system without overlap detection (more discussion
on this is done in Section [[Evaluation Metrics]]).

The reported overlap detection performances are in the 55% to 85% range for
precision, paired with 40% to 20% recall, when evaluated on subsets of AMI \cite{_ami_2017}
or NIST RT \cite{greenberg_rich_2009} corpora for different acoustic feature combinations. And, in
general, longer duration overlaps are more well detected than shorter duration
ones. Such steep tradeoff between precision and recall (Section [[Evaluation
Metrics]]) has lead to the claim that overlapping speech detection in
conversations is an extremely challenging problem.

Nevertheless, as is common in many classical investigations for developing a
pattern recorgnition system, various groups have explored the use of different
acoustic feature combinations in their studies for overlap detection. In
general, using only Mel-Frequency Cepstral Coefficients (MFCCs; Section [[Acoustic
Features]]) has not been found to be sufficient \cite{wrigley_speech_2005}. Their
use for speaker diarization itself (which requires invariance against /what/ is
being spoken but not /who/) and also for speech recognition (which requires
invariance against /who/ is speaking but not /what/) has, at least, lead to a
/theoretical inconsistency/ \cite{anguera_speaker_2012}. Some of the milestone
studies on overlap detection in diarization systems are discussed below.

# HMMs are one of the most commonly used probabilistic
# finite-state machines for modelling sequential data, and are still used in many
# state of the art acoustic classification systems. Recently, Long Short-Term
# Memory (LSTM) based Recurrent Neural Networks (RNNs) have been shown to be
# worthy replacement for HMMs for certain tasks
# \cite{graves_connectionist_2006,fernandez_application_2007,geiger_detecting_2013,deng_recent_2013},
# but most state of the art systems
# \cite{xiong_achieving_2016,sercu_advances_2016,} still model sequential nature
# of their data using HMMs while using deep neural networks instead of GMMs to
# model the state emission probabilties.

Boakye /et al./ \cite{boakye_overlapped_2008,boakye_audio_2008}, in one of the
earliest works with mono-aural (single channel) audio recordings, investigated
various acoustic features for training their GMM-HMM based system for detecting
overlapping speech in conversations. Using only MFCCs did not show better
performance than when they were combined with other acoustic features like
short-term Root-Mean-Squared (RMS) energy, residual energy from Linear
Predictive Coding (LPC), or the Diarization Posterior Entropy (DPE). LPC
coefficients of a speech signal are supposed to encode the
speaker-specific information while the residual energy from their
extraction process represents the error by which these coefficients fail to do
so. It was hypothesized that this residual energy will be high in regions where
more than one speakers are active simultaneously. The DPE features were
calculated as the entropy in the frame-wise speaker posteriors predicted by a
diarization system, with the hypothesis that in regions of overlapping speakers,
this entropy will be high. The best feature combination was reported to be one
where MFCCs were combined with RMS energy and DPE (along with their first order
differences), and the diarization performance was improved significantly when a
second speaker was chosen based on speaker diarization posteriors in regions
that were detected to have overlapping speech.

Zelenák /et al./ \cite{zelenak_simultaneous_2012,zelenak_detection_2012} combined
spectral features (MFFCs, Spectral Flatness (SF), and LPC residual
energy) with Time Delay Of Arrival (TDOA) based features extracted from
cross-correlation of speech signals captured by multiple microphones used in a
recording. Spectral Flatness is calculated as the ratio between the geometric
and arithmetic means of spectral magnitudes over some number of temporal frames,
and can have relatively higher values in regions of overlap than single speaker
regions. Principal Component Analysis (PCA) and Artificial Neural Networks (ANN)
were investigated to transform the cross-correlation based features to reduce
their dimensionality and also to make them independent of the number of
microphones used in the recording. While spectral features performed well, they
were outperformed when they were combined with cross-correlation based features.
Nevertheless, this approach is limited to scenarios where a recording is
available from multiple microphones.

Zelenák /et al./ \cite{zelenak_speaker_2012,zelenak_detection_2012} also
investigated the use of prosodic features for overlap detection in mono-aural
(single-channel) recordings. Prosody describes the rhythm, intonation and stress
of speech, and, as discussed earlier in Section [[Double-Talk in Conversations]], can
trigger listener responses which can occur in the form of overlaps. However,
these attributes cannot be measured directly, only their acoustic correlates can
be extracted from speech signals. Zelenák /et al./ calculated indicators like Fundamental
Frequency (the rate of vibration of the vocal cords), loudness, and (four)
Formant Frequencies using PRAAT \cite{_praat:_2017}, and also their long-term statistics like
median, minimum, maximum, standard deviation, and range, extracted over
500 ms windows every 10 ms. They performed a two stage feature selection
procedure on these features, involving first, a minimum Redundancy Maximum
Relevance (mRMR) step to individually score each candidate feature against the target
classes of single-speech vs. overlapping-speech, and second, an iterative hill
climbing wrapper approach based step where the best scoring prosodic features were
iteratively added (in order of the score in previous step) to the acoustic features (from the previous study)
until performance on a held-out dataset stopped improving. As in their previous
work, the final overlap detection performance showed improved recall but lower
precision when compared to only using the acoustic features.

Geiger /et al./ investigated \cite{vipperla_speech_2012,geiger_speech_2012-1}
using Convolutive Non-negative Sparse Coding (CNSC) for overlap detection
and attribution. The CNSC algorithm was used to decompose the /magnitude
spectrograms/ (Section [[Acoustic Features]]) of individual speakers in a
conversation into lower rank bases. The energy ratio between these bases for all
possible pairs of speakers can then be used to detect and attribute overlapping
speakers when calculated at each acoustic frame. In overlapping regions, the ratio is expected
to be nearer to unity for the contributing speaker pairs, while the ratio will
be skewed to the contributing speaker in non-overlapping regions. Compared to
the work by Boakye /et al./ \cite{boakye_overlapped_2008} on the same evaluation
set, their system showed similar precision but slightly worse recall with
respect to overlap detection. They particularly pointed out the difficulty in
detecting short-duration overlaps.

Geiger /et al./ later \cite{geiger_detecting_2013} also investigated the use of
Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs) for
detecting overlaps. A single-hidden-layer network with four memory block, each with 50
LSTM cells, was used as a linear regressor to output values in the range [-1,
1]. Overlaps were assigned the target value of 1, single speech 0, and
non-speech -1. They combined energy, spectral and voicing related features with
the CNSC based energies and ratios as the inputs to the neural network. The
outputs of the network were then concatenated with the above features to train a
GMM-HMM based overlap detector, resulting in a /tandem/ approach. The combined
features were found to work much better than when the outputs from the neural
network were not used. They also reported that using a simple threshold on the
outputs of the neural network already gave results comparable to those when the
GMM-HMM was trained on the same acoustic features, perhaps owing to the
capability of LSTMs to model long-range dependencies in time. Nevertheless, in
both their studies involving CNSC based features, the number of individual
speakers in a recording and the segments where they are active need to be known
beforehand for calculating the individual speaker bases. For this, the outputs
of a diarization system could have been used, but their experiments used
ground-truth labels to get the segmentations so as to avoid potential
degradation of speaker bases due to situations with overlaps.

Charlet /et al./ in \cite{charlet_impact_2013} focused on detecting overlapping
speech in mono-aural broadcast recordings of news, debates, etc. They tested the
use of a multi-pitch detection algorithm and compared it to an overlap detection
system based on spectral features alone. The multi-pitch system performed worse
in isolation, but the best performance was obtained when they were combined with
the spectral system. However, they concentrated on detecting overlaps longer
than one second in duration (and reported great precision and recall) because their
ultimate goal was to evaluate how can the performance of their diarization
system be improved with the handling of overlapping speech situations. In that respect, they
report significant improvement in the diarization error rate as well when such
overlapping speech segments were excluded from the clustering step and were then
used to attribute a second speaker based on nearest speakers in time.

Yella /et al./ \cite{yella_overlapping_2014} found good correlations between
(relatively) long term statistics of occurence of silence or speaker changes and
the occurence of overlaps. Over a 4 seconds long segment, the probability of
occurence of overlap became lower as the duration of silence within the segment
increased. The probability of number of occurences of overlaps increased as the
number of speaker changes within the same segment increased. This approach had
the benefit that silence and speaker change detections are relatively easy to
perform, and furthermore, these statistics generalized relatively well on other
meeting corpora when calculated on the AMI Corpus \cite{_ami_2017} alone. They reported
improved performance over an overlap detection system working purely on acoustic
features, particularly improving the recall performance.
** DONE Deep Learning
CLOSED: [2017-09-15 Fri 22:33]
A common thread in the approaches summarized in the previous section is the
focus on finding the right features (representations) to be extracted from the
raw data to best identify the presence of overlapping speech. This /feature
engineering/ is necessary in the processes of classic machine learning
technologies like Support Vector Machines (SVMs), Random Forest, GMM-HMMs, etc.
The algorithms used for learning are designed to be general purpose while also
being computationally efficient. But in doing so, these algorithms come with
characteristic assumptions and tradeoffs. Best performance from them is then
achieved when their inputs have been engineered specifically to exploit the
algorithms' capabilities and mitigate the impact of their tradeoffs. However,
such engineering of features itself requires considerable care and domain
expertise, both in the understanding of the problem domain and the machine
learning technologies. With the relevant investments, remarkable progress has
been made using these classical machine learning methods in areas of image
recognition, speech and language understanding, recommendation systems,
financial forcasts, etc.

Nevertheless, hand-crafting features for each problem is not an easy task.
Representation learning methods can allow a machine to automatically discover
the features that are the most suitable for a given task from minimally
engineered inputs. Deep learning methods are such representation learning
methods. They are capable of learning multiple levels of representations
suitable for a given problem, obtained by composing simple but non-linear
transformations of the representation at one level (starting from original
inputs) into a representation at a higher, more abstract level. With the
composition of enough such transformations, complex functions mapping the inputs
to the desired outputs can be learned \cite{lecun_deep_2015}, like, for example,
one mapping a relatively low-level acoustic feature vector extracted at a given
time to the number of speakers speaking simultaneously at that time in a
conversation's audio recording.

Popular deep learning methods use Artificial Neural Networks (ANNs) with
multiple interconnected layers of neural units inspired by the /biological/ neural
networks that constitute animal brains. Going from one layer to another, these
neural units compute a weighted sum of their inputs from the previous layer and
pass the result through a non-linear function, and all (or most) of these units are
subject to learning. With multiple layers of such non-linear transformations,
sufficiently deep architectures of ANNs (henceforth referred to as Deep Neural
Networks or DNNs) have been shown to be capable of learning heirarchial
representations starting, for an example in case of images, from the presence or
absence of edges at a location in an image near the input layer, to different
arrangements of such edges forming a motif, to their combinations that
correspond to familiar parts of an object, and later detection of these objects
as the assembly of these parts in the output layer
\cite{lecun_gradient-based_1998,farabet_learning_2013,lecun_deep_2015}.

However, to be fully realized, these capabilities require significantly more
amounts of data and the computation power to learn from them. Such challenges
made research on these technologies slow until relatively recently when wide
scale digitization has increased the amount of data available, and improved
algorithms for training DNNs have been proposed and made to run efficiently on
increasingly powerful hardware. Particularly helpful has been the availability
of implementations that can be parallelized using Graphical Processing Units
(GPUs) and even over mutliple machines, reducing the notoriously long training
times required by DNNs by orders of magnitude. With these affordances, research
and development with DNNs in a variety of problem domains has
accelerated in recent years, and with the record breaking performances and openly available
frameworks with efficient implementations of the required algorithms, the
interest and pace of research is only increasing. In fact, DNN based solutions
are already being deployed for many user facing applications of machine
learning, like object recognition, face detection and tagging in images, speech-to-text
transcription, language translation, recommendation engines, etc. that are
being provided by companies like Amazon, Facebook, Google, IBM, Microsoft,
Netflix and many others.

Most major areas of machine learning research have started adopting deep
learning methods for their respective tasks. The computer vision community,
which seeks to automate the tasks that a human visual system can do, has seen
particularly giant leaps in performance since the adoption of Deep Convolutional
Neural Networks (DCNNs) (DNNs with convolutional layers; more in Section [[Deep
Convolutional Neural Networks]]) for object recognition, image understanding,
etc., reportedly even beating human performance on certain flagship tasks
\cite{he_delving_2015,lake_human-level_2015}. The ability of DNNs to extract
heirarchial representations is more directly demonstrable in such visual tasks,
and has even lead to some interesting applications, e.g. artistic style transfer
for images \cite{gatys_neural_2015} and videos \cite{ruder_artistic_2016} where
DCNNs were shown to be capable of 'learning' the unique artisitc styles of
various painters well enough in there deep layers that these styles could be
applied to any given image with fascinating results.

Automatic Speech Recogntion (ASR) has seen similar gains in performance with
deep learning methods over the traditional methods that used GMM-HMM based
approaches. In fact, in some proposals, even architectures originally designed
for computer vision tasks have been shown to perform remarkably well when
adopted for acoustic modelling without many modifications, e.g. VGG-net
\cite{simonyan_very_2014} in \cite{xiong_achieving_2016}. Nevertheless, these
DNNs have been of the feedforward type, where the flow of data only happens in
one direction, from the input's end to the output's end, forming a directed
/acyclic/ graph. Because of this, they are not capable of modelling longer-term
temporal structure present in speech signals and work only on small windows of
extracted acoustic features at a time, traditionally requiring the use
of HMMs for producing the final sequence of the inferred text from a given speech
signal \cite{dahl_context-dependent_2012,weng_deep_2015}. Recurrent Neural
Networks (RNNs), however, are made of directed /cyclic/ graphs, and can model
such temporal and sequential dependencies on their own, but had faced certain
limitations, a particular one of /vanishing gradients/. Long-Short Term Memory
(LSTM) based RNNs have gained considerable attention for their capability to
overcome these and other limitations of conventional RNNs, and have shown promising
results, if not ground-breaking ones, not only for separate acoustic modelling
\cite{sak_learning_2015} or language modelling \cite{xiong_achieving_2016}, but
also in the realization of end-to-end ASR systems where both tasks are performed by a
single architecture \cite{graves_speech_2013,bahdanau_end--end_2015}. The reader
is encouraged to refer to Lipton /et al./'s review \cite{lipton_critical_2015}
for more details and interesting applications of RNNs.

For speaker diarization, only a handful of proposals have explored the use of
deep learning technologies. For the task of answering who spoke when in an
/unsupervised/ setting, a direct application of DNNs for an end-to-end speaker
diarization system is not straightforward. The shared idea among the proposed
methods involves learning to extract /speaker embeddings/ for speakers
previously unseen by the neural network and use these as features for speaker
segmentation and clustering in a given recording. After a neural network has
been trained for identifying speakers on a labelled training set, speaker
embeddings are the intermediate representations produced by the neural network
in its intermediate layers for inputs with an unseen speaker. These embeddings
have been shown to be more capable features than traditional ones in
discriminating between different, previously unseen speakers. For this purpose,
the proposals have used DNNs \cite{rouvier_speaker_2015,milner_dnn-based_2016},
DCNNs \cite{lukic_speaker_2016}, LSTMs \cite{bredin_tristounet:_2017}, and, very
recently, Deep Recurrent Convolutional Neural Networks \cite{cyrta_speaker_2017}.
Promising results, if not already better than relevant state-of-the-art
performances, have been reported, and further research is sure to continue in
the direction.

The above works, however, have not mentioned handling of overlapping speech in
their proposals. Apart from the work by Geiger /et al./
\cite{geiger_detecting_2013}, who proposed an LSTM-HMM based approach (discussed
in the previous section), no other published works have explored detecting
overlapping speech in conversations using deep learning technologies. While
there are strong motivations now for doing so, such lack of works to build upon
requires that the investigation of deep learning methods for overlapping speech
detection be a comprehensive one, particularly in the case of one that aims to
avoid the feature engineering prevelant in all previous works. Furthermore, the
previously mentioned challenges of using deep learning, of the need for large
amounts of representative data and of computational power to learn from them in
a reasonable amount of time, will still pose significant challenges especially
when constrained by time and hardware. Most record setting performances
mentioned in this section were achieved after training on large distributed
clusters specifically designed for such problems, and still took multiple weeks
to finish the process. Hence, it will be important to lay down the
objectives of this thesis' works more explicitly, and so is done in the next
section.
** DONE [0/0] Objectives and Expected Challenges                      :cite:
CLOSED: [2017-09-13 Wed 02:24]
The work done during this thesis is part of a larger project which aims to
improve various workflows for conversation analysis. A diarization system, at
least an ideal one, is perfect for the task of automating annotation of
conversations. As discussed in Section [[Speaker Diarization]], an overlap detection
system can help improve the overall performance of a diarization system,
especially when applied to conversational scenarios containing situations with
overlapping speech. Furthermore, as motivated in Section [[Double-Talk in
Conversations]], a system that can detect double-talk situations itself can be
directly valuable to conversation analysis. The ultimate goal of the work done
in this thesis, within this context, is the automatic detection and temporal
localization of double-talks that occur in natural conversations.

On the technical side, the objectives of this thesis include investigating the
use of deep learning technologies in realizing such an overlapping speech
detection system. As previously motivated, Deep Convolutional Neural Networks
(DCNN's), the particular deep learning technology proposed and investigated in
this thesis, can help avoid the need for manually engineering problem-specific
features (a common theme in the existing approaches), while promising ground
breaking and well generalizable results. The task of detecting overlapping
speech has proven to be extremely challenging, so it is worthwhile to
investigate the powerful promises of deep learning methods.

Nevertheless, many informed decisions need to be made in designing such a
system, from the nature of low-level acoustic features to be used as inputs, to
the configurations of various layers in the deep neural network, how they are
trained and later fine-tuned, etc. Furthermore, the characteristic challenges
present in working with naturally occurring double-talk situations, that of
short duration and the consequent imbalance with respect to their representation
in the dataset, need to be addressed using different possible approaches. These
result in a combinatorial explosion of avenues that should ideally be
investigated in a comprehensive study, and are especially warranted by the lack
of (as of yet) any existing work in open literature that use DCNNs for the task
of remedying what has been termed the /'Achille's heel'/
of speaker diarization systems \cite{anguera_speaker_2012}.

Restrictions imposed to the duration of this thesis, which are further taxed by
the amount of computation (power and) time necessary for properly working with
deep learning technologies, limited the number of possible approaches that could
be investigated in the allotted time. Therefore, priority was given to the
more straightforward approaches in the investigations reported here.
Work is expected to continue beyond this thesis' duration, and it is hoped that
it can build upon the learnings documented here.

The rest of this section formalizes the objectives alongside the expected
challenges that shape them. It is a fair summary of Chapter [[Approach]] which
will go into the details of each aspect of the approach proposed in this thesis.
They are then evaluted in Chapter [[Evaluations]]. Finally, conclusions from this
work and possible directions for future work are discussed in Chapter
[[Conclusions and Future Prospects]].
*** Setup and Assumptions
- **Acoustic Model:** :: The fundamental assumption of the work done in this
     thesis for detecting double-talk situations is that it is a purely acoustic
     phenomenon, and hence, the underlying classification task will only use
     acoustic information (in the form of low-level acoustic features) extracted
     from the audio of a given recording. The audio data will be mono-aural
     where a single stream has speech from all speakers, and no other
     modalities, like spoken content, extra microphones, etc. will be used. This makes the task
     more challenging than a mult-channel/-microphone setup, but also makes the
     solutions more versatile (Section [[Approach]]).

- **Dataset Used:** :: All experiments (trainings and evaluations) will be
     carried out on the conversational telephone speech recordings from the
     **Fisher Corpus** \cite{_fisher_2004,_fisher_2004-1}, as opposed to the
     NIST RT \cite{greenberg_rich_2009} or AMI \cite{_ami_2017} datasets used by
     most other works on this task. The choice is motivated by the fact that the
     Fisher corpus is a much larger dataset (necessary in general for deep
     learning technologies) which has a lot of /natural/ double talk situations.
     Nevertheless, this choice theoretically limits the maximum number of active
     speakers in a detected overlapping speech situation to /two/, and further only
     proves the applicability of the proposal here to recordings of telephone
     based conversations (Section [[Dataset]]).

- **Acoustic Features:** :: Based on initial experiments, relatively low-level
     acoustic features will be used for training the acoustic model, and it is
     hoped that the proposed deep neural network will be able to /learn/ the
     appropriate feature representations for the task automatically. These
     low-level features will be fixed to **64-dimensional
     $\text{log}_{10}\text{-Mel-Filterbank Coefficients}$** extracted every
     $\textbf{10\,ms}$ over a window of $\textbf{32\,ms}$ (Section [[Acoustic
     Features]]).

- **Supervised Learning:** :: Similar to other approaches, the overlap
     detection system will classify every frame of the extracted acoustic features
     into 3 classes: /**(0 speakers, 1 speaker, more than 1 speakers)**/. Consequently, how many
     speakers are active in situations of overlap is not inferred. The
     classifier is to be trained in a supervised setting, employing ground-truth
     labels for speaker activity from the transciption of the audio
     during training and evaluation (Section [[Supervised Machine Learning for Classification]]).

- **DCNN based Classifier:** :: The classifier will be based on a Deep
     Convolutional Neural Network (DCNN) whose architecture will be fixed for
     all experiments to evaluate the impact of other variables. The particular
     architecture proposed here was inspired by recent works in acoustic
     modelling with DCNNs in ASR (Section [[Deep Convolutional Neural Networks]]).
#
# - **Evaluation Metrics:** :: As done in previous works ~[]~, the effectiveness of the
#      system will be measured with respect to overlap detection in terms of the
#      frame-wise precision and recall.
#      (Section ~[]~). Nevertheless, since the dataset used for these evaluations
#      is from the Fisher corpus which is not the standard dataset used by other
#      works, the results reported in this thesis cannot be directly compared to those
#      works. Adaptations and evaluations on such datasets are planned to be
#      carried out in the future, but were not included in the objectives of this
#      thesis due to time limitations.
#
# - **Fine-Tuning:** :: Where necessary and possible, any fine-tuning or
#      comparisons of overlap detection systems will be performed by
#      giving preference to higher precision over higher recall (Section ~[]~).
*** Variables
- **Presence of Silence:** :: To measure the impact of presence of silence in
     the data on the classifier's training, configurations where such silence
     frames are removed during training will also be evaluated. The silence
     frames will be removed based on ground-truth annotations instead
     of by using an automated speech activity detection system in order to avoid
     the possible impact of additional variables brought in by such an automated
     system (Section [[Removing Silence]]).

- **Normalization of Inputs:** :: Normalization is a standard step in preparing
     inputs for many machine learning algorithms. However, since the implicit goal
     is to learn appropriately discriminative features for the existence of
     overlapping speech, the normalization of low-level features has to be done
     with some care. Impact of two standard approaches for normalizing speech
     signals will be investigated, and compared to the baseline approach without
     normalization (which is common in systems that want to perserve speaker
     discriminative information in the inputs). The two normalization approaches
     to evaluate are Mean Substraction, and further Variance Normalization
     (Section [[Normalization]]).

- **Tackling Class Imbalance:** :: The most potent challenge in detecting
     naturally occurring double-talk situations in a supervised machine learning
     framework is the inherent imbalance between the number of examples
     available for the classes. The most promising approach within this context,
     one involving the re-balancing of examples from different classes while
     training, will be compared against the baseline case where no such
     re-sampling is done (other possible approaches are also discussed in
     Section [[Tackling Class Imbalance]]).

- **Temporal Smoothing:** :: The raw predictions from the frame-wise classifier
     do not exploit the longer-term temporal patterns that are present in a
     conversation. The impact and possible improvement in performance by using
     the Viterbi algorithm for temporally smoothing the raw predictions will
     also be analyzed (Section [[Temporal Smoothing]]).
* CANC [13/13] Approach
CLOSED: [2017-09-17 Sun 00:22]
The essence of the methodolgy used in the work done for this thesis is that of
designing, implementing and evaluating a model based automatic pattern
recognition system.

The task of detecting and temporally localizing occurrences of double-talk in the
/mono-aural audio recording/ of a conversation is performed by using a /Deep
Convolutional Neural Network (DCNN)/ based classifier which was trained on (a subset of)
the /Fisher Corpus/ while working with /low-level acoustic features/ as inputs.

The choice of using only acoustic information for making such predictions comes
from the fundamental assumption of the work done in this and other related works
so far. This assumption is that the presence of overlapping speakers can be
reliably detected from acoustic information alone. Such an assumption is obvious
to make since humans are capable of doing so. Nevertheless, it is possible that
other modalities like an accompanying video recording, or a corresponding
transcription, could help the classifier by augmenting the available
information. But such approaches were not found in published works, and are
necessarily limited in application to situations where these extra modalities
are available. Furthermore, a system that can perform well by only using the
audio of a conversation, which is necessary for any conversation analysis task
anyway, can definitely be applied to conversations that have an accompanying
video recording or a transcription available.

Similar arguments explain the restriction to /mono-aural/ audio recordings.
Although research exists where the availability of multi-channel or
multi-microphone recordings has been shown to improve results for this task,
extra modalities of such types are not available for many situations where the
proposed system has to be ultimately deployed to. Merging multiple channels into
one could be done trivially (although sophisticated approaches do exist
\cite{zelenak_detection_2012}), and a system that can work reliably under these
lowest-common-denominator settings, although could face significant challenges
during development, will nevertheless be ultimately more versatile. (However, it
must be pointed out that the audio recordings available in the Fisher Corpus are
/technically/ dual channel (1 channel per speaker), but these were merged into one
channel following a trivial and reproducible method before being used for
acoustic feature extraction; more details in Section [[Preparation and
Analysis]].)

The choice of the /Fisher Corpus/ was made necessary to appropriately train the
DCNN. As discussed earlier (Section [[Double-Talk in Conversations]]), the total
number of examples with overlapping speech (in terms of duration) is very small
in a given conversation, whereas deep learning technologies typically generalize
well on unseen data only after being trained on a large number of examples.
Furthermore, the choice helps avoid various pitfalls involved in generating
artificially overlapped data by having a good amount of naturally occurring overlap
situations. Nevertheless, there are no existing works that have used this
dataset for this particular task, therefore the evaluations of the proposed
system presented in Section [[Evaluations]] are not directly comparable to any
existing works.

The choice of using a /DCNN based classifier/ itself is motivated by the
ambition to circumvent the need for manual feature engineering that has been
prevalent in previous works. As discussed earlier (Section [[Deep Learning]]),
DCNN's have been shown to have the ability to learn both low- and high-level
representations relevant for a task from minimally processed inputs, and have
achieved record-breaking performance on multiple occasions in recent years.
Acoustic features were still extracted from the audio before being fed into the
DCNN, but they were kept to be fairly low-level ones, and the impact of certain
simple pre-processing methods have been experimented with.

All of the above aspects and related challenges are discussed in appropriate
detail in the following sections of this chapter, and some highlights from the
implementation perspective are provided at the end (Section [[Implementation --
Highlights]]). The final application of this work in a speaker diarization system
(introduced in Section [[Speaker Diarization]]) was however not within the purview
of this thesis and is therefore not discussed.
** DONE Supervised Machine Learning for Classification
CLOSED: [2017-09-16 Sat 21:52]
The ultimate goal of a classifier is to map a new observation to a category (or
class) given what has been /learned/ from the categorization of perviously seen
observations, where the set of possible categories is finite and predefined
(e.g. whether, or not, more than one speakers are speaking simultaneously at a
given time-stamp). When the categorization of previously seen observations (training
data) is known, /supervised learning/ methods use this information to /train/
the appropriate classifier, whereas /unsupervised learning/ methods do not have
this categorization available (or do not use them) for such training (e.g.
speaker clustering in Section [[Speaker Diarization]]).

The problem of detecting and temporally localizing double-talk was formulated
the following way in this thesis: the proposed DCNN based classifier (with
parameters \theta) predicts the conditional probability $p(C_j | \textbf{x}_t,
\theta)$ of each acoustic feature vector $\textbf{x}_t$ extracted from the audio
at time $t$ for having speech from the $j^{th}$ class of $C =$ /**{0 speakers, 1
speaker, more than 1 speakers}**/. The final decision $y_t$ can then be made by
choosing the class that was assigned with the maximum probability, as $$y_t =
\argmax_j p(y_t = C_j | \textbf{x}_t, \theta) \text{.}$$

The three classes mentioned above are more or less in line with the ones used by
other works (Section [[Speaker Diarization]]). An immediate possible extension would
have been to include a class for non-speech yet non-silence related acoustic events (e.g. bird
sounds, etc.), however, though present in the dataset that was used in this
thesis, such events were not annotated for at all in the available transcripts,
and hence this extra class was not used. Nevertheless, this means that the
results from application of the learned classifier on recordings with such non-speech
related events is undefined.

Another possible formulation could have directly predicted the number of
speakers speaking simultaneously at a given instance, instead of lumping all
cases of more than one speakers being active into one class. Such instances,
where more than two speakers are active at the same time, are very rare in most
natural conversations, and, furthermore, were not present in the dataset
that was used for training (Section [[Dataset]]) . Therefore, in a stricter setting,
the decision by the proposed classifier of presence more than one speakers being
active at the same time is only well defined for cases where there are utmost
two speakers speaking at the same time.
*** DONE Temporal Smoothing
CLOSED: [2017-09-14 Thu 15:30]
So far, for the sequence of acoustic frames
$\textbf{X}=\textbf{x}_1,\textbf{x}_2,\textbf{x}_3,\ldots,\textbf{x}_T$
extracted from a given speech recording, the corresponding sequence of labels
$\textbf{y}=y_1,y_2,y_3,\ldots,y_T$ are obtained by choosing the most probable
class for each acoustic frame $\textbf{x}_t$ as predicted by the classifier.
This simple /decoding/ of the sequence of labels from the class posteriors can
give good enough results, but in doing so, one would not be considering the
temporal relationships that exist within this sequence of labels in reality.
Individual instances of the three events mentioned earlier can last for multiple
acoustic frames, and the statistics of these durations can be very
characteristic of the events in question. And, knowing about how turn-taking
usually occurs in conversations (Section [[Double-Talk in Conversations]]), that
speakership changes are frequent and occur with gaps and overlaps, one can
also calculate the statistics regarding the transitions between these types of
events during a conversation.

If the classifier itself was designed to be able to model such long-term
statistics and dependencies, as is in the case of Recurrent Neural Networks
(RNNs), the results could be expected to be much improved even with the naive
decoding scheme. Nevertheless, as discussed in Section [[Deep Learning]], such
methods are very computationally expensive, and were not investigated for
overlapping speech detection in this thesis. The DCNN proposed in this thesis
works on a small contextual window around a given acoustic frame for its
predictions in order to compensate for this shortcoming (more discussion in
Section [[Acoustic Features]]).

A decoding scheme, then, that can incorporate such statistics can potentially
perform much better than the naive one being used so far for the proposed
classifier. It is very common for realistic frame-wise classifiers to
produce relatively noisy predictions, and the /raw/ sequence of labels
$\textbf{y}^{(raw)}$ obtained using the naive decoding method could end up
containing such short contiguous durations of these events which might be
physically impossible (or at least highly unlikely). It is therefore common to
/smooth/ out such erroneous predictions using more sophisticated decoding
schemes to obtain a better sequence of labels $\textbf{y}^{(smooth)}$.

The most popular approach for modelling sequential data is done with the use
Markov models. A discrete sequence of observations is understood to have been
produced by a system that switches states (between a discrete set of possible
states) at discrete time intervals, possibly even looping back to the same state
at the next time-stamp. When such a system is modelled to have the Markov
property, it is assumed that the state of the system at a point in time only
depends on its state at the immediately previous one. Furthermore, it is assumed
that this relationship between states, of /transitioning/ from the predecessor
to the current state, does not change over the time-intervals that the
observations are made. Lastly, when the observations are not direct measurements
of the states of the system but are instead modelled to have a probabilistic
relationship with them, leading to the actual sequence of state transitions
being effectively /hidden/, the model then becomes the Hidden Markov Model
(HMM) which is prevelant in speech technologies.

The Markov model has two parameters: the /initial state/ probabilty distribution
for a sequence to begin with the system in a particular state, and, the /state
transition/ probabilty distribution for the system to move from one state to
another over the time-stamps in a sequence. In case of HMMs, there is an
additional /state emission/ probability distribution for the likelihood of an
observation being produced by the system while it is in a (hidden) state. Once
an appropriate number of states the system has and the above parameters for them
are determined, this model can then be used for analyzing sequential data in a
variety of application domains (more
details on the involved methods and algorithms can be found in the excellent
tutorial by Rabiner in \cite{rabiner_tutorial_1989}). Various speech
technologies like those for ASR, speaker diarization, and even overlap detection
systems discussed in Section [[Double-Talk and Speech Technologies]] have been using
HMMs for modeling the sequential nature of relevant events that occur in speech
signals. One of the most relevant use case for these models, especially with respect to the
problems being discussed in this section, is in decoding the appropriate
sequence of words (in ASR), or speaker activity (in speaker diarization), etc.

For the current task, then, a conversation can be modelled to be in one of three
states at a given time-stamp, depending on how many speakers are speaking
simultaneously at that time. The outputs of the classifier are then posterior
probabilities of being in one of these states for a given input. A decoding
method can then be used to find the best sequence of states
$\textbf{y}^{(smooth)}$ that maximizes being in a particular state at a
particular time-stamp over the entire sequence of these state posteriors over a
given audio recording. The most popular algorithm to determine this
sequence in a maximum-likelihood setting is the Viterbi decoding algorithm
\cite{rabiner_tutorial_1989}. The various statistics of durations and
transitions mentioned in the first paragraph of this section will be made
available to this decoding algorithm in the initial and the state transition
probabilities after being calculated on the training dataset.
#+LATEX: \begin{figure}
\centering
#+LATEX: \includegraphics[width=0.8\textwidth]{img/smoothing-hmm}
#+LATEX: \caption{Topology of states used in Viterbi decoding for temporal smoothing, where
$a_{ij}$ are probabilities of transitioning from state $C_i$ to $C_j$.
#+latex: }
#+LATEX: \label{fig:smoothing-hmm}
#+LATEX: \end{figure}

The state transition topology that was used for Viterbi algorithm based decoding
is shown in Figure \ref{fig:smoothing-hmm}, and is similar to the topologies
used for this purpose by other studies mentioned in Section [[Speaker Diarization]].
Many of those studies also applied an Overlap Insertion Penalty (OIP) on the
transition from single-speaker state to overlapping speech state, and completely
forbid some other transitions entirely (like ones between non-speech and
overlapping speech). This was explained to have been done to reduce false alarms
of overlaps (for reasons discussed in Section [[Evaluation Metrics]]). Nevertheless,
in the implementation for this thesis' works, no such penalization was used.

In Figure \ref{fig:smoothing-preds}, the top third of the rectangle shows the
sequence of ground-truth labels for a sample audio recording, the middle third
shows the sequence of predicted labels as decoded using the naive method
($\textbf{y}^{(raw)}$), and the bottom third shows the sequence of predicted
labels as decoded by using the Viterbi Algorithm ($\textbf{y}^{(smooth)}$). In
each rectangle, grey (darkest) color signifies non-speech, green (lightest)
color signifies single-speaker speech, and red (medium intensity) color
signifies overlapping speech, at the respective instances in time. It can be
seen that $\textbf{y}^{(raw)}$ consists of a lot of very short predictions for
overlaps, mostly predicted at wrong time-stamps with respect to the ground truth
(false alarms). However, the application of temporal smoothing in obtaining
$\textbf{y}^{(smooth)}$ removes almost all such short erroneous predictions,
while often also filling in any missed in the middle of some long segments.
However, it can be seen that such temporal smoothing cannot compensate for all
types of errors. Certain erroneously detected overlaps, particularly those
detected close to each other are 'filled in' to make even longer duration false
detections of overlaps in the final predictions (e.g. around
$26\,\text{seconds}$ mark). Also, some overlap situations that were successfully
decoded by the naive decoding are erroneously removed in the temporally smoothed
predictions (e.g. around $12\,\text{seconds}$ mark).
#+LATEX: \begin{figure}
\centering
#+LATEX: \includegraphics[width=\textwidth]{img/eval/mn-skipzero21-preds}
#+LATEX: \caption{True labels, Raw predictions, and Smoothed predictions for part of a conversation.
        /Time axis is in *minutes:seconds*./
#+latex: }
#+LATEX: \label{fig:smoothing-preds}
#+LATEX: \end{figure}

Nevertheless, on average, this promises to obtain much better results than if
only the raw predictions would have been used directly. It is expected that by
/smoothing over/ the predictions, the decoding with Viterbi algorithm will
improve the precision of the predictions when compared to the raw ones, but most
likely at the expense of recall due to removal of some short yet correctly
detected overlaps (precision and recall are discussed in Section [[Evaluation
Metrics]]). Therefore, for all experiments performed during this thesis' works,
both the predictions, one obtained by the naive decoding method
($\textbf{y}^{(raw)}$), and the ones obtained after applying temporal smoothing
with the Viterbi algorithm based decoder ($\textbf{y}^{(smooth)}$), will be
reported.
*** DONE [0/0] Removing Silence
CLOSED: [2017-09-13 Wed 02:08]
It can be argued that the presence of silence frames in the training input can
degrade the performance of the classifier with respect to discriminating between
single speaker frames and overlapping speech frames. Silence, or lack of speech,
can be much more easily discriminable than speech from any number of speakers,
while discriminating between speech produced by a single speaker and that
produced by multiple speakers simultaneously can, even in isolation, prove
difficult. This may lead to the iterative gradient descent procedure used for
training a neural network getting stuck in a rather steep local minima where the
classifier's objective for detecting silence vs. speech (from any number of
speakers) could be so well met that moving on to other minima is too expensive.
This becomes even more challenging when the classes are as imbalanced as they
are in the present case.

In early experiments with simpler neural network architectures, it was indeed
observed that the classifier achieved very good results with respect to
discriminating between silence and the other two classes, while the performance
was not at all satisfactory in discriminating between the other two classes. It
is possible, however, that a more powerful network, like the proposed DCNN
(Section [[Deep Convolutional Neural Networks]]), with an order of magnitude more
number of learnable parameters, will be able to overcome this issue.
Nevertheless, detecting silence could be performed by much simpler methods than
such a complicated network. In fact, it is part of the standard procedures for
most state of the art speech technologies (e.g. ASR, Speaker Diarization,
Speaker Identification, etc.) to use a speech activity detector in an early
pre-processing step to remove segments with silence before the audio is passed
on to the next steps. Most of the previous works discussed in Section [[Speaker
Diarization]] also removed silence frames before training for speech and
overlapping-speech classes.

To study the possible impact that presence of silence can have on the
performance of the classifier, a set of experiments that were performed and have
been reported in this thesis included a configuration where the silence frames
had been removed from the input data during training. For this, ground-truth
annotations were used in order to avoid any impact on performance that may get
introduced by using an automatic speech activity detector. During evaluations
with temporal smoothing, the predictions of the classifier in known regions of
silence (based on ground-truth labels) were replaced with perfect predictions of
silence before temporal smoothing was applied.
*** DONE [0/0] Tackling Class Imbalance
CLOSED: [2017-09-12 Tue 22:02]
It was shown in Section [[Double-Talk in Conversations]] that, even though
individual double talk situations can occur quite frequently in normal
conversations, their predominently small duration lead them to have a much
smaller share of the final number of acoustic frames. This imbalance in
representative number of examples available for each class, especially when
detecting the disadvantaged class is the primary goal of the exercise, could
prove devastating when the total amount of training data available is too small.
And can be further exacerbated when there can be significant variations within
the minority class or if the minority class is difficult to distinguish from the
dominant one.

This imbalance is arguably one of the most potent source of issues in detecting
double-talk situations in natural conversations. In previous works (Section
[[Speaker Diarization]]), since almost all of them used a GMM-HMM framework for overlap
detection, such imbalance in availability of examples required them to use GMMs
with fewer components to model overlapping speech than for modelling other
classes. One can argue that overlapping speech demonstrates much more
variability within the class than could be suitably modelled with such few
components. However, with the small datasets that were used by those works, such
decisions may not be entirely in one's control.

There are a few approaches that were considered to solve for the class imbalance
problem during this thesis' work. The most important one was to choose the
Fisher Corpus over other datasets (e.g. AMI, NIST RT) that have previously been used by
other studies on detecting overlaps in conversations. The Fisher Corpus is
sufficiently large (Section [[Dataset]]) and almost all of the telephone
conversations in it have naturally occuring double talk situations. The classes
remain imbalanced, but there are more number individual frames with overlapping
speech available in this dataset than there are /total/ number of frames in some
other datasets of conversations. /Quantity has a quality all it's own./

Furthermore, since the dataset has natural conversations, the pitfalls of
using artificially generated overlapping speech are avoided. For example,
certain vocal events like laughter, or certain utterances like those used as
backchannels (e.g. "hmm", "m-hm",), which often and almost exclusively
occur in natural double-talk situations, are difficult to account for while
generating artificially overlapped speech from single speaker utterances.
Speakers use very different intonations, pace, volume, etc. when speaking alone
vs. when in overlapping situations. A natural conversation can be riddled with
many disfluencies like hesitations, repetitions, etc. or the recording
conditions themselves may be not as 'clean' as the more planned or read
speech used for creating artificial overlaps. In Section [[Overlapping Speech
Detection]], several studies were mentioned where artificially created overlapping
speech were successfully detected, but there have not been any such publications
so far which have demonstrated successful application of such overlap detection
systems in natural conversational scenarios.

Nevertheless, since the imbalance between classes still exists in the dataset
that was used in this thesis, other approaches to mitigate the issue because of
it were also investigated, and are discussed next.
***** Rebalancing Training Data
One of the most widely considered approaches to tackle imbalanced classes is to
use a biased sampling strategy for choosing examples from the dataset such that
the classifier sees a balanced representation from each class during training.
Such a goal can be achieved by either under-sampling from the examples for the
majority class(es), or over-sampling those for the minority class(es), or by
doing both simultaneously. Several algorithms exist to carry out such under- or
over-sampling, and more complicated methods might be warranted in situations
where the total size of the dataset is small.

Over-sampling from the examples of the minority class is the more popular
approach taken by many studies where such imbalance in classes exist
\cite{wang_training_2016,_learning_2016}. The argument in favor of this
technique is that doing this does not introduce more information, versus
under-sampling where potentially useful information is being thrown away.
However, a possible argument against this technique, at least when implemented
naively by simply duplicating random sets of examples, and made worse in case of
significant imbalance, is that the variables associated with such an
over-sampled class can appear to have lower variance than they do, and can lead
to overfitting of the classifier to the training set which will not generalize
well later on. There exist many techniques that can solve for this (e.g.
Synthetic Minority Oversampling (SMOTE)) where, instead of naively duplicating
examples, new examples from the minority class are created artificially
following some procedure which should not impact the ultimate classification
task. In computer vision tasks, for example, new samples can be created by
flipping or rotating existing samples, and have shown to improve
performance of the classifier on imbalanced classes.

Nevertheless, over-sampling from speech samples is usually not trivial. Popular
transformations involve warping of the feature vectors, but for the task of
detecting overlapping speech, such transformations can potentially negatively
impact the speaker-discriminative information available in an example. A
different approach would have been to simply create artificially overlapped
speech frames, but the problems associated with this approach have been
discussed earlier. Furthermore, given the limitations of hardware and time,
doing such over-sampling to satisfactorily reduce the disparity between the
classes involved in this thesis would have increased the size of the already
large dataset to impractical proportions, especially in terms of the amount of
training time required. Consequently, taking this approach for rebalancing the
training examples remains a task to investigate in future works.

On the other hand, availability of a large dataset can make the decision to
under-sample from the majority class a more comfortable one. It is often
recommended that such under-sampling should be done in regions of the feature
space which can lead to the most confusion between the majority and the minority
class, as opposed to naive uniform skipping of examples. This way, the
classifier will be able to learn potentially more useful information for
discriminating between the classes, rather than trying to model all possible
occurences of each. These will be the boundary regions between the two classes,
and in the case of overlapping situations, the frames near the transition between
the segments of single speaker activity and multi-speaker activity. This
under-sampling near the boundary was not done during this thesis' work, and a
relatively more straightforward approach was taken.
Due to inaccuracies in ground truth annotations in most datasets
of conversations, sampling solely from such transition regions could have
resulted in the under-sampled class being represented by bad examples.

A mirrorring argument from earlier can also be put forward against
naive strategies for under-sampling from all examples of the majority class,
where the under-sampled class can appear to have higher variance in it's
variables than the actual distribution. In large conversational speech datasets,
the biggest source of variance arguably stems from the individual speakers'
characteristics. Care should therefore be taken so that each of the speakers in
the dataset are proportionally represented in the results of the under-sampling
procedure. Unfortunately, there is no way to identify a speaker
uniquely in the Fisher corpus (Section [[Dataset]]). What is obviously guaranteed,
however, is that the two speakers within a particular conversation will be
different.

Therefore, the under-sampling that was performed in the experiments in this
thesis was done on a per-single-speaker-segment basis. All segments with only
a single speaker speaking were collected, and within each, frames were skipped
with uniform probability based on the total duration the speaker in that segment
was active during the conversation. This procedure at least approximates the
goal that each individual speaker is equally represented over the entire
dataset. Furthermore, the probability of picking a sample was tuned so that the
final under-sampled dataset has a ratio of 2:1 between single-speaker and
overlapping-speaker classes. This decision, as opposed to targetting a 1:1
ratio, can be intuitively explained as an attempt to represent each speaker and
their combinations equally. This is, again, an approximation, and future works
should investigate different parameters or other rebalancing techniques
\cite{huang_learning_2016} to achieve such goals if they are deemed worthwhile.

In the implementation, the under-sampling was performed on the fly while preparing
the inputs for each epoch of training the DCNN. No such under-sampling was
performed while choosing samples from the overlapping speech class in any
experiments. Furthermore, afforded by the reduced number of total training
examples, and supported by the desire to avoid any impact on convergence or of
biased variance for the under-sampled class, in experiments where such
under-sampling was performed, the neural network was trained for at least twice
as many epochs than in experiments where none of the classes were under-sampled.
Lastly, in all configurations where such undersampling was performed, silence
frames were skipped, to avoid the impact of such frames on training (Section
[[Removing Silence]]).

It should be noted that such under-sampling was only performed while preparing the
training examples, and were not performed on either the validation or testing
examples during evaluations.
***** Cost Sensitive Objective
While training a classifier, it's parameters are tuned with the objective of
minimizing it's misclassification rate, which is based on measures of the
errors the classifier makes in assigning categories to the inputs.

In cases where misclassifying instances from a particular class can be more
costly, the error measures can be biased for this class by some fixed or derived cost so that such
misclassifications can have a larger impact on the tuning of the parameters of
the classifier. This approach can also be employed in tackling class imbalance,
where the error measures for misclassifying the minority class can be scaled with some
cost (based on some priors) that is higher than the scaling done for the
majority class.

In implementation, for the categorical cross-entropy loss function used for
training the DCNN (Section [[Training]]), experiments were performed with: a fixed
cost of twice for overlapping speech class compared to that for non-overlapping
speech; and costs based on priors derived from the training set. In either case,
however, the DCNN training did not converge even after many times more the
number of epochs for other experiments. It is possible that there were some
issues with the particular implementation that was used, or that both choices of
the cost used were inappropriate. It is also possible that the adverse impact of
inaccuracies in the ground-truth labels could have been magnified by using such
costs.

Nevertheless, more experiments could not be performed within the time
limitations. The results of many attempts at taking this approach for tackling
class imbalance were considered inconclusive and have not been reported. It will
thus be an open avenue for systematic research in future works where other
objective functions \cite{wang_training_2016,dalyac_tackling_2014} that might be
more appropriate should also be experimented with.
*** DONE [0/0] Evaluation Metrics
CLOSED: [2017-09-12 Tue 22:21]
The imbalance between the classes makes using simple summary metrics for
evaluating an overlap detection system less informative, and, sometimes,
misguiding. Considering a toy example, a classifier that predicts every sample to belong to the
single-speaker class will achieve an overall $81.20\%$ accuracy score (on the
testing set in Section [[Preparation and Analysis]]).

The two types of errors that an overlap detection system can make on a speech
recording are: the total duration of missed overlaps $T_{miss}^{(ov)}$, and the
total duration of falsely detected overlaps (False Alarms)
$T_{false}^{(ov)}$. /Precision/ and /Recall/ are used for the reporting these errors, made
by a system that assigns a total duration of $T_{sys}^{(ov)}$ as overlaps in
the recording which is known to have a total duration of $T_{ref}^{(ov)}$ overlaps.

/Precision/ ($P^{(ov)}$) is the proportion of times that the overlap detection
system correctly identified an overlap, with respect to all of its decisions of
existence of overlap, calculated as: $$P^{(ov)} = \frac{T_{sys}^{(ov)} -
T_{false}^{(ov)}}{T_{sys}^{(ov)}} = \frac{T_{ref}^{(ov)} -
T_{miss}^{(ov)}}{T_{sys}^{(ov)}} \text{.}$$

/Recall/ ($R^{(ov)}$) is the proportion of times that the system was correct in
its decision for detecting overlaps, with respect to the amount of times
overlaps actually were known to be present, calculated as: $$R^{(ov)} =
\frac{T_{sys}^{(ov)} - T_{false}^{(ov)}}{T_{ref}^{(ov)}} =
\frac{T_{ref}^{(ov)} - T_{miss}^{(ov)}}{T_{ref}^{(ov)}} \text{.}$$

Some related works on overlap detection
\cite{boakye_overlapped_2008,geiger_detecting_2013,geiger_speech_2012-1} also
report the $F-measure$, which is the harmonic mean of precision and recall
defined above, but it seemed redundant and has not been reported in this thesis.

Precision and recall, being ratios with value between 0 and 1, will be reported
as percentages ($\%$) in this thesis. A perfect overlap detection system will
then achieve $100\%$ score on both $P^{(ov)}$ and $R^{(ov)}$, while the classifier
from the toy example above will achieve $0\%$ on both the metrics. Real
classifiers, however, are not this perfect in being either good or bad.

In most practical scenarios, there will be a tradeoff between being able to
detect more number of overlap situations (high recall) versus being precise
about these detections (high precision). In situations where the classifier can
be tuned to prefer one over the other (e.g. by tuning the Overlap Insertion
Penalty), almost all studies that use an overlap detection system in a speaker
diarization system prefer higher precision (with possibly low recall) over
higher recall (with possibly low precision). False alarms (low precision)
directly impact the diarization performance of the system since the extra
speakers that will be predicted in such situations will certainly be errorneous
decisions (Section [[Speaker Diarization]]). Missed overlaps (low recall) will
result in missing the chance to predict extra speakers in a segment of speech,
but then the system performance in such a situation will be at least equivalent
to one that does not use overlap detection at all.

Furthemore, for conversation analysis, a high precision detection of overlapping
situations can at least detect the time points that a linguist can later
concentrate on to annotate manually. Too many false alarms may prove
frustrating in this situation.

Similar precision and recall metrics were also used for the silence (non-speech)
class (as $P^{(no)}$ and $R^{(no)}$) and the single-speaker speech class (as
$P^{(sp)}$ and $R^{(sp)}$), and will be appropriately reported during evaluations.
** DONE [2/2] Dataset
CLOSED: [2017-09-13 Wed 02:08]
# #+latex: \afterpage{
#+LATEX: \begin{table}
|--------------------------------------------+--------------|
|--------------------------------------------+--------------|
| /Dataset [ref.]/                           | /Rough Size/ |
|                                            |    /(hours)/ |
|--------------------------------------------+--------------|
|--------------------------------------------+--------------|
| AMI \cite{_ami_2017}                       |          100 |
| NIST RT \cite{greenberg_rich_2009}         |           11 |
| ICSI \cite{morgan_meeting_2001}            |           40 |
|--------------------------------------------+--------------|
| Fisher Corpus - Part 1 \cite{_fisher_2004} |          961 |
|--------------------------------------------+--------------|
|--------------------------------------------+--------------|
#+LATEX: \caption{Rough sizes of different conversational datasets used in
overlapping speech detection, and that of the Fisher Corpus
/(NIST RT size as reported in \cite{zelenak_detection_2012})/.
#+LATEX: }
#+LATEX: \label{tab:data-sizes}
#+LATEX: \end{table}
# #+LATEX: }
The dataset used in training (and evaluating) a classifier should
be representative of goals the of the task. As has been mentioned earlier, for
the task of building an overlapping speech detection system, datasets like AMI
\cite{_ami_2017}, NIST RT \cite{greenberg_rich_2009}, ICSI
\cite{morgan_meeting_2001}, and others \cite{cetin_speaker_2006} have been used by
most of the previous works. These corpora are made up of annotated audio (and
sometimes also video) recordings from different meeting scenarios where the
number of participants in a particular recording can be between 4 (most common)
to 11 (maximum). The audios are usualy recorded from multiple microphones,
placed near each speaker or on a table shared by the participants. These corpora
are primarily used in evaluating state of the art speaker diarization systems
\cite{anguera_speaker_2012}.

Pertinent to the task of detecting overlapping speech, there are a fair number
of examples of the relevant situations available in most such recordings (in
fact, in some cases there could be as many as 4 speakers active simultaneously).
These corpora are thus suitable for training an overlapping speech detection
system. However, it was observed that all previous works used only a subset
of these datasets for developing and evaluating their systems. In some cases,
limitations were imposed by the proposed approach, e.g. the requirement of data
from multiple microphones \cite{zelenak_detection_2012}, while in others, it seems this was done to keep
the results comparable to other works
\cite{geiger_speech_2012-1,geiger_detecting_2013,yella_overlapping_2014}.

Nevertheless, due to time limitations, during this thesis' work, comparing with
existing works was given a lower priority than comprehensively investigating the
use of deep learning technologies for the task. For appropriately training deep
neural networks, it is almost necessary to have a large training set available.
The Fisher Corpus is many times larger than all other standard datasets
/combined/ (Table \ref{tab:data-sizes}), and furthermore, also has naturally occurring double talk
situations. Other reasons for choosing this large a dataset have previously been
explained in appropriate context. The choice was thus made to limit the work done to this particular
corpus, while any adaptations and evaluations on the other datasets, which are
nevertheless warranted for making any worthwhile claims, are scheduled to be
carried out in future beyond this thesis' submission.
*** DONE [0/0] Overview
CLOSED: [2017-09-13 Wed 00:01]
The Fisher English training corpus
\cite{cieri_switchboard_2003,_fisher_2004,_fisher_2004-1} was made available by
the Linguistic Data Consortium (LDC; \cite{_language_2017}) in two parts, in
2004 and 2005, catalogued as */LDC2004S13/* and */LDC2005S13/* containing speech
data and */LDC2004T19/* and */LDC2005T19/* containing the corresponding
transcripts. Taken as a whole, the corpus is made up of 11,699 (= 5,850 + 5,849)
recorded telephone conversations, each given a unique 5-digit ~CALLID~, starting
from ~00001~. The corpus is predominently used in conversational and
large-vocabulary speech recorgnition systems in literature
\cite{xiong_achieving_2016,chen_advances_2006}. For this thesis, only data from
the first part (*/LDC2004S13/* \cite{_fisher_2004} and */LDC2004T19/*
\cite{_fisher_2004-1}) were used.

Each conversation is upto 10 minutes long, and is carried out between two
participants in English on a provided topic. Over 12,000 participants were
initially recruited, including both native and non-native speakers of English,
and each were assigned a unique ~PIN~. However, due to the procedures used while
collecting these recordings, it is not guaranteed that the same ~PIN~ in
different calls represent the same speaker. Therefore, it is also not possible
to determine exactly how many unique speakers are present in the entire dataset.
But it is obviously guaranteed that the two speakers within a call are not the
same.

The audios are available in NIST SPHERE format containing two channels (one
channel dedicated to each speaker's side in the telephone conversations) sampled
at 8,000 Hz. The corresponding transcripts are available as plain text files
(example in Figure \ref{fig:data-eg-transcript}) alongside a separate database
with information about the recording situation and the speakers for each call.

The transcripts were created by first performing automatic speech detection on
each channel of the audio data to identify start- and end-points (in seconds) of
utterances in that channel, and then the spoken content of these utterances were
transcribed manually. The final transcript file then has one line per utterance,
with start- and end-time stamps and the corresponding channel/speaker as "A:"
for channel 1, "B:" for channel 2 (Figure \ref{fig:data-eg-transcript}). The
maximum resolution (theoritically minimum segment length) of these time-stamps
is $10\,\text{ms}$, which govern the frame-rate of the acoustic
feature extraction process (Section [[Acoustic Features]]) and how various duration
related statistics are reported in this thesis.
# #+latex: \afterpage{
#+LATEX: \begin{figure}
  #+LATEX: \includegraphics[width=0.75\textwidth]{img/data-eg-transcript}
  #+LATEX: \caption{First few lines of a transcript file in \emph{LDC2004T19}.}
  #+LATEX: \label{fig:data-eg-transcript}
#+LATEX: \end{figure}
# #+LATEX: }

It has been explicitly pointed out in the documentation that no manual attempts
were made to modify the automatically derived utterance boundaries, leaving the
possibility that the start- and end-points may not be as precise as would have
been possible if done manually, or by a more precise speech detection system.
The characteristics of the automatic speech detector that was used in the
transcription process have, unfortunately, not been discussed in the
documentation. Such lack of precision does not impact speech-to-text systems,
the primary intended users of this dataset, since the outputs of such systems do
not need to be localized in time. But for systems that do want to localize
different events in the audio, these imprecisions could adversely impact the
final evaluation results. It was indeed observed for the systems proposed in
this thesis that the predicted segment boundaries for overlapping speech were
sometimes more accurate than the boundaries derived from ground-truth. These
inaccuracies may also explain certain prominent peaks in the segment duration
histograms (Figure \ref{fig:actspk-hist-tst}) It would have been impractical to
perform speech detection properly again on the entire dataset to get more
precise boundaries due to time and resource limitations, and thus such a process
was not performed during this thesis.
*** DONE Preparation and Analysis
CLOSED: [2017-09-13 Wed 01:55]
#+latex: \afterpage{
#+LATEX: \begin{table}
|------------+-----------------------+-----------+------------+---------|
|------------+-----------------------+-----------+------------+---------|
| /Set/      | ~CALLIDs~             | /# Calls/ | /Duration/ | /Ratio/ |
|            |                       |           |  /(hours)/ |   /(%)/ |
|------------+-----------------------+-----------+------------+---------|
|------------+-----------------------+-----------+------------+---------|
| Validation | ~{00007,00013,00028,~ |        99 |       1.34 |    0.48 |
|            | ~00062,00065,00069,~  |           |            |         |
|            | ~00086}~              |           |            |         |
|------------+-----------------------+-----------+------------+---------|
| Training   | ~{00100,00101,…~      |      1200 |     188.22 |   66.97 |
|            | ~...…,01298,01299}~    |           |            |         |
|------------+-----------------------+-----------+------------+---------|
| Testing    | ~{05300,05301,…~    |       551 |      91.48 |   32.55 |
|            | ~…,05849,05850}~    |           |            |         |
|------------+-----------------------+-----------+------------+---------|
|------------+-----------------------+-----------+------------+---------|
| Total      |                       |      1850 |     281.04 |  100.00 |
|------------+-----------------------+-----------+------------+---------|
|------------+-----------------------+-----------+------------+---------|
#+LATEX: \caption{Final list of calls used to form different sets, their total durations and ratios.}
#+LATEX: \label{tab:splits}
#+LATEX: \end{table}
#+LATEX: \begin{table}
|--------------------+---+-----------------+---+---------------|
|--------------------+---+-----------------+---+---------------|
|    /# of Speakers/ |   | /# of Segments/ |   | /# of Frames/ |
|           /Active/ |   |           /(%)/ |   |         /(%)/ |
|--------------------+---+-----------------+---+---------------|
|--------------------+---+-----------------+---+---------------|
| */Validation Set/* |   |                 |   |               |
|--------------------+---+-----------------+---+---------------|
|                  0 |   |           15.41 |   |          6.25 |
|                  1 |   |           50.59 |   |         77.77 |
|                  2 |   |           34.00 |   |         15.98 |
|--------------------+---+-----------------+---+---------------|
|--------------------+---+-----------------+---+---------------|
|   */Training Set/* |   |                 |   |               |
|--------------------+---+-----------------+---+---------------|
|                  0 |   |           29.18 |   |         14.26 |
|                  1 |   |           50.44 |   |         76.04 |
|                  2 |   |           20.37 |   |          9.70 |
|--------------------+---+-----------------+---+---------------|
|--------------------+---+-----------------+---+---------------|
|    */Testing Set/* |   |                 |   |               |
|--------------------+---+-----------------+---+---------------|
|                  0 |   |           17.48 |   |          4.81 |
|                  1 |   |           51.58 |   |         81.20 |
|                  2 |   |           30.93 |   |         13.99 |
|--------------------+---+-----------------+---+---------------|
|--------------------+---+-----------------+---+---------------|
#+LATEX: \caption{Overall ratios of the number of segments and acoustic frames with different number of simultaneously active speakers in different sets.}
#+LATEX: \label{tab:actspk-sets}
#+LATEX: \end{table}
\clearpage
#+latex: }
Each NIST SPHERE format audio file was first converted to two-channel WAV format
using the ~sph2pipe_v2.5~ utility provided by LDC \cite{_sphere_2017}. These two
channels contain speech from one speaker's side in the conversation, and were
merged into a final single-channel WAV file using ~FFMPEG~ \cite{_ffmpeg_2017} by giving both
channels equal weights. The sample rate of the audio files during all these
steps were kept to the original value of 8000 Hz.

The class label for speech from 0 speakers, 1 speaker, or more than 1 speakers
for a given time stamp in the audio was derived based on if utterance from none
of the channels, only 1 of the channels, or both the channels were,
respectively, present at the particular time-stamp in the corresponding
transcript file.

The entire dataset of 5850 calls (\sim960.3 hours) was then split to make training,
validation and testing sets. The validation set was only used during the
training phase for monitoring or experimentation, but was not used for training
or evaluation. Final evaluations that have been presented in this thesis were
done on the testing set. These sets were made
as follows: 99 calls (\sim16.71 hours, 1.74%) were
assigned to the validation set, 4,000 calls (\sim652.85 hours, 68.00%) to the
training set, and 1,751 calls (\sim290.74 hours, 30.26%) to the testing
set.

However, due to hardware and time limitations, the final sets that were actually
used during the works in this thesis were only subsets from the initial
assignments above. The final calls that were used in each set are shown in Table
\ref{tab:splits}. The particular choice of validation calls was made to control
for gender ratios and certain other properties, while the first 1,200 calls from
the originally assigned training set, and the last 551 calls from the originally
assigned testing set were chosen for the respective final sets.
#+LATEX: \begin{figure}
#+LATEX: \includegraphics[width=\textwidth]{img/actspk-whist-trn}
#+LATEX: \caption{Weighted histograms of durations of segments (truncated to $<4$ sec. long)
with different number of active speakers in the final *training set*.
#+latex: }
#+LATEX: \label{fig:actspk-hist-trn}
#+LATEX: \end{figure}

#+LATEX: \begin{figure}
#+LATEX: \includegraphics[width=\textwidth]{img/actspk-whist-tst}
#+LATEX: \caption{Weighted histogram of durations of segments (truncated to $<4$ sec. long)
with different number of active speakers in the final *testing set*.
#+latex: }
#+LATEX: \label{fig:actspk-hist-tst}
#+LATEX: \end{figure}
Table \ref{tab:actspk-sets} shows the ratios of the segments and the total
durations with different number of active speakers for the final sets. Figure
\ref{fig:actspk-hist-trn} shows the histograms of the propotional (weighted by
duration) contributions of segments of different lengths to the total set of
segments with respective number of speakers active in the training set. Figure
\ref{fig:actspk-hist-tst} does the same for the final testing set. It can be
seen that, as discussed in Section [[Double-Talk in Conversations]], 0.5 to
1.5 seconds long segments contribute the most the overlapping speech situations.
Particular peaks could depend on the nature of the dataset, but overall
characteristics of the data matches what has been reported for other
conversational datasets used in other overlapping speech detection systems.
# #+LATEX: \begin{table}
#   |-----------------+---+-----------------+---+---------------|
#   |-----------------+---+-----------------+---+---------------|
#   | /# of Speakers/ |   | /# of Segments/ |   | /# of Frames/ |
#   |        /Active/ |   |           /(%)/ |   |         /(%)/ |
#   |-----------------+---+-----------------+---+---------------|
#   |-----------------+---+-----------------+---+---------------|
#   |               0 |   |           15.41 |   |          6.25 |
#   |               1 |   |           50.59 |   |         77.77 |
#   |               2 |   |           34.00 |   |         15.98 |
#   |-----------------+---+-----------------+---+---------------|
#   |-----------------+---+-----------------+---+---------------|
#   |           Total |   |          100.00 |   |        100.00 |
#   |-----------------+---+-----------------+---+---------------|
#   #+LATEX: \caption{Overall ratio of active number of speakers at a time in terms of segments and frames in the final \textbf{validation set}.}
#   #+LATEX: \label{tab:actspk-val}
# #+LATEX: \end{table}
# #+LATEX: \begin{figure}
#   #+LATEX: \includegraphics[width=\textwidth]{img/actspk-whist-val}
#   #+LATEX: \caption{Histogram of durations of segments (truncated to < 4 second long) with different number of active speakers in the final \textbf{validation set}.}
#   #+LATEX: \label{fig:actspk-hist-val}
# #+LATEX: \end{figure}
#
# #+LATEX: \begin{table}
# |-----------------+---+-----------------+---+---------------|
# |-----------------+---+-----------------+---+---------------|
# | /# of Speakers/ |   | /# of Segments/ |   | /# of Frames/ |
# |        /Active/ |   |           /(%)/ |   |         /(%)/ |
# |-----------------+---+-----------------+---+---------------|
# |-----------------+---+-----------------+---+---------------|
# |               0 |   |           29.18 |   |         14.26 |
# |               1 |   |           50.44 |   |         76.04 |
# |               2 |   |           20.37 |   |          9.70 |
# |-----------------+---+-----------------+---+---------------|
# |-----------------+---+-----------------+---+---------------|
# |           Total |   |          100.00 |   |        100.00 |
# |-----------------+---+-----------------+---+---------------|
# #+LATEX: \caption{Overall ratio of active number of speakers at a time in terms of segments and frames in the final \textbf{training set}.}
# #+LATEX: \label{tab:actspk-trn}
# #+LATEX: \end{table}
** DONE [1/1] Acoustic Features
CLOSED: [2017-09-14 Thu 16:45]
Working with raw measurements for pattern recognition is possible, but it is
not usually ideal. Real world signals can be computationally impractical to work with at
best, but also often contain a lot of information that may not be necessary (or
might even be detrimental) for the task at hand. In a feature extraction step,
the raw signals are transformed into vectors in the feature space with the goal
that these vectors should be /similar/ when they are extracted for measurements
of the same phenomenon irrespective of the presence of other irrelevant
phenomena, and, should be /different/ when the underlying phenomena are
different (and, perhaps, competing). For a classifier, a perfect feature
extractor would map all measurements from the same category to the same vector
and those from different categories to significantly different ones, leaving the
classifier to simply assign which vector represents which category.

Again, real world feature extractors are not this perfect. In classical machine
learning approaches, significant amount of effort is therefore applied in
finding such extractors, and previous works on overlap detection are no
exception to this (Section [[Speaker Diarization]]). Engineering such features has
required the use of extensive knowledge of the problem's domain, and sometimes has
led to settling for handling a limited number of scenarios.

But, as motivated in Section [[Deep Learning]], such feature engineering methods
have become less relevant even in some of the most difficult pattern
recognition problems. The raw measurements need to be minimally processed
before an appropriate deep neural network can, effectively, simultaneously learn
the appropriate features and how to classify them.

In speech technologies that use deep learning, there have been some attempts to
work on the raw audio signal itself (e.g. \cite{oord_wavenet:_2016}), but the
computational burden is often too high owing to the high sample rates of such
signals, and the efficacy of this approach in most larger applications is yet
to be studied. Nevertheless, using acoustic features extracted at lower levels
of the pipelines for extracting traditional ones have been shown to lead to
significantly better performance when using deep learning technologies
\cite{hinton_deep_2012,deng_recent_2013,deng_new_2013}. In fact, the previously
de facto acoustic feature called Mel-Frequency Cepstral Coefficients (MFCCs)
have largely been replaced in recent state of the art speech recognition
systems. The one to take their place are $\text{log}_{10} \text{-Mel-Filterbank
Coefficients}$ that are extracted in a step earlier in the MFCC extraction
pipeline.

The process involved in MFCC extraction involves the following steps:
1. Audio signal is normalized to be in the range of [-1, 1].
1. Every $s$ steps in time (usually 10 ms), the audio signal is segmented into
   windows of length $w$, where $w$ is kept small (usually 25 ms to 60 ms) such
   that the audio signal can be assumed to be /quasi-stationary/ within this
   interval to support the validity of performing short-term spectral analysis in
   the next steps.
2. Each such window of samples is multiplied by a windowing function in the time
   domain (usually a Hamming, or Hanning function) to minimize /spectral
   leakage/ in the Fourier transform of this window.
3. A Fourier transform is applied to all such windows to get a vector of size
   $b_{ft}$ at every $s^{th}$ time-step, with $b_ft$ defining the frequency axis
   and $s$ the temporal axis of the resulting **spectrogram**. The square of
   this gives the /power spectrogram/ which is used in the next step.
4. The frequency axis is then warped according to the /Mel-scale/
   \cite{stevens_scale_1937}, which reduces the spectral resolution in the higher
   dimensions of the axis, similar to the frequency response of the human ear.
   This is done by applying $b_{mel}$ (smaller than $b_{ft}$, typically 14
   to 40) overlapping triangular filterbanks that are equidisitant on the
   mel-scale to get the **mel-filterbank coefficients** (aka mel-spectrogram).
5. The dynamic range of individual filterbank channels is reduced by taking the
   logarithm, so as to mimic the human perception of loudness, and also to
   avail certain normalization steps done later. The output at this stage is the
   **log-mel-filterbank** or the log-mel-spectrogram.
6. The discrete cosine transform is applied to the log-mel-spectrogram to
   decorrelate the filterbank channels and get the standard **MFCC** (cepstrum).

As seen, many of the steps above are inspired by the human auditory system,
which is perhaps the best acoustic feature extractor available for one to model
on. The final decorrelation step was necessitated in classic speech technologies
to allow for using diagonal covariance matrices in the involved acoustic modelling (e.g.
GMM-HMMs), this step has been found to be unnecessary when using deep learning
methods. Furthermore, the highly correlated filterbank coefficients, owing to
the use of overlapping filterbanks, can prove to be useful for DCNN based
architectures since such /smoothness/ between neighboring frequency channels can
be exploited better by the convolution and pooling operations involved in such
architectures (Section [[Layers]]). Nevertheless, while using log-mel-spectrograms
had shown improved performance in speech recognition, Deng /et al./
\cite{deng_recent_2013} reported that working with spectrograms directly, which
are extracted even earlier in steps, did not improve the results. They
hypothesized that perhaps the network could not learn the specific transforms
applied to achieve a similar impact that the mel-scale warping can.

The acoustic features that were used for works in this thesis were chosen to be
these $\text{log}_{10} \text{-Mel-Filterbank Coefficients}$. These were
calculated based on spectrograms extracted every $10\,\text{ms}$ (the temporal
resolution of the annotations; Section [[Overview]]) over windows of size
$32\,\text{ms}$. The particular choice of window size was made to, firstly, end
up with a number of samples per-window that is a power of 2 ($32\,\text{ms }
\times 8000\,\text{Hz } = 256\,\text{samples}$) for efficient Fast Fourier
Transform (FFT), and, secondly, to place the $10\,\text{ms}$ samples which decide the
label for the acoustic vector near the center of the windowing function, where
they contribute the most. Hanning function was used for windowing.

The number of filterbanks used was 64, which is different from the popular choice
of 40 in other deep learning based approaches. The decision was made with the
hope that the resulting features may keep more speaker-discriminative
information which may prove useful in overlap detection, and was supported by
some initial empirical evaluations.

To improve these features further, and as is common in many other works
\cite{sercu_very_2015,sercu_advances_2016,xiong_achieving_2016}, each feature
vector, before being input to the proposed DCNN, was attached with neighboring
frames so as to provide contextual acoustic information. This was motivated by
the understanding that contextual information could help in predicting
occurrences of double-talk situations, as was discussed in Section [[Double-Talk
in Conversations]]. Furthermore, it had been reported in previous works that
certain long-term acoustic features like prosody estimates
\cite{zelenak_speaker_2012}, or even statistics of silences and speaker changes
\cite{yella_overlapping_2014} helped improve the results over using acoustic
features alone. Knowing that DCNNs are capable of learning low- and high-level
representations of data for an appropriate task, it was hoped that the proposed
classifier could learn such patterns from the contextual acoustic
information automatically. Neural network architecture, like RNNs, are designed
to learn even longer-term patterns without the need of manually providing the
necessary contextual frames, but such architectures were not investigaged in
this thesis. The number of contextual frames added were ±10 (from before and
after the center frame) resulting in the total acoustic information available to
the classifier being $210\,\text{ms}$. The particular decision was based on
initial empirical evaluations and some practical concerns (Section
[[Architecture]]).
*** DONE Normalization
CLOSED: [2017-09-14 Thu 16:45]
There are two sources of motivation behind normalizing the extracted features before
being input to a classifier: to compensate for mimatch in the training and
testing conditions, and to make the data more suitable for the algorithms used
in the classifier.

A classifier should be robust against noise and other irrelevant phenomena that
are common in natural signals. In speech signals, the recording conditions, the
microphones used, presence of channel noise, or even speaker variability can
prove detrimental to the goals of an acoustic classification task. Various
normalization techniques have been studied over the years to compensate for
these.

Simple **mean-subtraction**, where each frequency component/channel is centered
to zero based on its mean value calculated over the utterance, has proven to
go a long way in mitigating such issues. In particular, the process is known to
suppress the convolutional distortions introduced by the transmission channel.
In fact, one motivation behind applying logarithm in the feature extraction steps
discussed earlier is partly also based on the reasoning that these convolutional
distortions become multiplicative in the spectral domain and then additive in
the log-spectral domain \cite{molau_normalization_2003,li_overview_2014}.

Additional **variance normalization** is often also performed, and is done so to
ensure that the variance of individual frequency components is 1. Various
machine learning algorithms are either designed to work with such unit-variance
data, or are at least known to perform better. This includes the algorithms used
for training deep neural networks \cite{bengio_practical_2012}. However,
additional variance normalization has started to become less common in the deep
architectures similar to the one proposed in this thesis
\cite{xiong_achieving_2016,sercu_advances_2016,sercu_very_2015}.

The impact of these normalizations on overlap detection, however, was uncertain.
Many systems that want to learn speaker discriminative information for their
tasks (e.g. speaker identification) often perform no normalization at all
\cite{lukic_speaker_2016}. It is possible that using such normalizations may in
fact lead to loss of relevant information. Furthermore, the underlying
statistical model behind these normalizations, that each frequency channel is a
uni-modal normal distribution, does not hold for most of these channels in the
persence of speech activity even over a single utterance \cite{molau_normalization_2003}.

Nevertheless, in this thesis, to study the impact of the two normalization
techniques discussed above, the performance of various configurations with and
without them was evaluated in this thesis' works. In implementation, the means
and variances were calculated on relatively long ($\sim2.5\,\text{minutes}$) contiguous
segments in a conversation so as to have reliable values for these metrics, as
opposed to other approaches which calculate these over single utterances (3 to
15 seconds) and can suffer from un-representative values of these metrics
\cite{prasad_improved_2013}.

More sophisticated approaches like Vocal Tract Length Normalization (VTLN),
Histogram Equalization, and others \cite{li_overview_2014} are also common in
literature, especially for ASR, however these were not studied in this thesis.
One factor was that many such methods are used to actively compensate for
speaker variations, something that an overlap detection system might, in
contrast, actually benefit from.
** DONE [1/1] Deep Convolutional Neural Networks
CLOSED: [2017-09-16 Sat 22:12]
The use of a deep learning based classifier was motivated in Section [[Deep
Learning]]. Ground breaking results have been achieved using these methods in
almost all areas of maching learning. For detecting overlapping speech in
conversations, Geiger /et al./ \cite{geiger_detecting_2013} used Long Short-Term
Memory (LSTM) based Recurrent Neural Networks (RNNs) to achieve comparable
performance to the traditional method using GMM-HMMs, and improved the results
further when they were used in a tandem setting (Section [[Speaker Diarization]]).

As discussed earlier, almost all previous works on overlap detection, including
the work by Geiger /et al/ mentioned above, have concentrated on finding the
right set of acoustic features that give the best performance. Deep learning
technologies, especially Deep Convolutional Neural Networks (DCNN), promise the
capability to /automatically learn/ robust representations from low-level
features that are the most appropriate for a given task. And recent results have
demonstrated that these promises are being satisfactorily fulfilled.
Particularly interesting are the studies where neural network architectures
originally designed for one problem domain (e.g. computer vision) have shown to
perform surprisingly well in other domains as well (e.g. speech recognition)
(Section [[Deep Learning]]).

It was for these reasons that a Deep Convolutional Neural Network (DCNN) was
used in this thesis. No works that have used DCNN for overlap detection in
conversations have been published, so it was made necessary that the
investigation be a comprehensive one. The basic approaches that need to be
evaluated in building an overlap detection system are already numerous (Section
[[Supervised Machine Learning for Classification]]), and the long training times
that any sufficiently deep architecture requires would prove taxing to the
limited time allotted for this thesis. It would have been impractical to then
also comprehensively investigate multiple DCNN architectures, or even the impact of different
hyper-parameters or the ordering of individual layers in even a single one. The
DCNN architecture that was used then, therefore, was fixed for evaluations
performed in this thesis, while fine-tuning and experimentation with other
variations would continue in the works beyond this thesis' submission.

This DCNN architecture is presented in the next section and each of its
components are later also briefly discussed. Most of these components are fairly
standard in any DCNN, and even though almost each of them can warrant a
scholarly article, their discussion in this thesis has been kept brief. The
reader is encouraged to follow the resources referred to in the respective sections for
deeper discussions.
*** Architecture
Table \ref{tab:dcnn-arch} shows the final DCNN architecture that was used in
this thesis. It was decided upon after initial experiments, and was limited
by what seemed reasonable for the given task and by available hardware and time.
There are a total number of 572,035 trainable parameters, which were trained
using the /adamax/ optimizer (a popular variant of batched stochastic gradient
descent) on the /categorical cross entropy/ (objective function) between the
output class posteriors and the categorical ground-truth labels.

This architecture is a heavily simplified version of VGG-net
\cite{simonyan_very_2014}, originally designed for large scale image
recognition, but was also recently investigated by Xiong /et al./
\cite{xiong_achieving_2016} and Sercu /et al./
\cite{sercu_very_2015,sercu_advances_2016} for acoustic modelling in ASR with
great results. The original network was simplified here to have fewer layers,
especially fewer convolutional layers before every pooling layer, and hence much
fewer number of learnable parameters than the original $\sim 85\,\text{million}$. This
decision was made since, in addition to practical reasons, the network was to be
trained for only three classes as opposed to in the order of thousands in the case of the
purposes of the original investigations. More powerful architectures in
the present case would have been prone to /overfitting/. Initial experiments for this thesis with
smaller architectures, or even those without any convolutional layers, had not
shown satisfactory results.

The DCNN consists of three /convolutional blocks/ near the input layer, followed by 3
/dense blocks/ before the final /softmax/ output layer for the three
classes: /**(0 speakers, 1 speaker, more than 1 speakers)**/. No padding was
performed on inputs or outputs of any layers, so the depth of the network (i.e. the
number of blocks and layers) is also partly constrained by the shape of the /input/.

The **inputs** are formed for a given frame by attaching 10 frames immediately
before and 10 frames immediately after as contextual information. This leads to
an input of the shape $(21, 64, 1)$ where the dimensions are /(time,
mel-frequency, channel)/, and, since these frames were calculated every 10 ms,
it represents a total contextual information of $210\,\text{ms}$ available to
the DCNN for each frame's classification. The **class label** for each such
input is decided by the frame at the center, and is provided to the network in
categorical form (3 dimensional vector with value 1 for true class and 0
otherwise). Other works with DCNN in speech recognition prepare their inputs in
a similar manner to capture a typical phoneme length, with the amount of context
added between 5 to 9 from both sides being popular
\cite{sercu_very_2015,sercu_advances_2016,xiong_achieving_2016}. The decision to
choose 10 contextual frames was made after determining that lower number of such
frames lead to relatively worse results in initial exploratory experiments,
while larger numbers would have imposed severe penalty on training times as well
as, given the typically small duration of segments with overlapping speech,
would have made the decision to choose label from only the center frame
questionable. The choice was made also to somewhat mitigate for any inaccuracies
in the ground-truth annotations.
#+latex: \afterpage{
#
  #+LATEX: \begin{table}
|-----------------+---+------------+---+---------------------+---+--------------|
|-----------------+---+------------+---+---------------------+---+--------------|
| Block           |   | Layers     |   | Config.             |   | Output's     |
|                 |   |            |   |                     |   | Shape        |
|-----------------+---+------------+---+---------------------+---+--------------|
|-----------------+---+------------+---+---------------------+---+--------------|
| Inputs Block    |   | Inputs     |   | -                   |   | (21, 64, 1)  |
|-----------------+---+------------+---+---------------------+---+--------------|
|-----------------+---+------------+---+---------------------+---+--------------|
| Conv. Block *1* |   | Conv2D     |   | filter: (3, 3), 64  |   | (19, 62, 64) |
|                 |   |            |   | stride: (1, 1)      |   |              |
|                 |   | BatchNorm  |   | -                   |   |              |
|                 |   | Activation |   | func: ReLU          |   |              |
|                 |   | Dropout    |   | prob.: 0.1          |   |              |
|                 |   | MaxPool2D  |   | stride: (2, 2)      |   | (9, 31, 64)  |
|-----------------+---+------------+---+---------------------+---+--------------|
| Conv. Block *2* |   | Conv2D     |   | filter: (3, 3), 128 |   | (7, 29, 128) |
|                 |   |            |   | stride: (1, 1)      |   |              |
|                 |   | BatchNorm  |   | -                   |   |              |
|                 |   | Activation |   | func: ReLU          |   |              |
|                 |   | Dropout    |   | prob.: 0.1          |   |              |
|                 |   | MaxPool2D  |   | stride: (2, 2)      |   | (3, 14, 128) |
|-----------------+---+------------+---+---------------------+---+--------------|
| Conv. Block *2* |   | Conv2D     |   | filter: (3, 3), 256 |   | (1, 12, 256) |
|                 |   |            |   | stride: (1, 1)      |   |              |
|                 |   | BatchNorm  |   | -                   |   |              |
|                 |   | Activation |   | func: ReLU          |   |              |
|                 |   | Dropout    |   | prob.: 0.1          |   |              |
|                 |   | MaxPool2D  |   | stride: global      |   | 256          |
|-----------------+---+------------+---+---------------------+---+--------------|
|-----------------+---+------------+---+---------------------+---+--------------|
| Dense Block *1* |   | Dense      |   | units: 512          |   | 512          |
|                 |   | Activation |   | func: ReLU          |   |              |
|                 |   | Dropout    |   | prob.: 0.1          |   |              |
|-----------------+---+------------+---+---------------------+---+--------------|
| Dense Block *2* |   | Dense      |   | units: 128          |   | 128          |
|                 |   | Activation |   | func: ReLU          |   |              |
|                 |   | Dropout    |   | prob.: 0.1          |   |              |
|-----------------+---+------------+---+---------------------+---+--------------|
| Dense Block *3* |   | Dense      |   | units: 32           |   | 32           |
|                 |   | Activation |   | func: ReLU          |   |              |
|                 |   | Dropout    |   | prob.: 0.1          |   |              |
|-----------------+---+------------+---+---------------------+---+--------------|
|-----------------+---+------------+---+---------------------+---+--------------|
| Outputs Block   |   | Dense      |   | units: 3            |   | 3            |
|                 |   | Activation |   | func: Soft-max      |   |              |
|-----------------+---+------------+---+---------------------+---+--------------|
|-----------------+---+------------+---+---------------------+---+--------------|
    #+LATEX: \caption{Architecture of the Deep Convolutional Neural Network
      used in the trainings and evaluations presented in this thesis
      (missing output's shape entry indicates no change from previous output's shape).
    #+latex: }
    #+LATEX: \label{tab:dcnn-arch}
  #+LATEX: \end{table}
#
  \clearpage
#
#+latex: }

Each **convolutional block** consists of a /2D convolutional layer/,
/batch-normalization layer/, non-linear /activation layer/, /drop-out layer/,
and finally a /2D max-pooling layer/, in that order. The max-pooling in the
final convolutional block is performed globally so that the input to the
following /dense block/ is a vector of the same size as the number of filters
learned in the final convolutional layer. Each of the **dense block** then has a
/dense layer/, followed by an /activation layer/ and then a
/drop-out layer/. The final outputs in the form of class posteriors are
obtained from the appropriately sized /dense layer/ with /softmax/ as the
activation function. In total, then, there are 7 layers (excluding
batch-normalization layers) with learnable parameters. These layers are discussed next.
*** Layers
**** Convolutional Layer
The use of /convolutional/ layers gives a Deep /Convlutional/ Neural network
it's name. Within this layer, a set of filters (of size smaller than the inputs
in respective dimensions) are /convolved/ over the inputs. Each filter of a
given size (called it's receptive field) is moved over the entire input with
some striding ratio, and the output at a position is the weighted sum of the
overlapped region of the input at the corresponding position
\cite{_cs231n_2016}. The multiplicative weight and additive bias parameters for
performing the weighted sum are learned during training after having been
initialized with some appropriate values. In this way, each filter produces a
feature map from a given input, which, in crude terms, represents the presence
of the /local patterns/ that the filter has learned to distinguish from others.
The number of such filters learned in a layer are increased as the position of
the convolutional layers goes deeper (Table \ref{tab:dcnn-arch}), with the
intuition of learning fewer lower level features (e.g. utterance specific) at
the beginning, which are then combined to form more number of higher-level
features (e.g. speaker specific) as the network goes deeper.

All convolutional layers in the proposed architecture used filters with
receptive field of shape $(3, 3)$, i.e. 2-dimensional convolution, and also used
a striding ratio of $1$ in both dimensions. The hope is that these filters will
learn the temporal and spectral patterns that occur during the three situations
where different number of speakers are active, while also being invariant to the
particular characteristics of what is being spoken, by whom, and when.
Furthermore, since none of the layers performed any padding to their inputs
before convolution, the final size of each feature map is reduced by an
absolute value of $2 \times \lfloor{\frac{3}{2}}\rfloor = 2$ in each dimension.
**** Batch-Normalization Layer
In recent works using DCNN's, the output of a convolutional layer is often
normalized to have zero-mean and unit-variance. During batch-wise training, this
/batch-normalization layer/ performs this in an online manner, learning the mean
and variance of the entire dataset at the particular position of the layer for
each component of it's input. During evaluation, the learned mean and variance
are used to normalize the inputs to the next layer. The goal is to standardize
the internal representations inside the network (similar to how the inputs were
normalized in Section [[Normalization]]), and have been shown to help the network
converge faster and generalize better \cite{ioffe_batch_2015}. To enable such a
layer to learn the mean and variance statistics more robustly in a batch-wise
online manner, care should be taken when the batches are constructed. For this,
the batch-size was kept relatively large and it was made /sure/ (upto a random
decision function) that consecutive batches did not come from the same recording
(while being shuffled within themselves anyway).

It is a subject of many informal debates within the deep learning community on
whether such a normalization should be performed on the outputs of the
convolutional layer, or the /activation layer/. Unfortunately, there have not
been any thorough investigations into the impact of the two decisions. It is the
personal view of the author of this thesis that using batch-normalization before
activation may lead to /loss/ of potentially useful information especially when
using Rectified Linear Unit (ReLU) as the activation function. This is because
ReLU is inactive for values $\leq 0$, and batch-normalization will necessarily
center all it's inputs around zero. However, one can argue that such /loss/ of
information could prove useful in forcing the DCNN to search for better
patterns, essentially acting as an additional form of regularization procedure.
Nevertheless, the decision was made to honour the original architecture that
inspired this work's DCNN, and batch-normalization was always performed before
activation. Further works are planned to investigate the other options,
including ones skipping this layer altogether.
**** Activation Layer
The /activation layer/ applies a non-linear function to it's inputs while
sometimes promising certain properties for it's outputs as well. The application
of a non-linear activation function in the neural units (neurons) to the
weighted sum of it's in coming values is what lies at the heart of what makes a
neural network capable of learning universal functions (Section [[Deep Learning]]).
Such an activation function should be non-linear, bounded, and a monotonically
increasing, preferably continuous functions. Traditionally, functions like the
logistic (sigmoid) function $f_{sig}(x) = \frac{1}{1 + e^{-x}}$, or the
hyperbolic tangent function $f_{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
have been used. Recently however, the Rectified Linear Unit (ReLU) function
$f_{relu}(x) = x^{+} = max(0, x)$ has become the most popular choice for most
applications, and neural networks using these have been shown to learn much
faster than with others
\cite{glorot_understanding_2010,glorot_understanding_2010,deng_new_2013,he_delving_2015}.
However, being non-continuous, weights of the neural units should be
initialization with some care. Furthermore, they are also known to lead to
overfitting, but certain regularization layers have been shown to mitigate such
issues \cite{dahl_improving_2013}.

All weights in the proposed network here were initialized using the Glorot
Uniform distribution \cite{glorot_understanding_2010}. /Drop-out layers/
were used during training for regularization (discussed later).
All activation layers in the proposed network used the ReLU function, except the
final layer that produced was tasked to produce the posterior probabilities for the three
target classes. For this layer, the Soft-max activation function was used, which
is a generalization of the logistic function above, but promises the
$C\text{-dimensional}$ output vector for a $C\text{-dimensional}$ real-valued input vector
$\textbf{x}$ to be real valued in the range $[0, 1]$, and sum up to $1$,
simulating a discrete probability mass distribution over the target classes:
$$f_{softmax}(\textbf{x}) = \frac{e^{x_j}}{\sum_{k=1}^{C} e^{x_k}} \text{ for }
j=1, 2, \ldots, C\text{.}$$
**** Drop-Out Layer
Randomly dropping out (setting to zero) the inputs (with some fixed probability)
before being passed on to the next layer was recently introduced as an elegant
architectural "hack" to prevent overfitting in neural networks, and also for
approximating the ensemble training (with model averaging) of exponentially many
/thinned/ networks within the same neural network architecture \cite{srivastava_dropout:_2014}. It acts as
a regularization method by preventing complex co-adaptation of the neural units
to fixed, simple patterns. When applied to the inputs themselves, it can also
simulate training on an exponential number of augmented datasets, although this
configuration is rarely used and reserved for situations when the original
training dataset is small and meets certain criteria.

In the proposed architecture, all ReLU activation layers were followed by a
drop-out layer with $10\%$ of the inputs to such layers being randomly set to
zero. It is arguable that a higher probability should be used in the /dense
layers/ (the classifier) to make them more robust, but this has been left for
future investigations.
**** Max-Pooling Layer
Another hallmark of DCNNs is the use of pooling layers that perform a type
of non-linear down-sampling on their inputs. They are implemented in a very
similar fashion to the convolutional layers, except for two key differences: the
filters are applied to non-overlapping receptive fields instead of overlapping
ones in case of convolutional layers (i.e. the striding length is equal to filter size
instead of $1$), and the output of the filters are not weighted sums of the
inputs within the receptive field, but rather the result of a non-linear
function, popularly the max or the average. Furthemore, the number of such
filters is kept equal to the number of filters (output feature maps) in the
earlier convolutional layer. The intuition behind using such a layer is that the
exact location of a recognized pattern in the inputs is less important than its
rough location relative to other recognized patterns, leading to a form of
translation invariance. Furthermore, with such a high striding ratio (or
striding length), the size of the learned representation is significantly reduced, leading
to fewer computations in subsequent layers while also helping against
overfitting in the previous layer \cite{_cs231n_2016}.

When applied /globally/, the chosen non-linear function is applied to a receptive
field of size equal to the shape of the input feature maps, effectively
replacing each feature map by a single value.

In the proposed architecture, all pooling layers applied the max function.
Except for the final global pooling layer, all other pooling layers use a 2D
filter of shape $(2, 2)$, applied with a striding ratio $(2, 2)$. The final
global max-pooling layer used a filter size equal to the shape of the input
feature maps, making the striding ratio irrelevant. The other popular option of
using the average function, or a different combination/location in the network,
is alas left for future works.
**** Dense Layer
Similar to a typical DCNN based classifier's architecture, the convolutional
blocks near the input's end are designed to learn and extract the features that
will prove to be the most relevant to the task at hand. The actual
classification task is then learned and performed by multiple layers of fully
connected dense layers towards the output's end. In the network then, these
dense layers (learn to) perform high-level /reasoning/ with the features that
have been learned by the feature extractor.

From the perspective of topology, each unit (neuron) in a dense layer performs a
weighted sum of all the inputs, which are then passed through a non-linear
activation function. The multiplicative weights and the additive biases are not
shared between the individual neurons. Hence, the output size of such a layer
will be of the size of the number of neural units in that layer.

In the proposed DCNN, max-pooling is performed globally in the final
convolutional block so as to end up with a vector of the size of the number of
feature maps produced in the final convolution. These then behave as the
extracted feature vector for /dense blocks/ to be used for classification. Each
such dense block consists of a dense layer such as one explained above, with the
number of neural units decreasing as the position of the block (and hence the
layer) goes deeper. After each dense block, however, a dropout layer was used
for regularization during training, as has been explained previously.

The activation function used in all such dense blocks was the ReLU function,
except in the case of the final output layer, which used the Soft-max function
as previously discussed.
*** Training
**** Loss Function
The basic idea behind supervised training of any classifier is to optimize its
parameters based on a measure of mismatch between its decisions for a given
input and the known correct decision. This measure of mismatch, called the
objective function or conversely the loss function, should encapsulate what
errors the classifier is making and direct an /optimizer/ to update the
parameters accordingly. Since the outputs of the proposed DCNN and its
targets are discrete probability distributions, the loss function that was
chosen for this task was the Categorical Cross Entropy between the known
probability distribution $\textbf{y}$ and the predicted probability distribution
$\hat{\textbf{y}}$ as, $$L(\textbf{y}, \hat{\textbf{y}}) = - \frac{1}{C}
\sum_{j=1}^{C} y_i \, log(\hat{y_i})\text{.}$$

Although this loss function does encapsulate the /divergence/ between the two
distributions, it weighs it equally for all $C$ classes. This would result in
the dominant class contributing more to the /gradient/ using which the parameters
of the classifier will be optimized. Knowing that the minority class is more
important to discover, this loss function can, following one easy way, be
weighted to have a higher value when the misclassified instance is from the
minority class. This weight could either be fixed, or based on probability of
seeing such a class in a batch, etc.

Unfortunately, as discussed in [[Tackling Class Imbalance]], doing this did not give
reliable or even stable results while working on this thesis, and thus their
results have been ommitted. A yet another loss function along similar lines of
weighting the loss values in favor of the minority class can be the Bayesian
Categorical Cross Entropy function \cite{dalyac_tackling_2014}~[]~, while other
such functions have also been proposed in literature \cite{wang_training_2016}.
These should also be considered in future works on this problem. For the purpose
of rectifying the above mentioned issue, then, only the impact rebalancing the
training data will be reported.
**** DONE [0/0] Optimization
CLOSED: [2017-09-16 Sat 22:02]
The value of the loss function, averaged over the training examples, for
different parameter values, can be seen as a hilly landscape in the
(high-dimensional) parameter space. The job of an optimizer then is to navigate
through this landscape to reach a region where the values of the parameters
result in minimum average loss. In practice, the essential algorithms for doing
this for deep neural networks are /stochastic gradient descent (SGD)/ and the
/backpropagation/ of such gradients.

An SGD based optimizer follows the average gradient of errors produced by small
samples (called mini-batches) taken from the training data and backpropagates
this gradient from the output units of the network to the input units. The
process is repeated over multiple mini-batches taken from the training set, and
often over multiple passes over the entire training set as well. The
optimization procedure is said to have converged when further iterations do not
reduce the loss function any more. The optimizer used to train the proposed DCNN
was /adamax/ (with parameters as set in the proposing paper
\cite{kingma_adam:_2014}). It is a variant of the popular /adam/ optimizer
(proposed in the same paper), but is based on the infinity norm. The
decision was based on early experiments showing fast training times while
demonstrating better results than /vanilla/ SGD.

However, given the power of deep neural networks, it is possible that the
network could have converged to a /bad/ minima, where it only performs better on
the (seen) training data but not very well on (unseen) test data. Such an
occurence is called overfitting, and should be avoided since the ultimate goal
of a classifier is to perform it's task on as yet unseen data. For this reason,
various regularization procedures (discussed earlier) are used, and
the training of a neural network is often monitored by calculating it's
performance on a small (yet representative) validation set that the neural
network never sees for training. When the performance on this set starts to
degrade, certain measures can be taken to counteract overfitting.

For neural networks, popular measures also include modifying the learning rate
of the optimizer and early stopping of the training when the performance on the
validation set starts to degrade. These measures were not used in the works for
this thesis. It was observed that, perhaps due to class imbalance, the loss on
the validation set was considerably noisier than in situations where such
methods have been proven to improve results. Nevertheless, the performance of
the neural network was monitored using a validation set over a specified number
of passes over the dataset (also known as epochs) and the value for the weights
were saved at regular intervals. In cases where overfitting was observed,
weights from an appropriate epoch were used for subsequent evaluations on the
testing set.

All trainings were performed for 20 passes over the entire dataset, except in
the case where the single-speaker class was under-sampled when 40 passes over
the dataset was made (Section [[Tackling Class Imbalance]]).
** DONE [0/0] Implementation -- Highlights
CLOSED: [2017-09-17 Sun 00:22]
All implementations in this thesis were done using the Python programming
language, and it was hence also the medium of choice for working with the
APIs of the libraries that were used. Where possible, reputed open source
libraries were used, and where necessary, the custom implementations were done
with a test-driven-development (TDD) methodology \cite{beck_test_2002}. The
versions of some of the key libraries used are listed below:

- ~Python 3.5.3~ \cite{_python_2017}
- ~NumPy 1.12.1~ \cite{walt_numpy_2011}
- ~FFMPEG 3.2.x~ \cite{_ffmpeg_2017}
- ~LibROSA 0.5.0~ \cite{mcfee_librosa_2017}
- ~Dask 0.14.3~ \cite{dask_development_team_dask:_2017,rocklin_dask:_2015}
- ~h5py 2.7.0~ \cite{_hdf5_2017}
- ~Keras 2.0.2~ \cite{chollet_keras:_2017}
- ~TensorFlow 1.0.0~ \cite{abadi_tensorflow:_2016}
- ~matplotlib 2.0.2~ \cite{michael_droettboom_matplotlib/matplotlib_2017}
- ~Jupyter 5.0.0~ \cite{_project_2017}
- ~pytest 3.0.7~ \cite{_pytest:_2017}

The process that was followed was designed to be modular, where each step is
configurable and reproducible, in order to be helpful for works that are to
continue beyond this thesis' submission. It is briefly explained next with the
context of the relevant implementation highlights.

- Preparation :: The original NIST SPHERE audio files were converted to WAV
                 using the ~sph2pipe_v2~ tool \cite{_sphere_2017}, whose
                 individual speaker channels were merged using ~FFMPEG~. The
                 labels were parsed from the corresponding transcripts using a
                 custom implementation that heavily exploited ~NumPy~. This was
                 also the stage where the dataset was split into the training,
                 validation and testing set.
- Feature Extraction :: The audios were loaded and the feature extraction
     procedure was performed using slightly modified versions of the relevant
     functions available in ~LibROSA~, and the process was parallelized using
     ~Dask~. The final features and the corresponding class labels were then
     stored in ~HDF5~ format, one for each split.
- Neural Network :: The DCNN was implemented using ~Keras~ while the actual
                    computation backend was provided by (GPU enabled)
                    ~Tensorflow~. Any parameters that have not been mentioned in
                    earlier descriptions were kept to the default values set in
                    the respective versions of ~Keras~ and ~Tensorflow~. All
                    experiments involved training for 20 or 40 passes over the entire
                    dataset, and each took between 5 to 7 days on an Nvidia
                    GeForce GTX TITAN X Black GPU (~cuda 8.0~) when a custom
                    /data provider/ was used.
- Batch Preparation :: A custom /data provider/ was implemented using ~NumPy~
     and ~h5py~ for preparing batches for the neural network on the fly and in
     parallel to the training loop running on a GPU. It performed
     context-adding, normalization, under-sampling of classes, seeded shuffling,
     etc. on the data read from the saved ~HDF5~ files. Since it was done in
     parallel to the training loop, the training times were improved by an order
     of magnitude when compared to a more straightforward approach. Furthermore,
     a ~NumPy~ specific striding trick \cite{pauli_virtanen_2.2._2016} was used
     to keep the operations memory efficient as well. The efficiency came at the
     cost that all samples in a batch, even though shuffled, came from a single
     conversation. It would have been impractically inefficient otherwise, and,
     at least, consecutive batches were made sure (upto a random shuffle) to
     come from different conversations.
- Monitoring :: The validation set was used to monitor the performance of the
                DCNN over each epoch by using callbacks provided in ~Keras~ to
                build a ~Tensorboard~ \cite{abadi_tensorflow:_2016}, and the
                learned weights were saved after each epoch using another
                built-in callback. A custom callback was implemented using
                ~NumPy~ and ~Keras~ to also monitor the class confusion matrix
                to indicate the classwise precision and recall metrics on the
                validation set.
- Evaluations :: For evaluations, a slightly modified version of the above data
                 provider (one with no shuffling, subsampling, or skipping) was
                 used to generate raw predictions from the neural network that was
                 set with learned weights obtained after training. These
                 predictions were then analyzed by functions implemented using
                 ~NumPy~, and almost all of the graphical analysis was done
                 using ~Matplotlib~. A custom and numerically stable ~NumPy~
                 based implementation of Viterbi algorithm for temporal
                 smoothing was used.
* DONE [3/3] Evaluations
CLOSED: [2017-09-17 Sun 03:51]
#+latex: \newcommand*\rfrac[2]{{}^{#1}\!/_{#2}}
#+latex: \newcommand{\ov}{$(ov)\ $}
#+latex: \newcommand{\ssp}{$(sp)\ $}
#+latex: \newcommand{\no}{$(no)\ $}
#
#+latex: \newcommand{\pnos}{$P^{(no)}\ $}
#+latex: \newcommand{\psp}{$P^{(sp)}\ $}
#+latex: \newcommand{\pov}{$P^{(ov)}\ $}
#
#+latex: \newcommand{\rno}{$R^{(no)}\ $}
#+latex: \newcommand{\rsp}{$R^{(sp)}\ $}
#+latex: \newcommand{\rov}{$R^{(ov)}\ $}
#
Since there have not been works yet which have used Deep Convolutional Neural
Networks (DCNNs) for the task of detecting overlapping speech in conversations,
many of the fundamental decisions involved in building such a system had to be
settled on before settling on a configuration for different experiments. For
this thesis, these decisions were made by performing a lot of exploratory
experiments in the initial phases of the work. For saving time, a simple neural
network with only dense layers and no convolutional layers was used, although
later a DCNN with fewer learnable parameters than the one proposed was also
used. These were evaluated on the verification set instead of the testing set
(Section [[Preparation and Analysis]]).

The exact results from these exploratory experiments are not reported
in this thesis, but the decisions they inspired are summarized below:

- Using log-mel-filterbanks gave better results over spectrograms or MFCCs.
- Adding more contextual frames (±1, ±5, ±10) while preparing inputs to the
  classifier helped improve the results for overlap detection. Adding too
  many contextual frames did however lead to increased training effort,
  sometimes even resource-exhaustion errors on the single GPU that was used.
- Non-speech vs. speech (from any number of speakers) classification was achieved
  well enough even in the absence of any contextual frames, especially on
  deep neural networks without any convolutional layers.
- Larger networks with more parameters performed better, but also cost more in
  terms of training and evalution times.
- Deeper networks (with more number of layers) performed better than wider
  networks (with more number of neural-units in each layer). Larger filter sizes
  (receptive fields) in the convolutional layers cost more in terms of time, but
  did not improve results over the size proposed, at least in the early
  epochs.
- Pooling with max function gave better results than with the averaging
  function.
- The /adam/ family of optimizers lead to faster training times, and often
  better results than other options.

Furthermore, three degenerate cases, when a classifier failed at satisfactorily
detecting overlapping speech, were commonly observed.

1. All frames in the evaluation set were classified as single-speaker speech
   $(sp)$. This would be seen as 100% recall for \ssp but the recall and
   precision for other classes will be 0%. This was particularly the case when
   the DNN architectures had the fewest number of learnable parameters, and
   sometimes the classifier always produced the posterior probability for \ssp to
   be 1 while it was always 0 for other classes. In such a situation, even
   temporal smoothing cannot help improve the results in any way.
2. All non-speech frames \no will be detected with high precision and recall,
   while the rest will be detected as single-speaker speech. No frames will be
   detected for having overlapping speech on the evaluation set indicated by 0%
   recall for the class, rendering the classifier useless for the current task.
   This was the case for many moderately sized DNN configurations, especially
   when fewer contextual frames were added and no attempts were made to rectify
   the class imbalance issue.
3. All non-speech frames \no will be detected with good precision and
   recall, but the classifier will appear to make random decisions between
   single-speaker speech and overlapping speech for other frames. This case
   would be indicated by having below 70% recall and precision for $(sp)$. Given
   \ssp is by far the most dominant class, such low recall scores would be a
   result of a substantially high false alarm rates for overlapping speech,
   rendering the system unsuitable for overlap detection (Section [[Evaluation
   Metrics]]). This case was seen in moderately large DNN configurations,
   especially when the training data was not rebalanced to rectify class
   imbalance. In a closely related case, the classifier will detect more number
   of frames to have overlapping speech than single-speaker speech, and was
   particularly observed when training was done with a cost-sensitive objective
   function. Although in this case, it would often also be observed that the
   classifier had not converged to a stable loss value even on the training set,
   even when larger DNN or DCNN configurations were used.

In the rest of this chapter, impact of various variables are evaluated with
respect to overlapping speech detection. In all experiments, the same DCNN from
Section [[Architecture]] was trained, and then evaluated on the testing set from
Section [[Preparation and Analysis]]. Furthermore, in addition to /raw/
classification results from the classifier (obtained by applying $argmax$ to the
frame-wise class posteriors), results after applying /temporal smoothing/ as
discussed in Section [[Temporal Smoothing]] are also reported for each experiment.
As mentioned above and in Section [[Cost Sensitive Objective]], many experiments
with different cost sensitive objective functions did not give reliable results,
and have thus been excluded. Lastly, as has been pointed out earlier a few
times, the reported results are not directly comparable to ones from other works
in Section [[Speaker Diarization]] since the dataset used for evaluation is different.
** DONE Training with Silence
CLOSED: [2017-09-17 Sun 02:15]
The flagship task for the proposed DCNN would be to differentiate between the
three possible classes with a single set of learned parameters. Furthermore,
it will achieve the task perfectly if it can do so without the need of
normalizing the inputs and in the presence of imbalance between the classes
while exhibiting high precision and recall scores for all classes. However, this
scenario is expected to be the most challenging one for a real-world classifier,
since no steps have been taken to assist it. The performance under these
conditions could therefore serve as the baseline for comparing the success of
any assistance provided to the classifier in later experiments.

In this section, evaluation results are presented for the DCNN after it was
trained to detect all three classes: \no (non-speech, no speakers active), \ssp
(speech, single speaker active), and \ov (overlapping speech, more than one
speakers active). These are grouped into two scenarios. In the first scenario,
results when the inputs to the DCNN were not normalized are presented. In the
second scenario, impact of applying Mean Subtraction and
Mean-Variance-Normalization is evaluated. In both the scenarios, however, no
methods were applied for rectifying the imbalance between the three classes.
**** Without Normalization
#+latex: \afterpage{
#
  #+LATEX: \begin{table}
|----------+---+------------+------------+---+------------+------------+---+------------+------------|
|----------+---+------------+------------+---+------------+------------+---+------------+------------|
| Pred.    |   | $P^{(no)}$ | $R^{(no)}$ |   | $P^{(sp)}$ | $R^{(sp)}$ |   | $P^{(ov)}$ | $R^{(ov)}$ |
|          |   |        (%) |        (%) |   |        (%) |        (%) |   |        (%) |        (%) |
|----------+---+------------+------------+---+------------+------------+---+------------+------------|
|----------+---+------------+------------+---+------------+------------+---+------------+------------|
| Raw      |   |      28.49 |      58.95 |   |      84.18 |      92.60 |   |      87.23 |       4.76 |
| Smoothed |   |      35.85 |      54.79 |   |      83.44 |      94.94 |   |      95.15 |       1.59 |
|----------+---+------------+------------+---+------------+------------+---+------------+------------|
|----------+---+------------+------------+---+------------+------------+---+------------+------------|
    #+LATEX: \caption{Class-wise precision ($P$) and recall ($R$) on testing set
      after training on
      *non-normalized inputs, with silence*.
    #+latex: }
    #+LATEX: \label{tab:eval-un-nosub}
  #+LATEX: \end{table}
#
  #+LATEX: \begin{figure}
    \centering
      #+LATEX: \includegraphics[width=\textwidth]{img/eval/un-nosub-preds}
      #+LATEX: \caption{True labels, raw predictions, and smoothed predictions for part of a conversation,
        after training on
        *non-normalized inputs, with silence*.
        /Time axis is in *minutes:seconds*./
      #+latex: }
      #+LATEX: \label{fig:eval-un-nosub-preds}
  #+LATEX: \end{figure}
#
  #+LATEX: \begin{figure}
    \centering
      #+LATEX: \includegraphics[width=\textwidth]{img/eval/un-nosub-precrec}
      #+LATEX: \caption{Precision vs. recall for all calls in the testing set,
        after training on
        *non-normalized inputs, with silence*.
      #+latex: }
      #+LATEX: \label{fig:eval-un-nosub-precrec}
  #+LATEX: \end{figure}
#
  \clearpage
#
#+latex: }
The classwise precision and recall scores calculated at the frame level for the
DCNN trained with this configuration are presented in Table
\ref{tab:eval-un-nosub}. It was quickly seen that the classifier did not detect
most of the \ov frames (frames with overlapping speech). However, the degenerate
case is avoided since, even though the recall ($R^{(ov)}$) is very low, the
classifier is very precise whenever it does detect \ov ($P^{(ov)}$).
Furthermore, it was observed that almost all of the misclassified \ov frames
were detected as single-speaker frames $(sp)$.

A more important observation was that, even though very few \ov were detected,
these were nevertheless detected across different segments (raw predictions in
Figure \ref{fig:eval-un-nosub-preds}). There was no discernible relationship
between the length of the overlapping speech segment within which $(ov)$ frames
were detected, but, for the task of simply estimating the count of double-talk
situations in a conversation, this could probably be useful as it is.
Nevertheless, since only a few $(ov)$ frames are detected, and they rarely are
detected over contiguous time indices, application of /temporal smoothing/ using
Viterbi algorithm discarded a large portion of these, as seen in Table
\ref{tab:eval-un-nosub} and the smoothed predictions in Figure
\ref{fig:eval-un-nosub-preds}.

There could be various contributing factors to what made most of the patterns of
\ov more difficult to /learn/. First of all, the inputs were not normalized, so
the data might have statistics that make it unsuitable to learn from.
Furthermore, the network is also not benefitting from any removal of noise,
which could have been a by-product of normalization. Secondly, since no attempts
were made to rectify the class imbalance, the gradients during training are
perhaps getting dominated by the majority class. A more subtle hypothesis is
that, even within the minority classes, the gradients produced by the non-speech
examples are /stronger/ than those produced by overlapping-speech examples.
/Stronger/ in this context means that a relatively little amount of
parameter-tuning is required to be done by the optimizer to better detect
non-speech examples correctly rather than overlapping-speech examples.

It can be seen in Table \ref{tab:eval-un-nosub} that the classifier did recall
($R^{(no)}$) nearly 60% of the non-speech frames \no in the test
set, which indicates that the classifier has learned patterns for detecting
$(no)$. But, due to lack of normalization (and possibly accompanied de-noising),
the classifier was pushed towards perhaps even trying to learn the different
background and channel characteristics in the training set as well, leading to a
precision ($P^{(no)}$) of only near 30% on the testing set (where such
characteristics could be very different). This is further illustrated in Figure
\ref{fig:eval-un-nosub-precrec}, which shows the precision vs. recall scores
for the 551 calls used in the testing set to be all over the board for $(no)$, as
compared to $\text{high-}P^{(ov)}\text{-low-}R^{(ov)}$ and
$\text{high-}P^{(sp)}\text{-high-}R^{(sp)}$ being the norm across these same
calls. The behavior more or less stays the same for \no with respect to
recall even after temporal smoothing is applied, although the precision values
for both \no and \ov improves by the removal of impossibly short segments
(Figure \ref{fig:eval-un-nosub-preds}).

These observations, at least with respect to $(no)$, indicate that the
classifier is probably suffering from mismatches in the training and testing
conditions, which is further distracting it from the task of detecting $(ov)$. The
two normalization procedures discussed in Section [[Normalization]] were explored in
order to mitigate these issues, and the results are discussed next.
**** With Normalization
#+latex: \afterpage{
#
#+LATEX: \begin{table}
|-------------------------+---+------------+------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------+---+------------+------------|
|                         |   | $P^{(no)}$ | $R^{(no)}$ |   | $P^{(sp)}$ | $R^{(sp)}$ |   | $P^{(ov)}$ | $R^{(ov)}$ |
|                         |   |        (%) |        (%) |   |        (%) |        (%) |   |        (%) |        (%) |
|-------------------------+---+------------+------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------+---+------------+------------|
| */Mean Subtraction/*    |   |            |            |   |            |            |   |            |            |
|-------------------------+---+------------+------------+---+------------+------------+---+------------+------------|
| Raw                     |   |      19.43 |      88.09 |   |      86.45 |      82.15 |   |      81.30 |       7.89 |
| Smoothed                |   |      20.90 |      89.10 |   |      85.92 |      83.61 |   |      91.76 |       5.05 |
|-------------------------+---+------------+------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------+---+------------+------------|
| */Mean-Variance-Norm./* |   |            |            |   |            |            |   |            |            |
|-------------------------+---+------------+------------+---+------------+------------+---+------------+------------|
| Raw                     |   |      18.39 |      89.31 |   |      86.49 |      80.68 |   |      81.15 |       7.30 |
| Smoothed                |   |      19.65 |      89.99 |   |      85.97 |      82.17 |   |      90.46 |       4.53 |
|-------------------------+---+------------+------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------+---+------------+------------|
  #+LATEX: \caption{Class-wise precision ($P$) and recall ($R$) on testing set
    after training on
    *inputs with different normalizations, with silence*.
  #+latex: }
  #+LATEX: \label{tab:eval-mvn-nosub}
#+LATEX: \end{table}
#
  #+LATEX: \begin{figure}
    \centering
      #+LATEX: \includegraphics[width=\textwidth]{img/eval/mn-nosub-precrec}
      #+LATEX: \caption{Precision vs. recall for all calls in the testing set,
        after training on
        *inputs with Mean-Subtraction, with silence*.
        Results for Mean-Variance-Normalization were similar.
      #+latex: }
      #+LATEX: \label{fig:eval-mn-nosub-precrec}
  #+LATEX: \end{figure}
#
  \clearpage
#
#+LATEX: \begin{figure}
  \centering
    #+LATEX: \includegraphics[width=\textwidth]{img/eval/mn-nosub-preds}
    #+LATEX: \caption{True labels, raw predictions, and smoothed predictions for part of a conversation,
      after training on
      *inputs with Mean-Subtraction, with silence*.
      Results for Mean-Variance-Normalization were similar.
    #+latex: }
    #+LATEX: \label{fig:eval-mn-nosub-preds}
#+LATEX: \end{figure}
#
#+latex: }
The two explored normalization procedures (Section [[Normalization]]), Mean-Subtraction and
Mean-Variance-Normalization, were both applied at a level of relatively long
segments of a conversation. The precision and recall
performance for the three classes with each of these settings are presented in
Table \ref{tab:eval-mvn-nosub}.

The mismatch issue seems to have been somewhat rectified (Figure
\ref{fig:eval-mn-nosub-precrec}) but it can be seen in Table
\ref{tab:eval-mvn-nosub} that the expectation of high recall and high precision
for \no are not exactly met with either of the normalization procedures applied.
While \rno has increased dramatically, it has come at significant cost to the
already low \pnos observed earlier. This was seen as very high levels of false
alarms for \no (Figure \ref{fig:eval-mn-nosub-preds}), especially taxing recall
for \ssp and moving in the direction of a degenerate case. It was also
observed that misclassification rate of \ov as \no was two to three times higher
with either of the normalization procedures, even though more number of \ov were
detected by the classifier when compared to non-normalized case earlier.

It was particularly interesting to see that simple mean subtraction seemed to
have been responsible for most of the observed changes, while additional
variance normalization had slightly-beneficial to slightly-detrimental impact. As previously
discussed in Section [[Normalization]], additional variance normalization is less
commonly observed in recent applications of deep learning technologies, and the results here
perhaps indicate why.

Nevertheless, looking at the precision vs. recall scores for the test set in
Figure \ref{fig:eval-mn-nosub-precrec}, one can argue that at least the problems
due to mismatch in training and testing conditions are being rectified to some
extent. These values for all the classes are clustered together for all calls
(though around less than ideal centers), which could mean that these
normalizations are able to compensate for any differences in the raw un-normalized
inputs in the training and test set.

The predominantly lower \pnos however suggests that these methods may not be
appropriately compensating for additional noise that well in either of two ways:
the noise specific to the each call's recording conditions is not being removed
well enough, or, that it is being over-compensated for leading to loss of useful
information in \ssp as well. The latter could to be the case in the present
scenario, since it was observed that most of the detected \no formed fairly long
and contiguous segments in time (Figure \ref{fig:eval-mn-nosub-preds}). These
could be due to loss of information in some word-units that were spoken.
However, this could also be observed if the noise was under-compensated for,
impacting such word-units more severely than others. More sophisticated
de-noising procedures might help in this situation, or, at least, future
attempts should also experiment with using different length segments for
calculating the statistics used for these normalizations. Nevertheless, an
additional challenge posed by the false \no being also contiguous and long is that
temporal smoothing with Viterbi algorithm is not able to remove them.

Another observation made from Figure \ref{fig:eval-mn-nosub-preds} is that \ov
were detected in relatively similar manner as earlier, i.e. fewer frames over
multiple segments, however the detected frames formed relatively longer and more
contiguous segments. This, accompanied by more number \ov being detected, lead
to much less relative reduction in \rov after temporal smoothing ($\sim 36\%$
vs. $\sim 67\%) relative) while \pov still increased. This keeps the prospects
of applying normalization promising for overlap detection, at least in terms of
making the inputs more suitable for the classifier to learn from.

With respect to gradients, however, \no still seems to mislead the classifier.
Furthermore, with respect to non-speech vs. speech detection, the classifier
still does not perform as well as much simpler methods. It was argued that
perhaps some methods for rectifying the class imbalance may help improve the
results, at least with respect to detecting $(ov)$. Some initial experiments did
show this to be the case for $(ov)$, but performance for \no degraded even further.
Lastly, it was deemed worthwhile to study the impact of various attempts at
tackling class imbalance in isolation, and results from these would motivate
future works where all three classes might be trained for simultaneously.
Therefore, in the experiments reported in the rest of the chapter, training was
carried out without silence frames. They were added during the application of
temporal smoothing using ground-truth labelling, but, since the \pnos and \rno in
these cases will always be $\sim 100\%$, they are not reported in the tables.
** DONE Training without Silence
CLOSED: [2017-09-17 Sun 03:51]
  #+LATEX: \begin{table}
|-------------------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------|
|                         |   | $P^{(sp)}$ | $R^{(sp)}$ |   | $P^{(ov)}$ | $R^{(ov)}$ |
|                         |   |        (%) |        (%) |   |        (%) |        (%) |
|-------------------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------|
| */No Normalization/*    |   |            |            |   |            |            |
|-------------------------+---+------------+------------+---+------------+------------|
| Raw                     |   |      86.28 |      99.83 |   |      87.72 |       7.21 |
| Smoothed                |   |      85.84 |      99.96 |   |      94.75 |       3.62 |
|-------------------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------|
| */Mean Subtraction/*    |   |            |            |   |            |            |
|-------------------------+---+------------+------------+---+------------+------------|
| Raw                     |   |      86.83 |      99.05 |   |      68.87 |      12.22 |
| Smoothed                |   |      86.48 |      99.79 |   |    *88.13* |     *8.82* |
|-------------------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------|
| */Mean+Variance Norm./* |   |            |            |   |            |            |
|-------------------------+---+------------+------------+---+------------+------------|
| Raw                     |   |      87.03 |      98.59 |   |      63.25 |      14.14 |
| Smoothed                |   |      86.74 |      99.61 |   |      82.97 |      11.01 |
|-------------------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------|
    #+LATEX: \caption{Class-wise precision ($P$) and recall ($R$) on testing set
      after training on
      *inputs with or without different normalizations, and without silence*.
    #+latex: }
  #+LATEX: \label{tab:eval-mvn-skipzero}
  #+LATEX: \end{table}
#
#+latex: \afterpage{
#
  \centering
  #+LATEX: \begin{figure}
    #+LATEX: \includegraphics[width=\textwidth]{img/eval/mn-skipzero-preds}
    #+LATEX: \caption{True labels, raw predictions, and smoothed predictions for part of a conversation,
      after training on
      *inputs with Mean-Subtraction, but without silence*.
      Results for Mean-Variance-Normalization were similar.
    #+latex: }
    #+LATEX: \label{fig:eval-mn-skipzero-preds}
  #+LATEX: \end{figure}
#+latex: }
Table \ref{tab:eval-mvn-skipzero} shows the precision and recall results for the
next three experiments without \no where different normalizations were performed on the
inputs. In all three cases, no procedures were applied for tackling class
imbalance still present between \ssp and $(ov)$.

It was immediately apparent that the impact of bad \pnos in earlier experiments,
where a majority of the false \no were actually \ssp, has been rectified, leading
to \rsp being nearly 100%. Furthermore, this was accompanied by an
improvement in \psp as well, irrespective of the normalization procedure
applied, which indicates that the degenerate case where all frames were
predicted as \ssp was been avoided.

\rov was seen to be almost twice as much compared to when training was carried
out with silence, irrespective of the normalization procedure applied. However,
this increase was expected to come at a cost on \pov of the /raw/ predictions.
The steep trade-off between precision and recall for \ov has been characteristic
of overlap detection systems in existing works. It was surprising then to see
the \pov to also slightly increase alongside \rov for raw predictions on non-normalized
inputs when compared to similar setup in the previous section. When any
normalization was applied, the increase in raw \rov was paired with a decrease
in raw \pov as expected.

It was observed that, even though more \ov were detected across more segments
with overlaps, as was characteristic of training with un-normalized inputs
earlier, these frames usually did not form longer contiguous segments in time.
This is further illustrated in Table \ref{tab:eval-mvn-skipzero} by the
similarly sharp decrease in \rov when temporal smoothing is applied.

#+latex: \afterpage{
#
  \centering
  #+LATEX: \begin{figure}
    #+LATEX: \includegraphics[width=\textwidth]{img/eval/un-skipzero-precrec}
    #+LATEX: \caption{Precision vs. recall for all calls in the testing set,
      after training on
      *non-normalized inputs, and without silence*.
    #+latex: }
    #+LATEX: \label{fig:eval-un-skipzero-precrec}
  #+LATEX: \end{figure}
#
  #+LATEX: \begin{figure}
    #+LATEX: \includegraphics[width=\textwidth]{img/eval/mn-skipzero-precrec}
    #+LATEX: \caption{Precision vs. recall for all calls in the testing set,
      after training on
      *inputs with Mean-Subtraction, but without silence*.
      Results for Mean-Variance-Normalization were similar.
    #+latex: }
    #+LATEX: \label{fig:eval-mn-skipzero-precrec}
  #+LATEX: \end{figure}
#
  \clearpage
#
#+latex: }
Application of temporal smoothing on the raw outputs of the classifier while working
with normalized inputs showed much more improvement in \pov for relatively
smaller sacrifice in recall. This indicated, and it was observed (Figure
\ref{fig:eval-mn-skipzero-preds}), that in these cases the raw predcitions had
more \ov that were contiguous in time making longer segments, which were not
removed while smoothing. Whereas, the falsely detected \ov were more isolated
and made shorter segments, and were removed on smoothing. These observations
hence motivate even more the importance of appropriate post-processing of the
raw predictions of a classifier that does not exploit long-term temporal
patterns. And, the relatively smaller sacrifice in \rov even on raw predictions
keeps the hopes high for the relevance of normalization to overlap detection.

In the precision vs. recall plots for both non-normalized (Figure
\ref{fig:eval-un-skipzero-precrec}) and normalized (Figure
\ref{fig:eval-mn-skipzero-precrec}) settings, temporal smoothing was seen to have a
similar impact as in previous experiments of moving the \pov to higher values
for the different calls. Similarly, the application of normalization leads to
movement of the performance to higher \rov even after temporal smoothing,
although the spread is larger here than in previous experiments where \no were
not skipped during training. This indicates that normalization is still
successfully able to reduce the mismatch between the training and testing
conditions. And, perhaps due to lack of /competition/ from \no here, the
classifier is able to learn more patterns for \ov than before. Skipping \no has
then proven to be beneficial and, as discussed earlier, a much simpler speech
activity detection method can be used in final applications to only pass speech
segments to the overlap detection system.

Nevertheless, the more important and far more severe competition from the
majority class of \ssp still exists. Experiments in the next section present
results when the classes were rebalanced during training to tackle this problem.
** DONE Training with Rebalanced Classes
CLOSED: [2017-09-17 Sun 03:51]
#+LATEX: \begin{table}
|-------------------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------|
|                         |   | $P^{(sp)}$ | $R^{(sp)}$ |   | $P^{(ov)}$ | $R^{(ov)}$ |
|                         |   |        (%) |        (%) |   |        (%) |        (%) |
|-------------------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------|
| */No Normalization/*    |   |            |            |   |            |            |
|-------------------------+---+------------+------------+---+------------+------------|
| Raw                     |   |      89.81 |      82.44 |   |      30.64 |      45.33 |
| Smoothed                |   |      90.16 |      88.74 |   |      39.77 |      43.42 |
|-------------------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------|
| */Mean Subtraction/*    |   |            |            |   |            |            |
|-------------------------+---+------------+------------+---+------------+------------|
| Raw                     |   |      89.12 |      88.39 |   |    *35.25* |    *36.93* |
| Smoothed                |   |      88.91 |      96.69 |   |    *60.46* |    *29.55* |
|-------------------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------|
| */Mean+Variance Norm./* |   |            |            |   |            |            |
|-------------------------+---+------------+------------+---+------------+------------|
| Raw                     |   |      88.74 |      90.18 |   |      36.58 |      33.11 |
| Smoothed                |   |      88.68 |      96.43 |   |      57.37 |      28.04 |
|-------------------------+---+------------+------------+---+------------+------------|
|-------------------------+---+------------+------------+---+------------+------------|
    #+LATEX: \caption{Class-wise precision ($P$) and recall ($R$) on testing set
      after training on
      *rebalanced classes, with or without different normalizations, and no silence*.
    #+latex: }
#+LATEX: \label{tab:eval-mvn-skipzero21}
#+LATEX: \end{table}
#
#+latex: \afterpage{
#
  \centering
  #+LATEX: \begin{figure}
    #+LATEX: \includegraphics[width=\textwidth]{img/eval/un-skipzero21-preds}
    #+LATEX: \caption{True labels, raw predictions, and smoothed predictions for part of a conversation,
      after training on
      *rebalanced yet non-normalized, and without silence*.
    #+latex: }
    #+LATEX: \label{fig:eval-un-skipzero21-preds}
  #+LATEX: \end{figure}
#
  #+LATEX: \begin{figure}
    #+LATEX: \includegraphics[width=\textwidth]{img/eval/un-skipzero21-precrec}
    #+LATEX: \caption{Precision vs. recall for all calls in the testing set,
      after training on
      *rebalanced inputs with Mean-Subtraction, and without silence*.
      Results for Mean-Variance-Normalization were similar.
    #+latex: }
    #+LATEX: \label{fig:eval-un-skipzero21-precrec}
  #+LATEX: \end{figure}
#
  \clearpage
#+latex: }
For tackling the class imbalance issue, the data from \ssp were under-sampled
such that the ratio between the examples from each speaker and their
combinations were equal in the training phase (Section [[Rebalancing Training
Data]]). Table \ref{tab:eval-mvn-skipzero21} summarizes the results for this when
different normalization procedures were applied. The raw results for \ov looked
humbling. While \rov had increased by many folds, the reduction in \pov was
jarring, even though such a trade-off was expected to happen.

What was more shocking was the result for the non-normalized scenario even after
the application of temporal smoothing. In earlier experiments without
normalization, the classifier was relatively very precise in detecting \ov in
the inputs, but these detections did not form many longer contiguous segments in
time. On the application of temporal smoothing most temporally isolated
detections of \ov were removed, including most false detections. This still
happens here, but far more contiguous frames are detected to have overlapping
speech than before, even for false alarms, which temporal smoothing is not able
to remove satisfactorily (Figure \ref{fig:eval-un-skipzero21-preds}).

The precision vs. recall plot (Figure \ref{fig:eval-un-skipzero21-precrec}) can
shed some light on the situation at hand. One is reminded of the big spread for
\no across the recall axis for different calls in the test set. Perhaps the
classifier is not able to compensate for the mismatch in the training and
testing conditions, which was exacerbated further by the increase in variance
for \ssp in the training set acquired due to under-sampling of the class.
Furthermore, it can be seen that \rsp for certain calls is dangerously low,
similar to the second observed degenerate case discussed at the beginning of
this chapter.

#+latex: \afterpage{
#
  #+LATEX: \begin{figure}
    #+LATEX: \includegraphics[width=\textwidth]{img/eval/mn-skipzero21-preds}
    #+LATEX: \caption{True labels, raw predictions, and smoothed predictions for part of a conversation,
      after training on
      *rebalanced inputs with Mean-Subtraction, and without silence*.
      Results for Mean-Variance-Normalization were similar.
    #+latex: }
    #+LATEX: \label{fig:eval-mn-skipzero21-preds}
  #+LATEX: \end{figure}
#
  #+LATEX: \begin{figure}
    #+LATEX: \includegraphics[width=\textwidth]{img/eval/mn-skipzero21-precrec}
    #+LATEX: \caption{Precision vs. recall for all calls in the testing set,
      after training on
      *rebalanced yet non-normalized inputs, and without silence*.
    #+latex: }
    #+LATEX: \label{fig:eval-mn-skipzero21-precrec}
  #+LATEX: \end{figure}
#
  \clearpage
#
#+latex: }
This mismatch problem was rectified earlier by normalizing the inputs, and it is
indeed observed in Figure \ref{fig:eval-mn-skipzero21-precrec} that this spread
across the recall axis is reduced significantly. For the first time,
normalization lead to lower \rov than when the inputs were not normalized, but
they were also accompanied by fewer false alarms. Furthermore, many of these
false alarms occurred as temporally isolated frames, which were then
successfully compensated for by the application of temporal smoothing (Figure
\ref{fig:eval-mn-skipzero21-preds}).

The goal of under-sampling the majority class was to reduce it's impact on each
parameter update during gradient descent. Without it, \ov made only a fractional
contribution to the calculated gradients. In order to isolate the impact of
under-sampling, the normalization was performed /before/ the random
under-sampling was performed to create the batches for the classifier in the
experiments presented in this section. This meant that the same statistics for mean and
variance were used for normalization as in the experiments in previous section.
If the normalization was performed /after/ the random under-sampling, these
statistics could have been very different in different passes over the same \ov
examples. It is not known whether this change would have helped in detecting \ov
better and further experiments should probably test for this.

Nevertheless, one could have predicted that the classifier will perform the best
when it has been assisted the most. The performance observed when
the training data was rebalanced and the inputs were mean-normalized is arguably
the best that was achieved during this thesis' works. It is not as perfect as
one would have hoped from the promises of deep learning, but given how
challenging overlapping speech detection in conversations has proven to be so
far, the prospects look promising. Additional variance normalization was seen to
slightly degrade the results, but this could be due to the procedure that was
followed for normalization in these experiments (explained above).

Finally, many individual occurrences of overlaps are still being missed, and the
increase in recall has come with a steep decline in precision. Attempts
to work with a cost-sensitive objective function did not give
reliable results, especially across the different configurations of the
variables. It was further observed that in most such scenarios, the training
itself had not converged to a stable loss value. These results were thus omitted
from this thesis, and this avenue remains to be explored after this thesis'
submission. The last Chapter of this thesis summarises even more directions that
future works should explore, while also summarizing what was achieved in this work.
* DONE [0/0] Conclusions and Future Prospects
CLOSED: [2017-09-17 Sun 17:13]
The study of human interactions has a keen interest in the phenomenon of
/double-talk/ that occurs in normal conversations, where participants speak
simultaneously for brief durations of time during some characteristic
situations. On the other hand, this phenomenon is a nuisance to various speech
technologies where it contributes as a major source of errors in their otherwise
state of the art performances. Automatically detecting and temporally localizing
these double-talk situations in audios of conversations was the ultimate goal of
this thesis' works. The results could be either directly useful in conversation
analysis, or direct appropriate measures for handling them in speech
technologies.

Knowing that achieving this goal has proven to be extremely challenging with
classic approaches, a Deep Convolutional Neural Networks based system was
explored. The most important goal of the exploration was to be able to use
low-level acoustic features for detecting double-talks, and avoiding heavy
feature engineering done in previous works. It was seen that, even in the presence
of many characteristic hurdles posed by the problem, the neural network had
learned to automatically extract useful features for overlap detection. Various approaches for
tackling the expected hurdles were enumerated and scientifically evaluated. It
was seen that using simple normalization procedures provided benefit by reducing
the expected mismatch between the training and testing conditions. Removing
silence from the training objective helped reduce the competition on the already
disadvantaged class of double-talk examples. The system performed better when
the disadvantage was compensated for by rebalancing the dataset during training.
The system performed best when assistance from all the above methods was made
available and when the classifier's inability to model longer-term temporal
dependencies was compensated for using a Viterbi algorithm based decoding
procedure. The decoding procedure was able to smooth over the short, noisy
predictions made by the classifier, including many false detections, while
keeping, and often even enhancing many correct detections.

The precision and recall of the system are good enough to be useful for
conversation analysis, by at least being able to direct attention for later,
more precise manual annotation. The good recall would help one in quickly finding
more instances of double-talk, and higher precision would reduce the
frustrations of too many false alarms. Furthermore, seeing that the best
configuration of the proposed system produces rather contiguous segments for
double-talks, such false alarms would count to be even fewer when only
enumerating the segment-wise individual occurrences of double-talk. Speech
technologies will similarly benefit by being able to reduce the amount of errors
faced by a baseline system that cannot detect and handle overlapping speech.

The modular and cross-platform implementation of the system will enable
smooth and reliable deployment to most application scenarios. Furthermore, future works
along this direction would require minimal modifications to test new approaches.

The most important among the tasks slated for the future is to evaluate the
proposed system on the datasets that have been used by previous works, so as to
have a more directly comparable set of results. This is expected to involve adapting the
proposed model to the new dataset, perhaps by retraining a few layers of the
neural network on a subset. Some other adaptation procedures would also be
required during the final deployment of the system. Chief among them would be
more powerful normalization and appropriate denoising procedures, so that the
system is more robust against the less than ideal nature of real world signals.

Furthermore, an automatic speech activity detector will have to be integrated
into the proposed system, seeing that removal of silence from the classification
objective had helped in the final results. It was observed in the many exploratory
experiments for this thesis that a neural network based system for speech detection
performed very well. Perhaps a heirarchial classification approach could be
taken, where silence is removed in the first step by one neural network and
overlaps are detected in a second step.

Talking of heirarchial systems, improvements could be achieved by using the
information from other modalities in addition to the acoustic features. One
approach could be to combine the outputs of an automatic speech recognition
system with the ones of the proposed system for obtaining a more sophisticated
decoding procedure. Such a procedure could exploit the characteristic failures
and perplexities in the outputs of the speech recognition system in
regions of overlaps, and add to the performance achieved using only the proposed
system. This approach would prove complicated to appropriately implement, but
the endeavour could prove worthwhile.

Nevertheless, only a subset of the originally assigned splits of the Fisher
Corpus were used in this thesis' works. While the reasons for it were practical,
it is expected that improvements after using the entire dataset would only be
moderate, while costing disproportionately more training time. More
beneficial would be to work with a different dataset which is sufficiently large, is
representative, and, crucially, has higher fidelity audios and finer annotations.
However, such a dataset would certainly prove difficult to acquire. Prehaps the
idea of training on artificially overlapped speech can be revisited. While doing
so, it will be important to avoid the various pitfalls involved in the process
of creating such a dataset. Care should be taken that the new dataset faithfully
reflects the characteristics of natural double-talks with respect to typical
duration, production and content. And final evaluations must be carried out on
real conversations.

With respect to other approaches for tackling the inherent class imbalance,
different parameters or procedures for rebalancing the training data could be
explored. More urgent would be to thoroughly investigate different cost-sensitive objective
functions with more reliable implementations. If no improvements are achieved in
the latter case, at least a comprehensive study speculating on the possible
causes should be done. Learning from imbalanced classes is a common challenge
when working on real world problems, and contributions in improving the
performance of deep learning technologies in such situations will be widely
influential.

Finally, the Deep Convolutional Neural Network that was proposed in this thesis
was settled on after many initial exploratory experiments, where it was very
clear that more powerful configurations performed better on the given task. The
particular architecture proposed here had to be fixed to evaluate the impact of
other problem specific variables, and it had to be limited to meet the
constraints of time and resources. Architectures with more number of learnable
parameters and different layer configurations should be explored if
the same formulation as the one proposed in this thesis is used. Care should
however be taken in tackling expected problems of overfitting and those due to
the inability of current /raw/ formulation to model longer-term temporal
dependencies. For the latter, the use of Recurrent Neural Networks for modelling
such dependencies, and related architectures where convolutional layers act as
feature extractors, should be explored. Such directions would require a larger
investment of resources but openly available state of the art frameworks should
at least reduce the impelementation effort. Detecting double-talks in
conversations continues to be a challenging problem, and, given the results
presented in this thesis, further efforts in solving the problem should be
encouraged to continue testing the powerful promises of deep learning.

\clearpage

# * Bibliography
# \bibliographystyle{plain}
# \bibliography{thesis.bbl}
\printbibliography[heading=bibintoc]
* Workflows                                                        :noexport:
[[https://bitbucket.org/motjuste/masters][This repository on BitBucket]]
** org-mode setup

Look at all the fiddling I have done, and there is bound to be more.

We have some example thesis.org files in `Documents` if you ever need
inspiration. Also checkout the references.
*** References
- [[http://bastibe.de/2014-11-19-writing-a-thesis-in-org-mode.html][Writing a thesis in org-mode]]
- [[http://www.macs.hw.ac.uk/~rs46/phd-thesis.html][Rob Stewart's PhD thesis]]
- [[http://orgmode.org/manual/In_002dbuffer-settings.html][Summary of in-buffer settings]]
- [[http://orgmode.org/manual/Export-settings.html#Export-settings][Export settings]]
- [[http://orgmode.org/manual/Embedded-LaTeX.html][Embedded LaTeX in orgmode]]
- [[https://www.gnu.org/software/emacs/manual/html_node/emacs/Specifying-File-Variables.html][Specifying File Variables]]
** DOIN [17/38] Finale Planne whatever
Most of this is going to have to be talked about in the [[Approach]] Section of the
thesis, and maybe also in the [[Introduction/Preliminaries]] where the concepts are general.
*** WAIT [0/4] Data Analysis
**** WAIT [0/2] Fisher
***** WAIT [0/9] About
- [ ] Where does the data come from, with reference to paper
- [ ] What does the data have
  + [ ] From the main readme of the dataset, all the params
- [ ] Why use this dataset
  + Real Double Talks, similar to KA3
    + [ ] Some examples
  + Not a laboratory dataset (?)
  + SNR (?)
  + Giant, may help the models generalize better
  + [ ] How have others used it?
- [ ] What part was used
- [ ] How is double talk inferred
- [ ] What are the limitations / problems
  + Only Telephone conversations, and only in English
  + Designed for speech recognition for conversations
  + VAD done automatically, not manually, only transcription done manually
  + No way to explicitly determine unique number of speakers over the dataset
  + Segmentation not as fine as TIMIT
  + Some parts are not annotated, and have to be taken out carefully
***** WAIT [0/13] Analysis of segment lengths : General, 0T, 1T, 2T
- [ ] *Do all analysis in a notebook, either here or `rennet-x`*
- [ ] *Do All analysis at /frame level/*
- [ ] *Use consistent colors*
- [ ] What is the annotation length + histogram
- [ ] What are the inferred segment lengths for 0T, 1T and 2T + histograms
- [ ] When do 2T segments occur? Check also @heldner:2010pauses
  + [ ] S1 -> DT -> S1 (back-channel)
  + [ ] S1 -> DT -> S2 (turn)
  + [ ] S1 -> DT -> No (back-channel)
  + [ ] No -> DT -> Sx (overlapping-start and takeover)
  + [ ] No -> DT -> No (overlapping-start and backing down)
- [ ] What is the gender distribution for different segment lengths, 1T and
  2T, + pie-chart of n-frames + /maybe/ histograms
- [ ] /maybe/ What are the distributions for other params, like topic-id,
  dialect, etc.
*** WAIT [0/2] Data Preparation
**** TODO [0/6] Split into train, val, test/eval
- [ ] Which groups were added to which split, and possibly why.
  + [ ] replicate the same labels data on myrmidon as planned on unumpu, even
    though we do not have the audio, so that we can do some analysis while at home
- [ ] Check the distributions of different statistics
  + [ ] segment lengths : general, 0T, 1T, 2T + histograms
  + [ ] gender distributions for 1T, 2T + pie-chart + /maybe/ histograms
  + [ ] /maybe/ the distribution of other params
**** WAIT [0/7] Convert all to merged, mono, 8kHz, wav files
- [ ] Mention that we only export parts of the audio that are within =min-start=
  and =max-end=, although we actually do it before feature extraction on the
  read =numpy-data= later on.
- [ ] Check how it is being done in =pydub= and document
- [ ] expected to be =int16= files, without compression, and equal weights for
  all channels.
  + [ ] The values are normalized at the time of feature-extraction to be in
    range (-1, 1) and mean 0 when getting input to feature extraction. Done by =librosa.load=.
  + [ ] Check for each split to confirm.
- [ ] this is where the model hyperparameters have already started to
  accumulate, although it is arguable if using only Telephone conversations
  should be made part of that, especially since we are working with Deep Learning.
  - [ ] how to account for robustness?
*** WAIT [5/7] Feature Extraction
**** DONE [2/2] Load audio using =librosa.load=
CLOSED: [2017-08-04 Fri 22:12]
- [X] make sure that they are in the range (-1, 1) and mean close to zero.
- [X] Take only the slice between =min-start= and =max-end= calculated with =samplerate_as(audio_samplerate)=.
**** DONE [7/7] Calculate the fbanks_64
CLOSED: [2017-08-04 Fri 22:10]
- [X] use params:
  + win_sec = 0.010
  + hop_sec = 0.032
  + samplerate = 8000
  + window = 'hann'
  + power = 2
  + n_mels = ~{40, 64, 96}~ = 64
    + [X] check that audio-classification-keras guy's explanation for 96
      * I can't find anything about his work.
    + Going with =64=. It serves a nice middle ground of serving the purpose of
      fbanks and keeping more information as far as I am concerned. Refer
      [[file:img/fbanks-v-spect.png]]
    + Yes, this will mean that the training will be slower than that for 40.
    + I am hoping that it will keep enough info about the speakers as well, more
      than 40 would.
    + I just can't find justification for 96, except if my samplerate was really high.
- [X] Use =librosa.features.stft= with =center=False=, and implement wrappers
  + [X] Save simple log10 of the mel-scaled spectrogram
  + [X] *Make sure that the final shape is in terms of (time, frequencies).*
    * [X] *Make sure that the shape in time dimension matches =samples_for_labelsat=.*
    * [X] Have to write at least my own spectrogram just to set center-ing to
      False. Damn you librosa!
**** DONE [2/2] Make 16k equivalent long chunks per-file and save as single dataset in master h5
CLOSED: [2017-08-04 Fri 22:12]
***** DONE [11/11] Dry run with a single file from validation set
CLOSED: [2017-08-04 Fri 22:12]
- [X] Make overlapping chunks with =strided_view=
  + win_shape = =2**14 = 16384=
  + step_shape = 10 seconds = 10 * 100 = 1000 ~= =1024=
- [X] Concatenate them either using =dask=
- [X] when reading into dask, make sure that chunk-size is win_shape, aka 16k equivalent.
- [X] Create one hdf5 dataset per file.
- [X] Make sure that all chunks for a file are stored in the same dataset in h5.
  + [X] Make sure that the chunking value is the same as the 16k equivalent we created.
  + [X] Check that reading all chunks do give the expected results.
- [X] Use compression
- [X] Use Checksum
- [X] Add fft-frequencies as attribute or whatever =h5py= provides, to each dataset.
  + [X] Check [[http://docs.h5py.org/en/latest/high/dims.html][Dimension Scales]] in h5py
  + Couldn't, and wouldn't ... h5py was not helping
***** DONE [5/5] Final notebook for all splits
CLOSED: [2017-08-04 Fri 22:11]
- [X] Keep to and from location for data configurable.
- [X] Run on *myrmidon*
  - [X] Remove old data for new space.
- [X] Run on *unumpu*
  - [X] copy results to *nm-raid*
**** DOIN [1/6] Normalization
- [X] see if the log-mel-spec values are in good range
- [ ] Normalize on chunk (== utterance) level at the time of feeding into the network.
  + Don't worry about skipping vectors with silences. Fuck it.
- [ ] Do dimwise-MN
- [ ] Do dimwise-MVN
- [ ] Do pixel-MN
- [ ] Do pixel-MVN
***** Why?
- Main reason: compensate for mismatch in training and testing conditions,
  especially related to channel distortions.
- Also: Make it more suitable for the machine learning algorithm.

**** IDEA [0/3] Add other possible features for future investigations, mainly as text
- [ ] look at links on MFCC, iVectors, prosody, pitch, CNSC, PLP, etc. for inspiration.
  + [ ] Look at these links from Todoist
    * [[https://en.wikipedia.org/wiki/Pitch_(music)][Pitch (music) - Wikipedia]]
      - [[http://www.fon.hum.uva.nl/praat/manual/Intro_4_1__Viewing_a_pitch_contour.html][Intro 4.1. Viewing a pitch contour]]
      - [[http://librosa.github.io/librosa/generated/librosa.core.piptrack.html#librosa.core.piptrack][librosa.core.piptrack — librosa 0.5.0 documentation]]
      - [[http://www.let.uu.nl/uilots/lab/courseware/phonetics/basics_of_acoustics_1/praat_pitch.html][praat_pitch]]
    * [[https://github.com/timmahrt/ProMo][timmahrt/ProMo]]
    * [[http://www.nature.com/articles/ncomms13654][Rapid tuning shifts in human auditory cortex enhance speech intelligibility]]
    * [[https://www.kaggle.com/primaryobjects/voicegender][Gender Recognition by Voice | Kaggle]]
    * iVectors : [[http://pydoc.net/Python/bob.spear/1.1.8/spear.utils/][Python bob.spear package v1.1.8, spear.utils module source code :: PyDoc.net]]
    * [ ] Fractal Dimensions, check zotero
- [ ] See if the argument that 'spectrogram' is the mother of all features
  still holds.
*** DOIN [12/18] The Classifier and Configurations
**** DONE [0/0] Fixed number of =steps per chunk=, {=8= or whatever runs (likely one with 2k examples)}
CLOSED: [2017-08-10 Thu 20:52] DEADLINE: <2017-08-06 Sun> SCHEDULED: <2017-08-05 Sat>
- This is to make sure we can predict exactly how many steps are required for a pass
- The same number of chunks are to be used with the same number of steps per chunk
  + For skipping/sub-sampling, do it before feeding into the stepper.
**** TODO [0/5] Number of epochs / passes over the dataset - =steps_per_chunk epochs per pass= * {=5=, =10=, =20= =passes=}
DEADLINE: <2017-08-06 Sun> SCHEDULED: <2017-08-05 Sat>
- [ ] Make atleast 5 passes over the entire dataset for every model.
- [ ] upto 21 = (5 + 5 + 10) total passes for the best/most promising/relevant models.
  + 2 types of promising results both working with atleast the same features,
    and other such input of same parameters, like, context, etc.:
    1. Excellent 0T v/ {1T + 2T}
    2. Best 1T v/ 2T, with 0T skipped
  + [ ] build upon saved checkpoints from earlier runs.
  + [ ] pass starting epoch as a parameter to Keras.fit_generator?
- *Fixed number of keras epochs per pass anyways* == 8
- Since there is a fixed number of steps per pass, irrespective of skipping or
  subsampling, the number of steps per keras epoch is also fixed.
  + equivalent to (total_steps // 8) + 1 for a keras epoch.
  + equivalent to ((nchunks * 8) // 8) + 1
  + Or, just use nchunks as the nsteps for keras epochs, lel
  + [ ] Make sure these invariants hold
- It is okay if we pass over a little more due to rounding, but we don't want to
  pass less than the entire dataset.
**** WAIT [2/5] The Neural =Network= - {~c3~}
- There is essentially just one model based on the code below.
- There is one output per-sequence, as in, we do sequence classification, but
  not at utterance level.
- The configurations will decide:
  + The input shape, and hence the context per frame.
  + The number of classes.
- We use BatchNormalization *BEFORE* Activation, to follow the original paper.
- We use Categorical crossentropy, and categorical accuracy.
- [X] We use adamax as optimizer, but this can change
  + Nah .. we're sticking with it ... too many other things waiting
- [ ] Use Average Pooling instead of Max Pooling?
- [ ] Check more conv nets for speech and decide one final that we can run.
  - [ ] @deng:2013deep
- [X] Move this to =keras_utils= or =models= or =model_utils=, and actually see
  the model output. Too much time getting wasted in making it work in spacemacs.
  + moved to =keras_utils=

#+BEGIN_SRC python :results output
  from keras.models import Sequential
  import keras.layers as kl

  def c3(input_shape, nclasses=3):
      model = Sequential(name='conv3')

      # first conv2d layer
      model.add(kl.Conv2D(
          64,
          3,
          strides=1,
          data_format='channels_last',
          input_shape=input_shape[1:],
          name='c1_3_64_1',
      ))
      model.add(kl.BatchNormalization(name='c1_bn'))
      model.add(kl.Activation('relu', name='c1_relu'))
      model.add(kl.Dropout(0.1, name='c1_d_10'))
      model.add(kl.MaxPool2D(2, name='c1_mxp2_2'))

      # second conv2d layer
      model.add(kl.Conv2D(
          128,
          3,
          strides=1,
          data_format='channels_last',
          input_shape=input_shape[1:],
          name='c2_3_128_1',
      ))
      model.add(kl.BatchNormalization(name='c2_bn'))
      model.add(kl.Activation('relu', name='c2_relu'))
      model.add(kl.Dropout(0.1, name='c2_d_10'))
      model.add(kl.MaxPool2D(2, name='c2_mxp2_2'))

      # third conv2d layer
      model.add(kl.Conv2D(
          256,
          3,
          strides=1,
          data_format='channels_last',
          input_shape=input_shape[1:],
          name='c3_3_256_1',
      ))
      model.add(kl.BatchNormalization(name='c3_bn'))
      model.add(kl.Activation('relu', name='c3_relu'))
      model.add(kl.Dropout(0.1, name='c3_d_10'))

      # max globally
      model.add(kl.GlobalMaxPool2D(name='gmxp'))

      # first FC
      model.add(kl.Dense(512, activation='relu', name='f1_512_relu'))
      model.add(kl.Dropout(0.1, name='f1_d_10'))

      # second FC
      model.add(kl.Dense(128, activation='relu', name='f2_128_relu'))
      model.add(kl.Dropout(0.1, name='f2_d_10'))

      # second FC
      model.add(kl.Dense(32, activation='relu', name='f3_32_relu'))
      model.add(kl.Dropout(0.1, name='f3_d_10'))

      # output layer
      model.add(kl.Dense(nclasses, activation='softmax', name='sfmx'))

      # Compile and send the model
      model.compile(
          loss='categorical_crossentropy',
          optimizer='adamax',
          metrics=['categorical_accuracy'],
      )

      return model

  input_shape = (None, 21, 64, 1)
  c3(input_shape).summary()
#+END_SRC
**** DONE [2/2] Features to use - {=fbanks_64=}
CLOSED: [2017-08-05 Sat 16:20]
- [X] choose one between ~{40, 64, 96}~, and stick to it.
  + [X] We wait on the final decision any way from [[Workflows/Finale Planne whatever/Feature Extraction]]
**** WAIT [1/2] Making sequences to input with =context= - {=±10=}
SCHEDULED: <2017-08-05 Sat>
- There are multiple options, and adding more context has helped results.
- I have decided to choose and evaluate only on ±10 frames (±100 ms).
- The decision comes from @ryant:2013speech
- We can go for ±20 or ±30 as in @xiong:2016achieving, but why not:
  + hardware limitations
  + Run time limitations
  + [ ] Add this to future works
- [X] Add `[..., None]` at the end to make it `channels_last` for conv2d
**** DONE [1/1] =Skipping= class(-es) {=0T=, =None=}
CLOSED: [2017-08-10 Thu 20:54] DEADLINE: <2017-08-07 Mon> SCHEDULED: <2017-08-05 Sat>
- *We only experiment with skipping 0T when we choose to, and it is preferable*
  + skipping 1T does not make sense, use subsampling instead
- We *still* maintain the same number of steps per chunk, even though the
  batches now may be of different sizes
- [ ] should check it out offline first to see that there are no unforseen
  circumstances where the batches may end up being empty.
- We want to avoid making copies of giant arrays, so the convoluted algo below.
- *Skipping will be done on validation/test data as well*
***** DONE [0/0] How to skip, the algo
CLOSED: [2017-08-10 Thu 20:54]
- Do normal strided data_prep
- Do normal strided label_prep.
  + This is the final decision of labels to skip or not is made.
- The label prepper returns two things
  1. the prepped_labels
  2. keep, which is:
     + True, if to keep all
     + np.array of booleans of size of prepped labels, indicating which example
       to keep
- The packaging method that sends data to stepper forwards all three
- The stepper looks at keep
  + if not is nd.array and True (check beforehand that this is never False)
    * return step-wise in nsteps_per_epoch
  + if nd.array of booleans
    * do cumsum of keep
    * nexamples_per_step = cumsum[-1] // step_per_epoch
    * ends = searchsorted(cumsum, arange(steps_per_epoch)  * nexamples_per_step,
      side='right')
    * starts = [0].extend(ends[:-1])
    * return data[keep[start:stop]] and label[keep[start:stop]] for start, stop
      in zip(starts, ends)
- Make sure that we never go out of bounds in our calculations, and never reture
  empty batches/steps, and always return the same number of steps per chunk.
**** DONE [1/1] =Sub-sampling= class(-es) {=1T_0.2=, =None=}
CLOSED: [2017-08-10 Thu 20:54] DEADLINE: <2017-08-07 Mon> SCHEDULED: <2017-08-05 Sat>
- *We only ever sub-sample 1T, cuz it is the majority class*
- Since the contexts would have been already added by now, we can subsample all
  1T, whether or not it is near 2T or 0T
- *Sub-sampling is never done on the validation/test set*
***** DONE [0/0] How to subsample, the algo
CLOSED: [2017-08-10 Thu 20:55]
- Do data and label prep as is, and also intercept the keep from label_prepper
  + Therefore, subsampler will be a sub-class of skipper, /maybe/
- make a var keep_list = []
- do a groupby on labels, key of label == class (1T)
- if key == False:
  + make array of all trues of the size of group
  + Append to keep_list
- if key == True:
  + make array of all False of the size of group
  + Set every nth (=5) to True, starting at first
  + Append to keep_list
- np.concatenate keep_list, which should be the same size as keep returned earlier
- set keep = keep && keep_list
  + this ensures that any skipping is carried along as well.
**** WAIT [0/1] Class-/Sample- =Weights= {=clsw_1= =clsw_2=}
DEADLINE: <2017-08-06 Sun> SCHEDULED: <2017-08-05 Sat>
- *We only use class weights, set them to 1 for both 0T, and 1T*
- we prefer to set class weight for 2T as 1, and at most 2
  + prefer 1 especially when skipping and/or sub-sampling
  + using 2 to perhaps support the argument that adding a cost matrix doesn't
    help much. I hope the results support it.
- *We use all ones as clsw when either skipping or subsampling*
  - [ ] Or do we?
- Why not more:
  + because, 2T is very similar to 1T.
  + It hardly ever gets confused with 0T.
  + Too much clsw for it has been shown to make the network results less confident.
    - Elaborate, with examples.
  + Slows down training in some ways.
  + We are also not 100% sure about each and every label. There is a collar.
- Why not based on data, entire or per batch:
  + the class weights become even more skewed.
  + Experiments were performed, things went wrong
**** WAIT [1/5] Choosing =label= for a sequence - {=center-frame=, =max-mid-±5-frames=}
SCHEDULED: <2017-08-05 Sat>
- priority is to choose the center frame with the idea that we are doing frame
  wise prediction, and the context is just there to ... well, put the frame in context
- The other choice of choosing the max of the mid ±5 frames, as per our
  inspiration, is to say that there is 2T happening somewhere near the center.
  + in terms of time, it means that we are adding a collar to the boundary at
    training time. This collar basically makes the boundaries fuzzy, in priority
    of 2T > 1T > 0T.
  + What is the collar size? The boundaries get fuzzy by ±5, depending on which end.
  + [ ] Hence, the post-processing should make sure that segment lengths are atleast
    11 frames (=110 msec) at the end.
  + [ ] The validation data to keras and confusions callback should have this
    collar applied, if the training data does so
  + [ ] We may even apply this collar to the val/test data on trainings not
    done with this collar
  + [ ] Finally, there should also be an evaluation step on the pure,
    un-collared val/test labels as well.
- Both of these can be implemented in the same code, cuz we only have to max,
  and both types involve the center frame anyway, with a var label_ctxt, which is:
  + 0 : for center-frame
  + 5 : for max-mid-±5-frames
- [X] maybe be smarter about creating strided_views based on data_ctxt and label_ctxt
  - There is not much smart way around it, since we need to know what frames
    need to be in the center. Have to create the strided views with the same params
**** DONE [2/2] Save model =checkpoint= on every keras epoch - {=per-keras-epoch=}
CLOSED: [2017-08-06 Sun 19:49] SCHEDULED: <2017-08-05 Sat> DEADLINE: <2017-08-05 Sat>
- [X] decide on file name formatting.
  + should reflect the true epoch and sub-epoch number
  + [X] Add a convenience function to =keras_utils= that accepts activity path
- Save checkpoints every keras epoch
**** DONE [2/2] Save =Tensorboard= events {=per-keras-epoch=}
CLOSED: [2017-08-06 Sun 19:52] SCHEDULED: <2017-08-05 Sat> DEADLINE: <2017-08-05 Sat>
- [X] Is the images and stuff not showing up an issue from my side?
  - Very likely that this is due to using generator for val data, because that
    is not mentioned to lead to histogram generation.
- [X] Is there a way to append to existing events file, instead of adding a new one?
  - if nothing else, if keras reflects what epoch we are training on, maybe that
    will help.
  - Skipping this ... cuz we can plot our plots from the accuracy in the log
  - Plus ... since we plan to pass =initial_epoch= to keras.fit ... there is a
    likelihood that the tensorboard reflects that.
**** DONE [0/0] Part of =training data= to use - {=all=}
CLOSED: [2017-08-05 Sat 16:21]
We use all the data we have for training. We'll see that we train each model for
atleast one pass. Of course, we pass it via the data provider.
**** DONE [3/3] Part of =validation data= to use - {=('00007', '00013', '00028', '00062', '00065', '00069', '00086')=}
CLOSED: [2017-08-10 Thu 20:58] DEADLINE: <2017-08-07 Mon> SCHEDULED: <2017-08-07 Mon>
- We use the same data for *all* validations while training, the keras one and
  the confusions one as well
  + so the confusions are printed for the same predictions/loss
- [X] *Find 1 or two calls after data-analysis over the extracted frames*
  + We have the validation data on myrmidon, so can be done at home
  + Choose for large ratio of 2T, long/good 2T
  + Sorry, we are, for now, choosing very many more than just 2, hoping it will perform
    + chosen based on gender, ratio of 2T, length of 2T, etc.
    + More explanation, for now, not required
  + If not, we'll reduce the number
  + Added that list of calls to =datasets.fisher= for easy access.
- [X] give as generator
  + We can also predict the nstep size as it is with nchunks * 8
- [X] Do not do any sub-sampling in the validation data provider.
  + skipping will be done, however, if done on training data.
**** DONE [11/11] Chatty =Confusions= in callback - {=init+per-keras-epoch=}
CLOSED: [2017-08-06 Sun 19:55] SCHEDULED: <2017-08-05 Sat> DEADLINE: <2017-08-05 Sat>
- [X] Add to a new =keras_utils= file.
- [X] requires path to export the h5 to.
- [X] Make a confusion calculation on init, on 0th batch of 0th epoch
  + this is to make sure that any errors due to the size of the batch are caught early.
  + Save the true labels
  + Save the output and confusions, and print the prec rec, to give idea of if
    the network has learned anything at all, calling it 'init' in the file/logs
- [X] After the init one, Only do per keras epoch, and we have 8*npasses any way.
  + [X] how to calculate preds? =predict_generator=?  =Yes=
  + [X] print out the precision and recall for all classes being trained on, with correct
    epoch and sub-epoch number.
  + [X] Save the full confusion matrix as well.
- [X] print full confusion after training ends.
- [X] Make sure that there is no code being called that needs packages not on
  the GPU.
- [X] Structure of the h5?
  - basically: [/init/x, x/pass/epoch, /final/x, /true] where x = [preds, confs,
    precs, recs]
- [X] use the same val_dp as one given to keras for it's validation
**** DONE [2/2] =Predict= over validationat the end of training, in the script - {=val-all=}
CLOSED: [2017-08-10 Thu 20:57] SCHEDULED: <2017-08-07 Mon>
- [X] predict only on the last epoch? What if it starts overfitting?
  + We explicitly predict at the end of training
  + Hopefully, the geepu thing will make it run fast enough, and
  + give an idea of run time on test data, if we ever decide to run on that.
- Prioritize saving the model first before this. We can do it offline, and
  probably will have to anyways.
  + Want to do on GPU cuz there is so much data, and inference takes that long.
- [X] Have to come up with a better /loop/ to save the predictions, cuz
  =predict_generator= works on returning one giant numpy-array.
  + We're doing explicit predict on batch on every step from flow.
  + No multi-processing, but atleast, it is supposed to run predictably
  + We save the trues and preds and the confusion matrix based on the same
    structure of labels in the chunking info
**** IDEA [0/1] Adapting a model for KA3
- [ ] This depends on whether or not the models will be evaluated on the KA3 dataset.
- Looks time consuming, and not much promising.
*** WAIT [0/0] The Table of Experiments
**** Fixed
- Values between '~~' may change before final models
|----------------------------+------------------------------------------------------------------------------|
| What                       | Value                                                                        |
|----------------------------+------------------------------------------------------------------------------|
| features                   | fbank 64                                                                     |
| data_ctxt                  | ±10                                                                          |
| steps_per_chunk            | 8 or whatever doesn't crash                                                  |
|----------------------------+------------------------------------------------------------------------------|
| val_dp_shuffle_seed        | None                                                                         |
| val_dp_callids             | =('00007', '00013', '00028', '00062', '00065', '00069', '00086')=            |
|----------------------------+------------------------------------------------------------------------------|
| trn_dp_shuffle_seed        | 32                                                                           |
| trn_dp_callids             | all                                                                          |
| epochs_per_pass            | steps_per_chunk                                                              |
|----------------------------+------------------------------------------------------------------------------|
| nclasses                   | 3                                                                            |
| input_shape                | trn_dp.inputdatashape                                                        |
| model                      | ~c3~                                                                         |
|----------------------------+------------------------------------------------------------------------------|
| trn_steps_per_epoch        | trn_dp.nchunks                                                               |
| val_steps                  | val_dp.steps_per_pass                                                        |
| max_q_size                 | 2 * trn_dp.steps_per_chunk + 1                                               |
| verbosity                  | 2                                                                            |
| pickle_safe                | True                                                                         |
| model_checkpoints          | per-epoch ='w.{epoch:03d}-{val_loss:.3f}-{val_categorical_accuracy:.3f}.h5'= |
| confusions                 | init + per-epoch on val_dp + final                                           |
| tensorboard                | per-epoch                                                                    |
|----------------------------+------------------------------------------------------------------------------|
| predict_on_inputs_provider | val.for_callids('all')                                                       |
|----------------------------+------------------------------------------------------------------------------|
**** Variables, and order of experiments

*** WAIT [0/1] Post Processing / Smoothing / Inference
- [ ] What was that thing where the likelihoods were multiplied by something
  before feeding into the HMM?
*** WAIT [0/0] Evaluations and Comparisons
** WAIT [0/1] Finale Palnne - Remainers
*** WAIT [0/7] Feature Extraction
- [ ] Extract the same features for KA3
- [ ] will have to argue on why not spectrogram, so cite recent works by
  @deng:2013recent ... maybe
  + [ ] /maybe/ and why log
- [ ] add to thesis plot of mel-frequencies
- [ ] /maybe/ document what is being done to maintain audio b/w (-1, 1) (look at =librosa.load=)
- [ ] Normalization ~Section above~
- [ ] /maybe/ other possible features for future work, ~Section Above~



* [50/61] Logs                                                     :noexport:
** DONE [3/3] 27-Jul-2017
CLOSED: [2017-08-04 Fri 21:39]
*** DONE [0/0]  5:53 PM : Setting up.
CLOSED: [2017-07-27 Thu 18:43]
I think I am going to be wasting a lot of my time fiddling with org-mode and
spacemacs. Add to that my perversion for using [[https://normanlayout.info/][Norman layout]] for typing, and I
am not sure how my numbers for productivity will look like.

And it is stupid, especially in the current context. There is a lot of stuff to
write and there is lot of stuff that will need to get done before a lot of stuff
gets written. And don't even get me started on the amount of back and forth that
will inevitably take place until the final document is ready to submit.
**** Why choose org-mode?
***** Pros
+ Pure text is easy and convenient to write, and adding $$\LaTeX$$ formatting is
  pretty easy towards the end.
+ Text files are easy to put in git.
+ There are many handy tools available for exporting, formatting, task
  management, etc.
+ I can run code from within the the org file, potentially making this repo a
  single file one.
+ I have some helpful reference usages available for using org-mode to write theses.
+ The experience can result in a life-long competency.
***** Cons
- Too many opportunities to fiddle with, especially considering I don't have
  much exprience of working this seriously, at least not with success, in
  org-mode beforehand.
  + I don't have enough experience with $$\LaTeX$$ either, but it is likely that I
    would have used Atom and some hacky, possibly inefficient process to make it
    work, just like I did for my seminar report.
- Too many opportunities to get distracted by, including making this my one-file
  repo idea, where this file holds other, non-thesis related, stuff as well,
  like these loggings.
- Can only use emacs to make best use of this file.
  * Frequent exports may be necessary.
**** Why do [[https://normanlayout.info/][Norman keyboard layout]]?
***** Pros
+ I type faster in it.
+ It is overall more comfortable for me.
+ I have some practice of using this layout while using org-mode, so not very many keys to relearn.
***** Cons
- Not very comfortable while using VIM keybindings, but not absolutely abysmal either.
*** DONE [5/5]  6:40 PM : First incision.
CLOSED: [2017-08-01 Tue 13:55]
If you don't believe me, I have writing the above log entry till *now!*

I have a bunch of things to do in order to even call all these hours to not have
been a waste. Those things shall be, at least for today:

- [X] Create an outline of the possible chapter headings.
- [X] Add some outlines in [[Introduction/Motivations]].
  + Added to a bunch of other headings too, main points that is.
  + There is still a lot of literature review kinda things needed.
  + I can keep on going, but ... hey ... good start eh!
- [X] Add links to pages that helped setup org-mode this far as references in
  [[Workflows/org-mode setup/Refrences]].
- [X] Test a preliminary export. Make sure git doesn't find it interesting.
- [X] Sync Google Drive.
*** DONE [0/0]  8:31 PM : After first incision
CLOSED: [2017-07-27 Thu 20:35]
I hope I can do this. I am finding this interesting, so that is a positive sign.
And I am talking about writing, not just fiddling with org-mode. In fact, it is
very likely that I never close this window of emacs, unless something forces me to.

I have done my things till syncing with Google Drive. It is a nice Checkpoint.

Next changes at hand are not exactly here, but in Todoist, essentially a
complete overhaul. That is definitely daunting and time-consuming, and I am
already hungry.

I hope that the next update is today, and I hope it comes with good news.

Back to the writing experience, I need to read a lot of papers again, if I have
ever come across them at all. That ... is ... scary.

Hope Allah Helps.
** DOIN [6/7]  1-Aug-2017
*** DOIN [2/3] 11:46 AM : Final Planning
No, I have not reorganized Todoist yet. Fuck!

But today, We do it!

After brainstormings and experimentation in the past days, I have come to a
conclusion which means that I basically have to start over ... from scratch.
That is definitely a daunting task. And I have to finishe writing this thing in
the meantime as well. I am very much screwed, and that will be mild to say.

And, since I have less than a month to do all of that (for buffer, we see why
later), not only does it demand excellent efficiency, but also aggressive
pruning and perhaps compromises. There is a small buffer to accommodate any
unforseen emergencies, but don't rely on it. There will be emergencies, the
first of which has been that I may have been calculating my spectrograms all
wrong till now!

Therefore, the plan, the final plan. Also, moving updating todoist to today as well.
- [ ] Make a final-ready plan in Workflows for all the things that need to be run.
- [X] Reorganize Todoist ... please ... dude ... it is unusable ... cluttered
  with outdated and/or impossible ideas and tasks.
  + [X] Find paper about `fe_03_p1` and add to zotero. :todoist:
*** DONE [3/3]  1:56 PM : Progress ... is slow
CLOSED: [2017-08-02 Wed 14:28]
After quite a bit of unnecessary waffling, I have finally started writing the
[[Finale Planne whatever]]. I started from the very beginning, hoping to make sure
that I don't miss any thing, and to organize my thoughts anyway.

I have only reached till feature extraction, although there is still a section
left for melspectrogram and normalization. There are more todos here that I am
not sure what I should do with them, and will multiply the [[The Classifier and
Configurations]] set of todos even more.

But, I am making progress. One idea that I got in the middle was to save the
chunks as overlapping by 10 seconds. That will help solve the issue of making
appropriately overlapping context frames. The choice of 10 seconds is to set the
upper-bound of the context frames I will be using later. I know that I will
actually only need like 100ms, but ... the repetition will hopefully not be an issue.

Furthermore, I may then settle to do CMN or even CMVN on the chunk level,
treating it as an utterance that is more than 2 minutes long.

Finally, I have had a few other ideas for aggressive sub-sampling.
- [X] Make sure to make this parametrizable, and skippable for validation data provider
- [X] In order to remove silences, and train only on 1T and 2T
  + Read the chunk for audio and labels
  + Remove the audio and labels where label == 0
  + Calculate the mean and variances on these
  + group the audio and labels based on label == 0 or not
  + If label == 0, keep a (context_len) amount of data, initially set to None, to_prepend
  + if label != 0, and if to_prepend is not None, prepend it to the current data
  + Make strided views for each group
    * No need to keep data from from here, cuz we are only going to group if/not silence.
  + Concatenate the strided views ... will need to make copies and increase memory need.
  + Give this to the stepper
- [X] In order to aggressively subsample 1T and 0T
  + If we have groups (as in, skipping silence)
    * create strided views with step_size = int(win_size / (larger_factor))
    * concatenate, and give to stepper
  + If not working with groups
    * still make groups based on label == 0 or not
    * Repeat above for grouped case only if all labels in the group are 1T (and/or 0T)

It is already 2:37 PM! Actually, I finished the normalization and melspectrogram
sections as well, for now. There are a LOT of todos, just for today, and many of
them are decisions and explorations. I am gonna go shower and pray. Lunch only
after all the planning has been done. Damn, the hardest parts are still to come.
*** DONE [1/1]  5:05 PM : I am late now am I not?
CLOSED: [2017-08-01 Tue 19:01]
Can't deny that I predicted this 'not being back before 5pm' thing. It's a
tragedy, and I didn't spend the time on eating. And all that while knowing that
the next sections to work on are by far the most crucial, and probably will give
me the most peace of mind. And also the fact that *I am only planning*, and all
those todos need to actually get done, and then written about in the thesis.

Hours are passing by in minutes, and I am waffling.

Here's the thing... I know that when I say that the tasks ahead are the most
important and difficult and what not, my heart starts beating like crazy and I
get stressed, and that makes decision making even more difficult and scary.
Therefore, I am not going to be too angry about this whole thing, because, one
way or another, I have to make sure all the things get done, and breaking down
will be a disaster. Calm down.

Here's an idea ... start tracking time. To the minute I say. The hope is that it
will pull me back to work when I am wasting it away, and push me away from it
when I am waffling too much and spending too much time on some thing. Like right
fucking now!. I can't plan to the minute, but that shouldn't stop me. And hey,
may be the tracking and looking at the actual time being spent will help me make
better plans!

- [X] Start tracking time, granularly, for *EVERYTHING*
*** DONE [1/1]  7:01 PM : Starting with what is kinda fixed.
CLOSED: [2017-08-01 Tue 20:07]
Have setup the trackers, tracking *EVERYTHING*, and then took a break for prayers.

The main thing is, there are so many options for the networks and the configurations.

I started with what I know, it is going to be a CNN classifying sequences, where
sequences are essentially single frames provided with left-right context.

At first I was thinking of adding one with starting filter size 3, but keep the
one with 5 as my top priority. Then, something stupid happened while adding the
source for the model and I had to restart. Then I decided to fuck it, and go
with a single network config, just the 5, and wrote the whole code.

But ... once done, I decided to make that 3. You see, now I will be working on
10ms hopped data. Furthermore, there are very few (64) filters in the first
layer (limited by hardware, per experience), so ... made that 5 into 3. /sigh/.
And, it makes sense that it fewer local patterns, and then more and more global
patterns going up. I hope it is fine. It looks a lot like the conv from that
@xiong:2016achieving paper (actually, more like VGG net) but with much fewer layers.

So, I have only one architecture to train on. I hope it works.

Furthermore, even though I may look into using configs with different context
sizes, I believe I am going to stick with (-10, +10) frames ((-100, +100) ms),
ending up in size 21 frames per sequence (210 ms). I don't want to test out the
other one for one obvious reason that I don't have the time, and there are far
more number of other hyperparameters to test out. Also, it makes sense to add
more context, and was shown to be good in that paper doing SAD on YouTube @ryant:2013speech.

And you know what ... there is still a lot of work to get done, and it is
already past 7:00 PM. 💩.

- [X] Prioritize models with ±10 frames context, and maybe skip the others.
*** DONE [0/0]  8:07 PM : Aggressive pruning ... slowly
CLOSED: [2017-08-01 Tue 20:13]
Added that section on options for context. I am adding the set of options
available for each 'hyperparameter' at the end of the heading. So far, the
features to use has more than 1 option, but I have to keep in mind that I have
not even begun to add a lot of other things.

They will depend on the arguments I want to make. The fewer, and clearer
arguments I want to make, the better I can design my experiments, and definitely
the more time I will have ... if I don't decide to go too overboard with nepochs.

I will, however, also have to keep in mind that I show-off some novelty in my
approach. Just CNN on context is an okay bet, but you know how much I want to
use the max-in-center-label approach. The only problem there is post-processing,
and that is another can of worms I have not opened yet, and it is past 8:00 PM.

Wasn't the plan that I get done by 12:00 PM. /sigh/
*** DONE [1/1]  9:19 PM : Couldn't take a break, updated network
CLOSED: [2017-08-02 Wed 14:27]
I wanted to take a break, and have dinner and what not, but in the cooling down
period and looking at how much still had to get done, I was ... waffling about
making myself something.

I realized that my earlier and complicated idea to do silence-skipping was
overly complicated, and could be done by simply removing the silence labels
/after/ they have been made into sequences. Yes, there will be forced copying in
=numpy=, but my earlier solution also involved that. Furthermore, this solution
is much less complicated, and doing sub-sampling of 1T is pretty similar, except
that I will already have the labels for the sequences, and can choose create my
boolean for keeping things by working only on the final sequence labels.

- [X] Add the approach of filtering away /after/ sequences have been made, in
  addition to the one brainstormed previously, and choose one.

After that, I actually got back to running the network and looking at the number
of params. I stopped fiddling with org mode to run my source block, and simply
copied and pasted the code in ipython. And lo-and-behold, I had missed max-pool
layers in between conv layers. Because, my number of trainable params were
beyond 1M. Now, that could definitely improve our model, but the problem in that
case would be training times. Plus, how many params do I need to learn for 3
fucking classes?

Anyway, I can still fiddle with it a little more, and change things. Like I did
by making the final FC layer tapered, instead of square. Too many params.
*** DONE [0/0] 11:32 PM : There are only so many hours ... I am awake
CLOSED: [2017-08-01 Tue 23:41]
The dinner break was a forced and unrewarding one. And then came in my nightly
routine. I don't feel very accomplished today. The biggest things are still left
to do in the planning task itself, let alone to start implementing on those
plans, which could definitely prove a lot more ominous, if not filled with
distractions and what.

If history is worth trusting, I may not even finish this planning thing. And,
also, I may not even end following a lot of it. There are definitely things that
can go wrong which may force me to abandon all hopes and get the minimum done.

The only reason I decided to get through with this was that I believed that I
had more knowledge now than ever. And that the deadlines will force me anyway.
And, otherwise, I can stop my brain from having incoherent thoughts.

The sad thing is ... a lot of the tasks are like the 'figure this out' kind, and
there are definitely more to come I am sure, my first deadline of having getting
this planning done today has been horrendously overspilled, and, just look at
how many hours it took me to write that much, I was that incoherent with my ideas.

Tomorrow is another day. Just like today.

I hope having finished a few of the easy sections will not come back to bite me
and make it more difficult for me to get started tomorrow.
** DOIN [4/5]  2-Aug-2017
*** DONE [0/0] 12:04 PM : Half the day left, after the other wasted frivolusly
CLOSED: [2017-08-02 Wed 12:08]
Well, it is officially more than 12 hours since I did something. Literally
anything useful, except probably sleep, but feeling the way I am right now, I am
not sure I even did that correctly.

We finish the plan in the next 6 hours, InshaAllah. It is very likely that I may
not have very good plans about the post-processing and evaluation sections, but
we can leave them with a few todos that can be figured out while our models are
training and we are writing.

Oh fuck ... yes ... I have been relying on writing the thesis while the
trainings ran in the background. I am screwed ... but that is not new.

Let's get started. Remember to sit properly, last night's back pain was not good.
*** DONE [0/0]  2:28 PM : Been making progress, better skipping algo now, I hope
CLOSED: [2017-08-02 Wed 14:34]
I have been making progress on certain decisions and plans. For example, I now
have it decided that only clsw are going to be used, and that too only for 2T,
and only upto 2.

I also made a lot of other adjustments and additions, that frankly I don't
remember much about right now. But, at least, I am in the zone.

Given the amount of time that has been spent, I am still going slow, but the
decisions are making me feel good, even though it is only an iceberg.

I will continue working on this, especially because I am in the zone, and don't
want to get it interrupted by anything. Not really hungry.

Furthermore, the next major sections on post-proc and evaluation can either be
straightforward, or require a lot of research. (FML). But, I can add some
desirables and todos for now and take them up later.

There is a lot of dev work waiting for me, and quite an uncomfortable few of
them will need careful research as well. Hence, I am pushing for finishing the
planning before I put something energizing (more like sleepy-making) in my stomach.

I really hope that there are no big surprises waiting for me. I really hope I
haven't been something really obvious.
*** DOIN [1/2]  4:29 PM : More progress, but shit there's so much still left.
Took a small break after the last update and push, but the sub-sampling thing,
and later the labelling thing kept distracting me (!). So, got back to work.

Yes, all of it just "planning", but I am significantly more detailed in my
decisions and algos today. I hope that works in my favor. There is one todo though.

- [X] Decide on that validation call(s) to be used while training, based on analysis.
  + choose based on amount, length, etc of 2T

There is still one giant task left, that is the table of experiments to perform.
That was /the/ main goal of this entire exercise, to end up with the final list
of experiments to perform, and the final list trainings to run. Quite a few of
them will remain tentative, especially beyond the first pass, but, let's see.

- [ ] Make the table of experiments with the configs to use

I am gonna go shower and pray. Eating, even though my stomach has started to
make sounds, will have to happen later. Hopefully, I will be done by 7:00 PM.
*** DONE [0/0]  9:29 PM : Well that break stretched for too long
CLOSED: [2017-08-02 Wed 21:35]
I had that shower and prayers break, and was actually back around 6:15 PM, but
... I just couldn't make myself to do anything apart from some tweaking here and
there. Wagering that it was the lack of food issue, I decided to take care of that.

Once done, I was still not in the mood, physically, to restart work. Followed by
a confusion between whether it was 8 PM or 9 PM, and I lost another hour.

I am still not in the mood to work, and God help me, because I just want to go
to bed, and maybe take a nap. But ... I don't want to ruin my delicated sleeping
routine which, even though I agree hasn't been paying off very well yet, but
will come in really handy soon. Actually, there might be one coming just around
the corner. And I am more confident of having one of those and sticking to the
tasks better when I am actually coding, and not researching, and definitely not
when I planning. Nevertheless, the coffee, as cold and old it may be, at this
time of day ... may end up doing something bad anyway.

In lieu of all the problems that I am having, I believe I am gonna grind a
little more, until I can't. Having finished the planning, and only being left
with develpment work might have worked, but ... well atleast I have a full
stomach and clean kitchen, and some quick snacks ready.
*** DONE [0/0] 11:35 PM : Well, got rid of the spectrogram bullshit
CLOSED: [2017-08-02 Wed 23:47]
I first started by making a nice table of the hyperparameters that going to be
fixed anyway, mainly from the scripts' point of view.

Then I used =itertools.product= on the rest of variables.

I can't lie but I was very surprised to see that total number of total
combination of variables led to atleast 32 different experiments. More scared
than surprised. I tried to reduce the list, but ... it was just not budging.

Then, I saw that using pure spectrograms was not promising much, and was
definitely going to need more epochs to settle down anywhere. Add to that that
these features are giant (129 dim). And ... all that ... for one agument that
while the networks can learn features for themselves, but helping it with better
features makes it easier. And possibly the other one that some features can end
up taking away too much speaker information away.

It was not worth it, and I remembered that Microsoft paper @deng:2013recent that
showed that fbanks worked better than not only spectrograms, but also MFCCs.

So, heck with it ... skipping spectrograms. Plus, now, the export size of the h5
files will be smaller. I am not going to add more calls to any splits though.
And, it will hopefully speed up the training times, all with the on-demand
log-ing and fbank-ing done beforehand.

It is a good decision, and halves the number of experiments I will have to run.

But that still leaves me with ... at least 16.

Aggressive pruning, I have to do that. They will depend on the set of arguments
I want to make. Best Wishes.

I am going to take a break.

Fuck ... I also have to decide on the n_mels ... FUUCK!
** WAIT [4/5]  3-Aug-2017
*** WAIT [0/1] 11:24 AM : n_mels = 64
Yes, the day started a little bit early today. Well ... I will be awake at this
time on earlier days as well, but today ... I could start working. The trick was
to start setting up the feat-ext script/notebook while watching (more like
listening, nay, hearing) YouTube.

I got in pretty easy into tackling the decision for n_mels, and ... even though
I still not sure about this, I am gonna stick with 64. It looks better than 40,
and I am not sure why I would choose 96, except if I already had a lot of
frequency bands already available (which would have needed a higher samplerate
audio in the first place).

- [ ] mention how the use of features need to be updated and a thorough study
  conducted in light of deep learning models now being prevelant.

Now, off to the next part ... looking at how the fbanks and the log fbanks look.
What values do they have, and in what range. How should I go about normalizing
them later (Most likely gonna stick with CMN).

I can't lie but, the chance than I may not be able to get my models running this
weekend was a pushing force today. Not because I am not confident in my
abilities to finish something as complicated as the training scripts in only
like 18 hours, but administrative reasons.

FML. Both ways.
*** DONE [0/0]  4:35 PM : Damn this crashing!
CLOSED: [2017-08-03 Thu 16:41]
At 1:00 PM, looking at how plotting histograms was taking time, I decided to
capitulate on my early start to have lunch. All was finished in proper amount of
time, but after that, I just did not want to return to work. I wasted another
hour here and there, and when I returned, all I have done since then is just
write some code.

And that's the thing. I making decisions about what features to use, and what
normalization to use, and why. That decision making is dangerous especially when
it is such a crucial part of the system. It is the darn fucking features that I
am talking about.

I realize that I don't have to worry too much about normalization right fucking
now and can afford to work on just the feature extraction, the log-fbanks. And I
can do the normalization debates when I am actually preparing for the models.

But the thing is *there* on my mind, and just not letting me have peace or anything.

There is another impending break any time now for Prayers. May be a shower will
help as well.
*** DONE [0/0]  6:29 PM : I hope the break works
CLOSED: [2017-08-03 Thu 18:33]
After the last update, I realized that I was wasting my energy on the wrong
thing at the wrong time. I don't have to worry too much about feature
normalization at the stage of feature extraction. I just need to make sure that
the values that I am saving make sense.

So, that is my goal for today, and, after finishing this log, I am gonna be
enumerating my tasks/checklist in Todoist for that. I will be extracting
log-mel-spectrograms, and have already written some wrapper functions for them.

I will start a fresh notebook, aggressively copying stuff where I can, and not
spending too much time in that notebook on frivolous stuff.

I will come back to planning later, likely towards the end of the day, after
another break. But, my train should not stop before I have a working script for
feature extraction.
*** DONE [0/0]  9:16 PM : Damn! is this taking time
CLOSED: [2017-08-03 Thu 21:19]
I have been working on the final feature extraction scripts, and I am only done
till setting up sources and sinks! There are pre-flight checks before I even
begin the new stuff I have to code for chunking and what not.

Damn!

It took me an hour to just put the feature extraction todos in Todoist! And the
rest nearly 2 hours ... and I am not even done with pre-flight checks!

What can I say, I like doing things right ... at the wrong fucking times!
*** DONE [0/0] 11:11 PM : Good boy pre-flight checks, no sleep before done!
CLOSED: [2017-08-03 Thu 23:18]
Yes, I am still in the pre-flight checks state, but what did you expect.

And good that I did the checks, because there are audio files that are smaller
than the labels available for them. I can't do anything about it, except that
while loading audio and extracting labels, I should trust my =AudioMetadata= for
the right number of samples available, and choose the minimum between that and
what is the =SequenceLabels.max_end= in order to determine the right endings for
the labels, and ofcourse for chopping the audio data itself.

I am kinda feeling a little smug about how elegantly the
=SequenceLabels.min_start_as= method solves the problem of missing labels beyond extrema.

I am gonna have to update a few things here in the workflows, but only after I
am done with the script. There is still the pre-flight check for how the
features are calculated, and later the monster of a task of making chunks.

For now, I have some nightly routines and Prayers to take care of.
** DONE [1/1]  4-Aug-2017
CLOSED: [2017-08-04 Fri 22:01]
*** DONE [0/0]  9:45 PM : Phew, done with feature extraction
CLOSED: [2017-08-04 Fri 22:01]
I know that I am bascally a week late on this. I tried doing it on 31-Sep-2017,
but I didn't have many of things figured out, and had to make the plans.

I was obviously successful in finishing the training scripts last night, but at
least I went to sleep with a pretty solid plan for how to implement dasking the
striding thing and concatenating such that the results were with appropriate
chunking size, overlaps, etc.

The morning today, which started pretty early I must say, especially relative to
my going to bed time, was basically implementing that idea, with a nice surprise
helping solve a problem for me as it is, thanks to some test cases I had had
running last night.

What I didn't expect, I should be scolded for it, especially because I was so
smug last night about it, was that my 'just shift the min_start to zero and
everything will work out' had a serious flaw, especially in the case that I
wanted to use it to solve my problem of having last labels beyond the length of
the audio, which itself was caught in a pre-flight check, like this problem was.
Thankfully, the fix was easy, and relatively elegant, and there was some smiling
and dancing involved.

The hard deadline of needing to be in office definitely helped speed things up,
but I was, of course, not done till the last minute. At office, the scripts (I
keep saying scripts when it is a jupyter notebook) worked almost flawlessly.
Took less than half the time for nearly 50% more data. However, I did notice
that there were a lot of empty CPU times in the bokeh plot of the dask progress
report, even though the features I was saving were 1/3 the earlier one. Most
likely, the striding thing for making chunks of constant size was messing things up.

In fact, come to think of it, there was no need to make all the chunks of the
same size, because h5py would have returned to me the right amount of data. Yes,
skipping could have made things a little messy, but I should not forget that the
main issue with the previous data provider was that I was skipping entire steps,
leaving me with no guaranteed estimates of what frames would be used, except
when I used the exact steps per chunk.

But, I hope I have saved myself time and (definitely) space while training cuz I
will be reading chunks of essentially 1/3 the size, essentially only needing normalization.

Oh you normalization ... I am confused like heck about you. I am not going to
touch you before I am done updating and implementing the training helpers, which
I plan to do this weekend. They will perhaps also make the analysis easier.

I am probably only going to update the done tasks for today ... and continue tomorrow.

This week has been tiring ... there is more to come ... I can't afford to rest
... much.
** DONE [3/3]  5-Aug-2017
CLOSED: [2017-08-06 Sun 19:46]
*** DONE [5/5]  4:05 PM : I certainly rested a bit too much
CLOSED: [2017-08-06 Sun 19:46]
Well ... let's not talk too much about it. Last night, I only went to sleep
after having made the updates I was not in the mood of making. So ... I thought
I had earned an extended relaxation session in the morning. Plus ... it is the weekend.

I have, nevertheless, tried to use the relaxation time to actually rejuvenate
myself ... and I hope I have succeeded on it. I ate ... read ... watched ... had
a long and deep sleep ... etc. Time to get back to work because I don't have
many more excuses and distractions left.

In that regard, I am most likely going to work on the trainings related utility
functions and classes, like the confusions calculations and the data providers.
Not the normalization part ... but that is rife with decision making ... which
scares me to no end, but also ... need me to have a reliable data provider ready anyways.

So, starting on those are my goals for today ... even though my planning has
essentially not even reached the most important parts of what needs to actually
shine in my thesis.

And, not just start ... I need to finish some things as well ... today! And, for
that ... I really don't know which one tackle first ... the easy one and risk
feeling like thursday past when my entire day was spent things that could have
been done by essentially copying pasting ... and ... the harder and more
involved one and risk having a yet another string of days spent on something
with no progress on other fronts whatsoever, like the finale planne.

I am gonna start with the easy one .. God help me ... but also prepend it with
some planning in Todoist (mainly because I am not finding task tracking in org
very intuitive yet ... and I just want to start with something even easier than
the easy task one.


- [X] Setup training utils project in Todoist with related tasks from here
- [X] Finish Confusions callback in =keras_utils=
- [X] keep model =c3= in the =model_utils= file. /maybe/ give a name.
  + Actually ... just added it to =keras_utils=
- [X] rename/replace confusing =training_utils.py= files.
  + There will be none now ... heh
- [X] Finish a simple =setup_callbacks= in a new =training_utils.py=
  + Make it a one-stop-shop training utils file inspired by the existing
    =rennet.training_utils= file, in the utils file.
  + =import * from rennet.utils.model_utils= to make all models available.
  + The dataprovider (or not) has to be setup by the user in his/her training /
    evaluation script ... because only they know what data they are using.
  + Everything is in =keras_utils=, and the user has to make sure the s/he is
    using the correct inputs provider, just like caring about providing the
    correct h5 files.
*** DONE [0/0]  5:55 PM : Setup Todoist ... yet to start real work
CLOSED: [2017-08-05 Sat 17:59]
Since the last log ... I have been setting up Todoist for the tasks for today
... and kinda also reorganized the outdated stuff there while at it ...

There are definitely some more possible tasks and ideas coming here later.

It is prayers time ... and I have to then get on with the real coding work for
today. I am hoping for smooth sailing ... especially because a lot of the things
are pretty straightforward, especially after having taken out a lot of decision
making from all of it ... and also because there are existing implementation
that I have written earlier that I know work.

woops ... dejavu?
*** DONE [0/0] 10:34 PM : It is taking sooo much time ... again!
CLOSED: [2017-08-05 Sat 22:38]
Damn Boy! Just printing stuff is taking so much time! What are you gonna do
about the real stuff???

It is nearly 4 hours that I have spent on this shit ... I am not finished yet
... at least not the hero tasks of printing things ...

What is causing the delay? It is taking me way too long to make even the most
trivial decisions ... and I am trying to make good ones ... I believe that most
of the code ... except the printing thing is there ... and I got distracted a
lot in the time between ... especially in stupidly trying to set some flag so
that the confusions history would be inactive ... INACTIVE?? What the hell was I
thinking?

Late start is making me pay ... and I am starting feel hungry again.
** DOIN [1/2]  6-Aug-2017
*** DONE [0/0]  3:44 PM : Long Break again huh?
CLOSED: [2017-08-06 Sun 15:50]
It was too damn easy to not have finished confusions history yesterday itself. I
was actually pretty much done by midnight. But I didn't commit and push because
I wanted to put some thought into it, plus, I hoped that it would be an easy
start and early win for today.

I actually woke up pretty early today ... and even with my shitty morning
'routine', I was kinda near starting work by 0930, but ... Fuck

Only good thing I did with my time till now is have 'some' lunch, although it is
arguable if what I had for lunch should be counted at all.

I will start by finishing yesterday's tasks ... not many left anyway ... and are
pretty straightforward ... but those are some very ominous words ... so ...
expect me to only start on something new only after 6.
*** TODO [1/2]  7:09 PM : Pretty much on time ... but ...
I am running pretty much on time ... finished a bunch of the planned things,
moved the model actually only to =keras_utils= ... and there is likely not going
to be any =training_utils=. The user will have to make sure that they are
importing the right stuff from their =dataset=, and the rest will be found in
=keras_utils=, if, of course, they are using keras at all.

Only left is committing some of the things ... but I have been distracted a little.

Distracted by ... data-preppers and data-provider related changes. Those are the
next set of tasks on my list you see.

The thing is ... it is kinda bugging me that when I implement a stepper as I
have been doing earlier, with the only change that I don't skip any step ...
that is ... there is a fixed number of steps per chunk ... that is all fine and
dandy for predictability ... but ... it is bugging me that for those consecutive
number of steps, each batch will be coming from the same 'utterance'/chunk and
hence the same call.

I will still have the issue where a batch comes from the same utterance ... but
... is there a way that the consecutive batches that keras sees ... they come
from different calls?

Apart from re-extracting the features with smaller chunking size, I can actually
simulate that where the stepper makes multiple chunkings out of the existing
one. But, I will then have the problem where my entire chunks could be
skippable, essentially negating the original purpose of keeping steps per chunk
and steps per epoch constant and predictable. I have analyzed the training h5
that I have on myrmidon, and can see that continuous segment lengths could go as
long as an expected input size / step / batch, leading to them getting skipped
if one uses the skipper.

The existing idea avoids skipping any potential batches by creating them /after/
the skipping has been done. And, as a reminder, we need stepped provider because
at full size, GPU will run out of memory, because keras essentially considers
each input as a single batch.

I can atleast shuffle the order in which the batches are provided ... even
though they will still belong to the same chunking.

- [X] Shuffle (if asked) the order of steps, and maybe also the samples as well,
  since we will be copying the data within the batch/step anyway.
- [ ] There is also the issue with when to normalize ... cuz that will result in
  copies if I do it after the striding ... and will be impacted by skipped
  classes if I do it before striding ... FUCK

I hope eating something will help ... but first ... some updates here!
** DONE [3/3]  7-Aug-2017
CLOSED: [2017-08-07 Mon 23:02]
*** DONE [0/0]  1:28 PM : Too many unsolved problems !!!
CLOSED: [2017-08-07 Mon 13:42]
Last night, I did not know how to proceed with many things regarding the data
providers. Plus, I had to attend to some other personal things. I decided to use
the time to create the KA3 dataset on myrmidon, because why not. Let's just say
that it was not as smooth as I had imagined it to be ... but not that rough
either. I had forgotten that KA3 can have more than 2 speakers as well! But ...
ultimately, the problems were solved, and I stuck to my earlier decision of
using Lisa_David as my validation data, because TBH, it sounded to best.

Nevertheless, I woke very late today ... and ... with my shitty morning routine,
was not having my coffee before 1130. But, I avoided the rest of the tragedies
by just opening up a few papers from zotero to help decide on the convolutional
network I am going to train on. But ... I got distracted by a paper on how large
batchsizes lead to bad generalizations. And that got me thinking, maybe that was
an issue with my trainings so far, because my batchsizes have been usually very
big, except probably when I was skipping 0T, when the batchsizes, in addition to
be unpredictable ... were rather small.

That got me into brainstorming whether I could ensure small batchsizes with my
new steppers, skippers and preppers. I hope I can ... given my steps_per_chunk
are big enough. Because, even if none of the chunk is going to get skipped, 8
steps_per_chunk will lead to 2048 examples per batch ... which is still 4x the
size in that paper for a batch to be the biggest small, but ... oh well ... I
could also use a different batchsize.

In fact, given that, I will have to settle on earlier decision of making batches
only after skipping has been done (i.e. by choosing from =keeps=). Consecutive
batches will still end up coming from the same chunk, but at least I can
randomize the order in which they do, and even within!

Finally, the normalization thing is going to kill me. I have no idea what to do
about it, with only the dynamic range normalization being slightly easier
looking than any other, especially considering the skipping and what not.
Nevertheless, I can't do any analyses until I have my data providers ready ...

So, those are my today's goals ... update the data providers and associated
classes, including the data preppers that, at least for now, only work with the
structure of the data, like, adding context, stepping, skipping, sub-sampling,
etc. I will probably need a notebook for the devving, for quick debugging. This
may still take more time than it should, and late start to the day is already
not helping.

I have ran out of real coffee ... /sigh/
*** DONE [0/0]  4:53 PM : I am so predictable
CLOSED: [2017-08-07 Mon 16:58]
Basically, I started from zero again ... although I did make heavy use of
copying and pasting. I am re-writing the inputs/data provider but this time
trying to keep the inheritance tree relatively more linear ... that is, there
is, so far, only parent per subclass ... but ... I am sure ... as I start adding
multiple different aspects to like stepping, etc ... there will have to be
multiple parents.

Rewriting ... in addition to bringing into the right zone so that I know what is
going on ... has also resulted in better (albeit still pretty much the same)
shuffling and initializations ... hope that speeds me up ...

... speeds me up to a place where I have no idea what to do ... like
normalization and model hyperparameters ... etc

Gonna go on a break now ... need food, need to go get coffee ... today will be a
late nighter.

BTW ... Hans Zimmer ... Dunkirk ... muah muah muah
*** DONE [0/0] 10:56 PM : I may go back to the old ways
CLOSED: [2017-08-07 Mon 23:02]
So ... I made good use of the break ... prayers, dinner, groceries, ... and was
back by 8:00 PM. I have a working Stepped Provider ... but ... I am kinda
feeling the need to go back to the multiple inheritance thing ... Smaller legos
are easier to manipulate, and have some good contraints built in ... having one
giant class like the one I have implemented which tries to do everything ... is
... well ... good for auto-completion ... but opens up doors to mistakes where
things get called and updated by some class that is not supposed to do it.

I am probably gonna go back to the old ways ... but this time ... I don't have
to rewrite anything ... just copy in the stuff. And then, I will implement the
specialized preppers and inputs providers ...

But ... I am feeling exhausted for some reason. I will go take a break for
prayers. It is very likely I don't come back till tomorrow.
** DONE [2/2]  8-Aug-2017
CLOSED: [2017-08-08 Tue 21:40]
*** DONE [0/0]  2:11 PM : Been working since early ... slowly but deliberatively
CLOSED: [2017-08-08 Tue 14:17]
Yeah ... I went to sleep early last night ... I was surprisingly tired.

Anyway, it helped me waking up early, and even with the shitty morning routine,
I was here and working around 9:00 AM.

I went with last night's resolution to keep the classes as they are ...
chunkings reader and Prepper required to make inputs provider.

Since morning, I have finished stepper, sub-sampler (and skipper when some ratio
is given to be zero, I might add a convenience wrapper, maybe), and I am
currently on the context-adder ... which is proving to be a little painful to
elegantly implement label_context ... especially the function to be applied to
choose the right label from a label_subcontext.

Been working for ~5 hours ... I think I will take a lunch and Prayers break.
Hopefully back by 1600, preferably, but unlikely, earlier.
*** DONE [0/0]  9:31 PM : =BaseWithContextClassSubsamplingSteppedInputsProvider=
CLOSED: [2017-08-08 Tue 21:39]
That break went into the 1800's, quite a bit later than 1600. Actually, I was
done with the eating and praying in the 1600's, but I was still so exhausted and
sleepy that I decided to take a small lie down in bed.

Once back, I was still sleepy, and may be with a vague headache. I decided that
since going to bed early was now almost certain, I should atleast finish
everything till Normalizers in =h5_utils=.

And that's where that long-ass named hero class comes in the title.

Anyway ... I have not tested most of the things at all, since for that, I will
have to implement them using =fisher.H5ChunkingsReader=, which I haven't looked
at yet. And it is important that I do that, because, I am playing with a lot of
multiple inheritance bullshit, and things can go wrong anywhere.

Anyway ... I might do that before bed ... if I am up for it... else ... hope
that tomorrow starts early, and ... that there are no giant bugs ... and that I
able to make quick decisions about normalization quickly, maybe because I may
have a shot at going to office and starting my first model training.

~700 lines ... damn you =rennet.utils.h5_utils= ... and barely any tested
outside some manual ones. However, since the hero class basically calls
everything, if my first test on that passes ... then ... I don't think I have to
worry too much about anything else. Otherwise ... definitely tomorrow.
** DONE [2/2]  9-Aug-2017
CLOSED: [2017-08-10 Thu 20:34]
*** DONE [0/0]  8:09 PM : Last double digit day and look at me wasting time
CLOSED: [2017-08-09 Wed 20:25]
I had started to get the headache tomorrow night which would go on to ruin my
falling asleep at the appropriate time. Nevertheless, I had myself setup to
start testing all my work on training utils as the first thing to do when I wake
up in the morning. Things didn't go so well after that.

I fell asleep late due to that headache, which was still there when I woke up
multiple times in the middle. The final wake up was too late (albeit with no
headache) and my shitty morning routine ate up my time till past 1400, even
though I tried to use a little of it to eat something.

Nevertheless, once I started, I could immediately find bugs. I started with the
simpler no-context one first ... and the bugs were easy to spot and fix. It
kinda made me happy and all.

But then came testing with-context one ... and that was just stupid. I mean ...
it was not working as expected to the point I was wondering how is Python
calling super ... because it was ending up in infinite recursions!

I finally decided to implement the context adder as a data-prepper instead of a
full on data-provider while still having to implement things in duplicates (with
tiny changes).

All in all ... sub-sampling ... implemented using =groupby= just slows things
down ... and I am in no mood to implement anything else.

Or ... I might ... using the =np_utils.groupby_value= ... because ... the next
tasks on my list are ... the ever dreaded ... normalization.

I have noted it earlier how I it looks to be far more complicated ... especially
when trying to get it done properly. Especially in the case of subsampling,
where I have to make sure that I only normalize based on the statistics of the
data that will be kept ... And, don't even get me started on normalizing on flat
or dimension wise normalization.

Lastly, I also have to choose some appropriate validation files ... based on
analyzing the validation set I have with me.

Then, ... setup the table of experiments ... FUCK!

And considering that this is the last day of the month with single digit date,
and I have not started writing my thesis yet ... it is safe to say that I am
royally FUCKED ...

... and I managed to waste 3/4 of the day today. /sigh/
*** DONE [0/0]  9:17 PM : Totally did that ... potentially 10x faster
CLOSED: [2017-08-09 Wed 21:23]
Yes ... in the time I had to wait for the Prayers, I just basically implemented
groupby based on =np_utils=, and the subsampling provider is almost on par with
the non-subsampling one, ~10x faster that original. However, it does assume
categorical labels, and I have no time to come up with something generic.

Anyways ... I even thinking about flunking normalization ... maybe ... but the
fact is that I don't want to do it now, even though it might potentially improve
my results. It needs a lot of researching and what not ... and for fuck's sake,
I don't want to do it. Maybe the last thing on my list before I run my models.

The other tasks on my list are also decision making ones. I might start on them
soon, and ... if I decide to run something without normalization anyways, then I
might have a model training running tomorrow! Well ... atleast ... submitted.

We'll see how it goes. I am anxious enough already to want to stop working for
today and go to bed, so ... please forgive me for trying to make things a little
easier for me to keep myself in the game.

I having serious doubts about going into research heavy career. I love to code.
** DONE [2/2] 10-Aug-2017
CLOSED: [2017-08-10 Thu 22:31]
*** DONE [0/0]  8:34 PM : I may be ready ... but I am not
CLOSED: [2017-08-10 Thu 20:50]
Last night, I started on choosing the validation calls to use, but was getting
no where. My code was sloppy, and things were ugly. So, I stopped.

Today didn't start very early, but I restarted with analyzing the labels for
validation set, and ended up choosing them based on genders, total ratio of 2T,
the number of such segments, and the average length of such segments (because,
you see, the annotations are not very granular, in fact I didn't choose the top
two wrt ratio of 2T for this reason). I was basically done with my decision by 1
PM, and, if I wanted to, could make the push to go to office and get them running.

However, that would definitely been an exercise in frustration, because there
were many other bugs, especially in the ConfusionsHistory callback that I only
discovered while writing a function to make predictions on end (instead of a
callback). I fixed those, and implemented the latter by just using
with_chunkings, which also I had to implement. Finally, I had to add the extra
channel dim to the with-context prepper.

All those bugs could have been fixed easily, and probably quickly, even if I had
gone to office with the plan to start a model training on unnormalized data.
However, it would certainly have been exhausting for the rest of the day.

I didn't save myself any grace however, because, even after getting done by
around 4PM with all of that, I didn't use my time till now in figuring out the
normalization, or making the scripts. The normalization thing is looking scarier
than ever, especially considering how it probably needs to get tackled in cases
of class skipping or subsampling. And I have no idea what to do.

I have been pushing my code basically, and just biting my time to reach night
time, so that I can go to sleep in this nice and chilly weather. But, there are
still enough hours left that at least working on the scripts can be started,
even if I decide to go with unnormalized data as it is.

And, who knows, it might even work out! There are way too many changes in my
current implementations from the previous ones that I just can't predict the
results.

Nevertheless, I will still then have to setup the table of experiments, the as
yet another dreaded task that I was able to avoid in the name of doing all this
dev work.

I will go through my tasks here and make sure that I haven't missed anything
stupid. I can definitely choose to waste my time in trying to explain in the
workflows why certain calls were chosen for validation (they are =['00007',
'00013', '00028', '00062', '00065', '00069', '00086', ]= BTW), but I hope I
don't, and instead, take up the setting up of the table of experiments, and
start on the scripts so that I have something running on the geepu when I return
tomorrow.

Yes, setting the scripts will require them to be tested beforehand as well. So,
my work is far from done. So much for the hopes of enjoying a sleep in the chill
weather. Nevertheless, I am still not sure about my conv2D model ... and FUCK is
this shit hard.
*** DONE [0/0] 10:27 PM : I might have a very heavily pruned set of experiments to run tomorrow
CLOSED: [2017-08-10 Thu 22:31]
As you can easily imagine, even with the configs that were heavily pruned, even
with unnormalized training looking like my only path, I still have double-digit
number of experiments to perform, if I want to do this correctly.

In this moment of fear, I have heavily pruned even further, most likely not
using clsw_2 at all, and keeping room for normalization later on, maybe.

I have setup the tasks for the necessary scripts I will have to prepare, and
hopefully start tomorrow at office. I will do them as first thing in the
morning, hoping that it will pull me out of my shitty morning routine earlier,
and make the deadline sort of like work for me.
** DONE [1/1] 11-Aug-2017
CLOSED: [2017-08-11 Fri 21:24]
*** DONE [0/0]  9:10 PM : They are running, and they're running fast!
CLOSED: [2017-08-11 Fri 21:24]
I was hoping for an early start today, but didn't really make sure of that last
night by having late dinner and adamantly finishing the new Dan Carlin's
Hardcore History episode.

But, the presence of a virtual deadline kinda pushed me to skip my shitty
morning routine and get started with the training scripts pretty much as soon as
I was out of bed. But ... I had woken up pretty late, and there were looming
clouds of doubt on whether I will have to postpone starting the training
sessions till tomorrow, not the least of which was just my difficulty in
comprehending some of the most basic variables in my code.

I had to perform some quick fixes towards the end, therefore the dry runs on
myrmidon were just immensely helpful. Nevertheless, albeit an hour later than
planned, I had the scripts kinda ready, and an hour or so later, I was trying to
make them run geepu-d.

And you know what, apart from some easy to spot mistakes, the first script
worked flawlessly. And, to my surprise, by the time I duplicated it to make 5
other configurations, the running one had already finished an epoch! Which is
like 1/8th of the first pass, but dude ... it was running fast! All thanks to
the new setup at the geepu-d main node.

Quickly, yet carefully, I got the other ones submitted, and by the looks of it,
I may have them finished by tomorrow! Yes, I only ran them for 5 passes (10 in
case of subsampling), but ... that possibility just exhilirated me. Which was
important given how anxious I was while preparing the first one, and how much I
lacked confidence in getting even one script started today just before leaving
my room.

That comes with a small caveat too ... because now, not only is it possible for
me to run for many more epochs, but also, for multiple possible configurations,
including maybe spectrograms, mfccs, etc not forgetting different
normalizations. I know that if I start to increase the number of epochs, my
training time will still go into weeks, and that will be dangerous, but ... hey
... I can see that light, after I have been in so much darkness.

However, I should still be careful, because it is far more important for me to
start writing my thesis document now than it is to start finding excuses not to.
And ... if history has taught me anything about my working, even the simplest
things can take a lot of time and effort. They may as well end up paying back,
but I have nowhere the luxury of time as I had even a week ago.

I'll see what I do ... and I'll see into it tomorrow, hopefully early in the
morning. I will be going to office to check on the finished jobs, and restart
them with for longer number of epochs. So, I hope, I will have the table of
experiments with those that are running up there tomorrow ... at some point at
least. Till then ... I really want to enjoy some deep sleep in this nice and
chilly weather.
** DONE [2/2] 16-Aug-2017
CLOSED: [2017-08-17 Thu 02:58]
*** DONE [0/0]  2:20 PM : Fuck My Life
CLOSED: [2017-08-16 Wed 14:34]
I have essentially not been working since the last entry. If you want to be
generous and count the model trainings going on in the background as a single
task that *I* am doing, then you are missing the point of this being the last
month to finish my thesis.

I tried, but my sleeping routine has gotten fucked, and my shitty morning
routine, which therefore happens much later in the day, has become worse. Plus,
I also had a bad headache yesterday, rendering me useless.

But ... let's stop degrading ourselves, and try to focus on some other possible
whys.

The thing is, the next tasks ahead of me ... are those that I had deferred
earlier because they were taking too much time because they needed analysis,
research and, by far the most dreaded, *decision making*. They include,
normalization, post-proc, and evaluations. Yes, there is this another one, which
is *actually writing the damn thesis*, but ... we'll come to that a bit later.

I need a plan for those difficult tasks, and a bounded one at that, and
definitely explicit in all regards. That is what I am going to do next, at least
for the normalization step, because that is going to define perhaps the next set
of models that I want to train, which looks like not starting before Friday. (FML).

The post-processing thing will have to be compromised a little bit, and the
evaluations will have to stick with frame level statistics, and /maybe/
segmentation metrics, if I plan to use the iFinder system for it. They both will
involve significant development effort if I don't act smartly, let alone faithfully.

Back to writing the thesis. /sigh/. Let's make one thing clear. I know that
writing the thesis will definitely help guide my decisions and steps better and
put things in context. Moreover, it is *the* the most important thing to do with
respect to my thesis. All the experiments will be for naught if I don't have a
proper written document. And, considering that I have lost half a month has
passed already, it is becoming more and more urgent.

We'll, unfortunately, have to defer it to a little later in the week. I *have*
to decide on normalization first, because I expect to be talking about it in the
thesis, and because it will perhaps improve my results. So, that's the plan for
starting today, till I go to sleep. Finish normalization things. /gulp/
*** DONE [0/0]  9:17 PM : What the hell am I supposed to do?
CLOSED: [2017-08-16 Wed 21:29]
I spent roughly 4 hours or more trying again to find out a way to normalize my
data. My initial plan was to just write down the why's and possible how's, but
then I started looking at other papers on google and what not for their
normalization strategies.

From VGG-net, the one that my network is the closest relative to, I saw they
were only doing dataset level mean normalization over all pixels in a channel,
and were definitely using it in their tests and evaluations, but I can't use
that as it is ... not at the frame with context level ... because it doesn't, so
far, make sense to me, except if I am trying to copy their method as is, without
any explanation.

So, I tried to come up with an explanation. The one thing that was haunting me
about their approach was that they have an explicitly limited range of possible
values for each pixel [0, 255]. In our case, we do have an explicit lower bound
on the values, they would have been 0 for the power-spectrogram, and -8 when
logged with amin of 1e-8. But ... the maximum ... that has been proving to be
elusive. Even though all the operations are very well known, and my input has an
explicit normalization of (-1, 1) range, I can't seem to find and neither come
up with that max limit of the final value. That ate up significant portion of my
time and energy, and was the last thing I tried doing before taking a break for
dinner.

In the middle though, before I went on doing that, I looked at more refrences
for overlapping speech detection, and lo and behold, there was an entire thesis
that I had never seen. It is explicitly on the topic of overlapping speech
detection, with more focus on improving diarization, and using GMM-HMM. I
skimmed through that quickly as well, mainly trying to find their approach to
normalization ... but all that they are doing is CMN, and that too on MFCC. I
can definitely, however, use the thesis to get ideas about Viterbi decoding ...
but that comes later.

In essence ... I am back to where I was, and I blame, in addition to the task
being scary, the lack of direction and appropriate planning. I know that writing
it down would help, especially in thesis form ... but hey ... I still don't know
about a lot of stuff.

Nevertheless, I am going to start on that planning and working late night today
... especially considering the date, and my shitty sleeping and waking-up rituals.
** DOIN [2/3] 17-Aug-2017
*** TODO [2/12]  3:03 AM : Alright ... CMN it is ... was it so hard?
Alright ... I believe I have some direction to go in. Reading that thesis of the
guy from RWTH was very helpful and introduced me to different possibilities of
normalization, and a summarization and sets of experiments with results. Very
very useful indeed.

I have decided to try at least mean normalization per-dim on the filter-banks. I
am not sure if I will try out MFCCs, but it doesn't look like it, especially
after reading how it's goal is to separate the source and vocal tract level
information from the extracted features.

The other method that I wanted to try out earlier was to do dynamic range
normalization but ... for some reason, I don't think it is a good idea, even
though I can somehow match it to what is usually done in computer vision
(scaling). My main concern is not knowing the bounds of the values, especially
the max. From the looks of the histograms, I may be fine with choosing
log10(n_fft), but ... I can't / won't explain it ... plus ... it can go fuck
myself. Actually, if I calculate mel-scaled values on a power spectrogram that
has all values equal to 2**14 (what I got as pow-spec for constant signal), then
... log10(/of that/) is almost 7 ... resulting in the data being centered
already ... !!! which is definitely not the case.

If I want to do something with the histograms, then it makes sense to do the
histogram equalization thing from that thesis, which ... God help me ... seems
to complicated and I don't want to them.

Talking of copyting computer vision kind of steps, that VGG-net paper, and many
others, actually only do channel wise pixel level mean subtraction (over the
entire dataset, and later use it on test as well). I may think about it, but
TBH, I don't think it will make any sense in speech. However, looking at the
plots for each dimensions, they don't look so independent to the neighbors, and
seem to lie in similar ranges, so .... /maybe/

Variance normalization alongside mean normalization makes sense ... a little bit
... but ... well ... it is a single step ... and increases the number of
experiments that I can run and evaluate ... without needing too much explanation.

Lastly, I am thinking about dropping the max-5-lctx configs altogether. From the
last results, they were worse than the per-frame ones, and require jumping
through a lot of hoops to explain them.

- [X] Implement Mean and variance normalizer that only acts per chunk
- [X] Final scripts to run the 0-lctx for mean, and mean+var normalization
- [ ] Offic:
  + [ ] start these scripts,
  + [ ] decide final if mx-5-lctx is dead
  + [ ] collect and bring the results.
- [ ] Other CNN changes
  + [ ] Average pooling
  + [ ] More context (±20)
  + [ ] Bigger first filter size
  + [ ] Deeper network with more convs before pooling
  + [ ] clsw???
*** DONE [0/0]  3:14 PM : The scripts are ready ... too much time here and there
CLOSED: [2017-08-17 Thu 15:22]
I went to sleep quite a bit later than I was done writing all of that above. I
was actually reading more about from that TUM speech recognition wiki, and also
found a paper which concentrated very heavily on analyzing the data imbalance
problem, and kinda skimmed through it all ... and could see that there is a
chance that my networks have not been learning very well, especially when I
don't help it. The idea of clsw has therefore risen again ... as a method of
modifying the objective function and make the loss function reflect the cost of
misclassifying the minority class.

Of course, there are also ideas of using more powerful network configurations,
and testing filter sizes, and using deeper networks ... etc.

But ... since I have woken, which I did at the same usual time, even when I had
gone to bed only after the sun had already come up, I have been working on mean
and variance normalizer, mainly testing the impact on the histograms, because
the code itself was pretty easy (because I am chunk level normalization,
irrespective of whether there is any subsampling or class skipping going on
later on). Then, I prepared the scripts, and now they are all ready. It is
almost the end of the day ... but I also don't have to do a lot ... I may leave
in this hour hopefully.

And when I come back, I will have the results of the previous trainings, and ...
even though it will be very tempting to take the rest of the day off ... no! I
will either start analyzing the results in a bit more detail ... or start
writing the thesis. Both, I agree, and dangerous looking tasks, but ... hey ...
we ain't got time for nothing.
*** DONE [0/0]  5:45 PM : No ... I didn't
CLOSED: [2017-08-17 Thu 17:53]
I actually prepared, put everything in the bag and went out to go to office.
But, by the time I had left, it was past 1600, and there was no way I was going
to reach office before 1700. Plus, the weather was / is this weird and bad mix
of post-rain humid and warm and very little wind.

I turned halfway on my way and got lunch/dinner and came back. Gonna go run
those things tomorrow.

An important reason for skipping office today was what I hinted at earlier what might
have happened to the rest of the day. I would have most likely gotten really
tired, and taken the rest of it till tomorrow morning ... off. /Oh yes ... we
were going to instead work on the thesis weren't we ... / nope ... definitely
not.

On my way, I was thinking about what I would write about anyway if I succeeded
in making myself do it. The goals/scope section made the most sense, because it
is relatively easy, and may even help in decisions about the various experiments
I plan to run later on.

So ... that is the plan for today ... or at least till I go to sleep. Finish the
Goals and Scope section. I may ... after that ... even start data-analysis
section, because certain arguments will need backing from the data. /Just .../
don't get too hooked up about making the plots and charts look pretty. Not right now.

For some reason ... the internet is not working ... and ... honestly, I don't
need it ... but there are a few more minutes I wanted to chillax after lunch,
which ... for good or for worse ... may have to get cut short.

Nevertheless, before I have even started on it, even the most trivial of the
sections of the thesis promise to be very difficult to write, if not only time
consuming. There is also the question of whether or not I should go into the
details of how a DCN works, and what is deep learning, and all the associated
algorithms and what not. If I have to inflate the number of pages, I can
certainly do that, or leave it as an appendix item with refrences.

Oh God the refrences ... I can no longer just skim the papers ... I'll have to
read them not only to make sure what I am referring to from their work does make
sense, but also to probably get some proper ideas to write them in my own words.
There is a lot of reading that is going to be involved. And I am not even
mentioning how many tons of fucking papers I have already.

Lastly, equations ... metrics, etc ... they will have to be written down as
well. I may wing it in the beginning but they will not be painless either.

Last night's work was surprisingly focused and I remembered a lot of it. I was
also relatively quick. The peace and serenity ... may be those are worth fucking
up my sleeping schedule, considering especially that my shitty morning routine
ends up fucking things up any way. This way, chillax in the evening at least comes after
something has already been achieved ... and so does going to sleep.
** DONE [1/1] 18-Aug-2017
CLOSED: [2017-08-19 Sat 20:21]
*** DONE [0/0]  8:23 PM : No ... I didn't ... again
CLOSED: [2017-08-18 Fri 20:32]
Wait ... regarding the running of trainings thing ... /that/ I got done today
... except that the cluster is pretty busy ... so I might end up taking more
time than just this weekend. That damn predit-on-end thing still didn't work ...
this time due to a stupid typo bug that I introduced in the last commit.

What I didn't do is ... writing anything in the thesis ... let alone anything in
the goals section or any other section ... although I had brainstormed a lot
about it and the no-internet situation was like a perfect opportunity, and I did
not go to sleep early either. I don't have an excuse ... just that I was stupid
and over-slept today as well ... and all I could do was go to office, get the
trainings running ... and because it was raining so heavily ... get some
preliminary and of course surprisingly underwhelming analysis of the results of
the trainings that had finished.

I will be doing more detailed analysis later (I hope), but as it is ... it looks
like the precision for double-talk just went very high while the recall went
into single digit percentages ... which essentially means it didn't work out. I
might look at the histograms more deeply and find a way to exploit any patterns
if there are any ... otherwise ... I have no idea other than just try smoothing
or ... choose a checkpoint before the last one. It is surprising, however, that
the model did not seem to have converged in any case, although I have a feeling
that that convergence would have come at very low recall and very high precision
for double talk ... since it is such a small fraction of the entire training set.

I know that there is barely one month left now for the real hard deadline, and
barely a week for my personal one. I am going to start on the thesis ...
especially considering that I have not much else to do for the next three days
which have more impact promised.
** DONE [1/1] 19-Aug-2017
CLOSED: [2017-08-19 Sat 20:29]
*** DONE [0/0]  8:21 PM : Shit
CLOSED: [2017-08-19 Sat 20:28]
Wait ... don't take ou the pitchforks so soon. I did keep the promise and
started writing the thesis last night. I started with the introduction section
because that was the one that was attracting my thoughts the most. Plus, it was
supposed to be the easiest one to handle.

Unfortunately, I have essentially wasted the nearly 7 hours that I worked on the
thing. I got distracted in a different way. For some reason, and definitely
without appropriate foresight, I started by talking about the Turing Test for
some reason, and on top of that ... made my sentences too complicated. The later
could have been fixed, but the path that I started on ... was going to get me
stuck in a corner later on. I do not have to debate machine intelligence v/s
human intelligence, and whether or not there is a potential of some elegant nods
and winks is not only irrelevant, it is also potentially dangerous.

I have to write the whole thing in a dry scientific and straightforward, clear
manner. Not only it is what is expected from me, it will also make my life a lot easier.

Nevertheless, those 7 hours gave me a taste of how hours can fly by in doing
this simple task without much progress to show for them. It is scary, and I have
to, again, be ruthless and, hopefully, responsible.

Nevertheless, I am still going to start with the introduction section, leaving
some sub-sections as merely outlines. The need for this approach stems from the
benefit of maintaining flow of thoughts and ideas.
** DOIN [1/1] 28-Aug-2017
*** DONE [0/0]  2:23 AM : I can only wish
CLOSED: [2017-08-28 Mon 02:38]
I can only wish that I would have been ruthless ... or at least responsible.
With my time, with my energy, with my focus, with my task in general and in
particular this thesis.

A week has passed, and I have only hastily been able to move on to chapter 2 by
skipping the most important section in the Introduction chapter, that on
relevant works, as mostly just outline peppered heavily with 'JIT FIO'
(Just-In-Time Figure-It-Out) tasks. Even the sections that have been closed are
definitely incomplete ... and the ones coming next are more difficult by an
order of magnitude ... with respect every metric one can think of, except
probably one which I am sure I should not exploit, and probably even can't.

It has been disappointing, my performance that is. I could definitely have made
a lot more progress if I had acted more responsibly. Forget about the days and
months that were wasted earlier ... the hours wasted in the past week are just
inexcusable. At this point in time? Abmoniable!

And here I am writing into the logs wasting my late night hours on words that
will perhaps never be read by a living soul.

Nevertheless, expect fewer updates in the upcoming week as well. I hope that the
next entry is titled ... FINALLY! ... but I cannot trust myself enough to be
sincere in hoping that.

A miracle would work just fine ... maybe ...
